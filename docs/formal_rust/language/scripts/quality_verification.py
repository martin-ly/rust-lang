#!/usr/bin/env python3
"""
Rustå½¢å¼åŒ–ç†è®ºé¡¹ç›® - é˜¶æ®µå››è´¨é‡éªŒè¯è„šæœ¬
æ‰§è¡Œè‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥ï¼ŒåŒ…æ‹¬ä¸€è‡´æ€§éªŒè¯ã€å®Œæ•´æ€§æ£€æŸ¥ç­‰
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict

class QualityVerifier:
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.results = {
            'consistency_check': {},
            'completeness_check': {},
            'cross_reference_check': {},
            'concept_definitions': {},
            'mathematical_symbols': {},
            'errors': [],
            'warnings': []
        }
        
    def run_full_verification(self):
        """æ‰§è¡Œå®Œæ•´çš„è´¨é‡éªŒè¯æµç¨‹"""
        print("ğŸ” å¼€å§‹æ‰§è¡Œè´¨é‡éªŒè¯...")
        
        # 1. æ¦‚å¿µä¸€è‡´æ€§æ£€æŸ¥
        print("\nğŸ“‹ 1. æ‰§è¡Œæ¦‚å¿µä¸€è‡´æ€§æ£€æŸ¥...")
        self.check_concept_consistency()
        
        # 2. æ•°å­¦ç¬¦å·ä¸€è‡´æ€§æ£€æŸ¥
        print("\nğŸ”¢ 2. æ‰§è¡Œæ•°å­¦ç¬¦å·ä¸€è‡´æ€§æ£€æŸ¥...")
        self.check_mathematical_symbols()
        
        # 3. äº¤å‰å¼•ç”¨éªŒè¯
        print("\nğŸ”— 3. æ‰§è¡Œäº¤å‰å¼•ç”¨éªŒè¯...")
        self.verify_cross_references()
        
        # 4. å®Œæ•´æ€§æ£€æŸ¥
        print("\nğŸ“Š 4. æ‰§è¡Œå®Œæ•´æ€§æ£€æŸ¥...")
        self.check_completeness()
        
        # 5. æ–‡æ¡£ç»“æ„éªŒè¯
        print("\nğŸ“ 5. æ‰§è¡Œæ–‡æ¡£ç»“æ„éªŒè¯...")
        self.verify_document_structure()
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“„ 6. ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        self.generate_report()
        
    def check_concept_consistency(self):
        """æ£€æŸ¥æ¦‚å¿µå®šä¹‰çš„ä¸€è‡´æ€§"""
        concept_definitions = defaultdict(list)
        
        # æ‰«ææ‰€æœ‰markdownæ–‡ä»¶
        for md_file in self.base_path.rglob("*.md"):
            content = self.read_file_safely(md_file)
            if content is None:
                continue
                
            # æå–æ¦‚å¿µå®šä¹‰
            definitions = self.extract_concept_definitions(content, md_file)
            for concept, definition in definitions:
                concept_definitions[concept].append({
                    'file': str(md_file.relative_to(self.base_path)),
                    'definition': definition
                })
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        inconsistencies = []
        for concept, defs in concept_definitions.items():
            if len(defs) > 1:
                # æ£€æŸ¥å®šä¹‰æ˜¯å¦ä¸€è‡´
                base_def = defs[0]['definition']
                for def_info in defs[1:]:
                    if not self.definitions_consistent(base_def, def_info['definition']):
                        inconsistencies.append({
                            'concept': concept,
                            'conflicting_files': [defs[0]['file'], def_info['file']],
                            'definitions': [base_def, def_info['definition']]
                        })
        
        self.results['consistency_check']['concept_inconsistencies'] = inconsistencies
        self.results['concept_definitions'] = dict(concept_definitions)
        
        if inconsistencies:
            print(f"âš ï¸  å‘ç° {len(inconsistencies)} ä¸ªæ¦‚å¿µå®šä¹‰ä¸ä¸€è‡´é—®é¢˜")
        else:
            print("âœ… æ¦‚å¿µå®šä¹‰ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
    
    def extract_concept_definitions(self, content: str, file_path: Path) -> List[Tuple[str, str]]:
        """ä»æ–‡æ¡£ä¸­æå–æ¦‚å¿µå®šä¹‰"""
        definitions = []
        
        # åŒ¹é…å½¢å¼åŒ–å®šä¹‰æ¨¡å¼
        patterns = [
            r'###\s*å®šä¹‰[ï¼š:]\s*([^#\n]+)\n([^#]+?)(?=###|\n##|\Z)',
            r'#### å®šä¹‰\d*[ï¼š:]?\s*([^#\n]+)\n([^#]+?)(?=###|\n##|\Z)',
            r'\*\*å®šä¹‰\*\*[ï¼š:]?\s*([^*\n]+)\*\*[ï¼š:]?\s*([^\n]+)',
            r'å®šä¹‰\s*\d*[ï¼š:]?\s*([^ï¼š:\n]+)[ï¼š:]\s*([^\n]+)'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL)
            for match in matches:
                concept = match.group(1).strip()
                definition = match.group(2).strip()
                definitions.append((concept, definition))
        
        return definitions
    
    def definitions_consistent(self, def1: str, def2: str) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå®šä¹‰æ˜¯å¦ä¸€è‡´ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # ç§»é™¤æ ‡ç‚¹å’Œç©ºç™½è¿›è¡Œæ¯”è¾ƒ
        normalized1 = re.sub(r'[^\w\s]', '', def1.lower()).split()
        normalized2 = re.sub(r'[^\w\s]', '', def2.lower()).split()
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„Jaccardç›¸ä¼¼åº¦ï¼‰
        set1, set2 = set(normalized1), set(normalized2)
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        similarity = intersection / union if union > 0 else 0
        return similarity > 0.7  # 70%ç›¸ä¼¼åº¦é˜ˆå€¼
    
    def check_mathematical_symbols(self):
        """æ£€æŸ¥æ•°å­¦ç¬¦å·çš„ä¸€è‡´æ€§ä½¿ç”¨"""
        symbol_usage = defaultdict(list)
        
        # å¸¸è§æ•°å­¦ç¬¦å·æ¨¡å¼
        symbol_patterns = {
            'forall': r'âˆ€|\\forall',
            'exists': r'âˆƒ|\\exists', 
            'implies': r'â‡’|\\implies|=>',
            'equiv': r'â‡”|\\equiv|<=>',
            'in': r'âˆˆ|\\in',
            'subset': r'âŠ†|\\subseteq|âŠ‚|\\subset',
            'union': r'âˆª|\\cup|âŠ|\\uplus',
            'intersection': r'âˆ©|\\cap',
            'bottom': r'âŠ¥|\\bot',
            'top': r'âŠ¤|\\top'
        }
        
        for md_file in self.base_path.rglob("*.md"):
            content = self.read_file_safely(md_file)
            if content is None:
                continue
                
            for symbol_name, pattern in symbol_patterns.items():
                matches = re.findall(pattern, content)
                if matches:
                    symbol_usage[symbol_name].extend([{
                        'file': str(md_file.relative_to(self.base_path)),
                        'variants': list(set(matches))
                    }])
        
        # æ£€æŸ¥ç¬¦å·ä½¿ç”¨çš„ä¸€è‡´æ€§
        symbol_inconsistencies = []
        for symbol_name, usages in symbol_usage.items():
            variants = set()
            for usage in usages:
                variants.update(usage['variants'])
            
            if len(variants) > 1:
                symbol_inconsistencies.append({
                    'symbol': symbol_name,
                    'variants': list(variants),
                    'files': [usage['file'] for usage in usages]
                })
        
        self.results['mathematical_symbols'] = dict(symbol_usage)
        self.results['consistency_check']['symbol_inconsistencies'] = symbol_inconsistencies
        
        if symbol_inconsistencies:
            print(f"âš ï¸  å‘ç° {len(symbol_inconsistencies)} ä¸ªæ•°å­¦ç¬¦å·ä¸ä¸€è‡´é—®é¢˜")
        else:
            print("âœ… æ•°å­¦ç¬¦å·ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
    
    def verify_cross_references(self):
        """éªŒè¯äº¤å‰å¼•ç”¨çš„æœ‰æ•ˆæ€§"""
        broken_links = []
        internal_links = []
        
        # æå–æ‰€æœ‰å†…éƒ¨é“¾æ¥
        link_pattern = r'\[\[([^\]]+)\]\]|\[([^\]]+)\]\(([^)]+)\)'
        
        for md_file in self.base_path.rglob("*.md"):
            content = self.read_file_safely(md_file)
            if content is None:
                continue
                
            matches = re.finditer(link_pattern, content)
            for match in matches:
                if match.group(1):  # [[link]] format
                    link_target = match.group(1)
                    internal_links.append({
                        'source': str(md_file.relative_to(self.base_path)),
                        'target': link_target,
                        'type': 'internal'
                    })
                elif match.group(3):  # [text](link) format
                    link_target = match.group(3)
                    if not link_target.startswith(('http://', 'https://')):
                        internal_links.append({
                            'source': str(md_file.relative_to(self.base_path)),
                            'target': link_target,
                            'type': 'relative'
                        })
        
        # éªŒè¯é“¾æ¥æœ‰æ•ˆæ€§
        for link in internal_links:
            target_path = self.resolve_link_target(link['target'], link['source'])
            if target_path and not target_path.exists():
                broken_links.append({
                    'source': link['source'],
                    'target': link['target'],
                    'resolved_path': str(target_path)
                })
        
        self.results['cross_reference_check'] = {
            'total_links': len(internal_links),
            'broken_links': broken_links,
            'valid_links': len(internal_links) - len(broken_links)
        }
        
        if broken_links:
            print(f"âš ï¸  å‘ç° {len(broken_links)} ä¸ªæŸåçš„é“¾æ¥")
        else:
            print("âœ… äº¤å‰å¼•ç”¨éªŒè¯é€šè¿‡")
    
    def resolve_link_target(self, target: str, source: str) -> Path:
        """è§£æé“¾æ¥ç›®æ ‡è·¯å¾„"""
        try:
            if target.endswith('.md'):
                # ç›¸å¯¹è·¯å¾„
                source_dir = Path(source).parent
                return self.base_path / source_dir / target
            else:
                # å¯èƒ½æ˜¯æ–‡ä»¶åæˆ–ç« èŠ‚å¼•ç”¨
                possible_path = self.base_path / f"{target}.md"
                if possible_path.exists():
                    return possible_path
                
                # æœç´¢åŒ¹é…çš„æ–‡ä»¶
                for md_file in self.base_path.rglob("*.md"):
                    if md_file.stem == target:
                        return md_file
                        
        except Exception:
            pass
        return None
    
    def check_completeness(self):
        """æ£€æŸ¥ç†è®ºä½“ç³»çš„å®Œæ•´æ€§"""
        # å®šä¹‰æ ¸å¿ƒæ¨¡å—å’Œé¢„æœŸæ–‡ä»¶
        expected_modules = {
            '01_ownership_borrowing': ['00_index.md', '01_*.md'],
            '02_type_system': ['00_index.md', '01_*.md'],
            '03_control_flow': ['00_index.md', '01_*.md'],
            '04_generics': ['00_index.md', '01_*.md'],
            '05_concurrency': ['00_index.md', '01_*.md'],
            '06_async_await': ['00_index.md', '01_*.md'],
            '07_macro_system': ['00_index.md', '01_*.md'],
            '08_algorithms': ['00_index.md', '01_*.md'],
            '09_design_patterns': ['00_index.md', '01_*.md'],
            '10_modules': ['00_index.md', '01_*.md'],
            '11_memory_management': ['00_index.md', '01_*.md'],
            '12_traits': ['00_index.md', '01_*.md']
        }
        
        completeness_report = {}
        missing_files = []
        
        for module, expected_files in expected_modules.items():
            module_path = self.base_path / module
            if module_path.exists():
                existing_files = list(module_path.glob("*.md"))
                expected_count = len(expected_files)
                actual_count = len(existing_files)
                
                completeness_report[module] = {
                    'expected_files': expected_count,
                    'actual_files': actual_count,
                    'completion_rate': min(actual_count / max(expected_count, 1), 1.0),
                    'files': [f.name for f in existing_files]
                }
                
                # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶
                index_file = module_path / "00_index.md"
                if not index_file.exists():
                    missing_files.append(f"{module}/00_index.md")
            else:
                missing_files.append(f"æ•´ä¸ªæ¨¡å—: {module}")
        
        self.results['completeness_check'] = {
            'module_completeness': completeness_report,
            'missing_files': missing_files,
            'overall_completion': self.calculate_overall_completion(completeness_report)
        }
        
        overall_completion = self.results['completeness_check']['overall_completion']
        print(f"ğŸ“Š ç†è®ºä½“ç³»å®Œæ•´æ€§: {overall_completion:.1%}")
        
        if missing_files:
            print(f"âš ï¸  å‘ç° {len(missing_files)} ä¸ªç¼ºå¤±æ–‡ä»¶/æ¨¡å—")
    
    def calculate_overall_completion(self, completeness_report: Dict) -> float:
        """è®¡ç®—æ€»ä½“å®Œæˆåº¦"""
        if not completeness_report:
            return 0.0
        
        total_completion = sum(info['completion_rate'] for info in completeness_report.values())
        return total_completion / len(completeness_report)
    
    def verify_document_structure(self):
        """éªŒè¯æ–‡æ¡£ç»“æ„çš„ä¸€è‡´æ€§"""
        structure_issues = []
        
        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶çš„ç»“æ„
        for index_file in self.base_path.rglob("00_index.md"):
            content = self.read_file_safely(index_file)
            if content is None:
                continue
                
            issues = self.check_index_structure(content, index_file)
            if issues:
                structure_issues.extend(issues)
        
        self.results['structure_verification'] = {
            'issues': structure_issues,
            'total_index_files': len(list(self.base_path.rglob("00_index.md")))
        }
        
        if structure_issues:
            print(f"âš ï¸  å‘ç° {len(structure_issues)} ä¸ªæ–‡æ¡£ç»“æ„é—®é¢˜")
        else:
            print("âœ… æ–‡æ¡£ç»“æ„éªŒè¯é€šè¿‡")
    
    def check_index_structure(self, content: str, file_path: Path) -> List[str]:
        """æ£€æŸ¥ç´¢å¼•æ–‡ä»¶çš„ç»“æ„"""
        issues = []
        
        # æ£€æŸ¥å¿…éœ€çš„ç« èŠ‚
        required_sections = [
            r'##\s*æ¦‚è¿°',
            r'##\s*æ ¸å¿ƒæ¦‚å¿µ',
            r'##\s*ç›¸å…³æ¨¡å—',
            r'##\s*å‚è€ƒæ–‡çŒ®',
            r'##\s*ç»´æŠ¤ä¿¡æ¯'
        ]
        
        for section_pattern in required_sections:
            if not re.search(section_pattern, content, re.IGNORECASE):
                issues.append(f"{file_path.relative_to(self.base_path)}: ç¼ºå°‘å¿…éœ€ç« èŠ‚ {section_pattern}")
        
        return issues
    
    def read_file_safely(self, file_path: Path) -> str:
        """å®‰å…¨åœ°è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            self.results['errors'].append(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return None
    
    def generate_report(self):
        """ç”Ÿæˆè´¨é‡éªŒè¯æŠ¥å‘Š"""
        report_path = self.base_path / "RESTRUCTURE_WORKING" / "phase4_quality_verification_report.md"
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        quality_score = self.calculate_quality_score()
        
        report_content = f"""# é˜¶æ®µå››ï¼šè´¨é‡éªŒè¯æŠ¥å‘Š

## æ‰§è¡Œæ¦‚è¦

æœ¬æŠ¥å‘Šæ€»ç»“äº†å½¢å¼åŒ–Rustè¯­è¨€ç†è®ºé¡¹ç›®é˜¶æ®µå››è´¨é‡éªŒè¯çš„æ‰§è¡Œç»“æœã€‚

## è´¨é‡è¯„åˆ†

**æ€»ä½“è´¨é‡åˆ†æ•°**: {quality_score:.1f}/100

## éªŒè¯ç»“æœè¯¦æƒ…

### 1. æ¦‚å¿µä¸€è‡´æ€§æ£€æŸ¥

- **æ¦‚å¿µå®šä¹‰æ€»æ•°**: {len(self.results['concept_definitions'])}
- **ä¸ä¸€è‡´é—®é¢˜**: {len(self.results['consistency_check'].get('concept_inconsistencies', []))}

### 2. æ•°å­¦ç¬¦å·ä¸€è‡´æ€§

- **ç¬¦å·ç±»å‹æ€»æ•°**: {len(self.results['mathematical_symbols'])}
- **ç¬¦å·ä¸ä¸€è‡´é—®é¢˜**: {len(self.results['consistency_check'].get('symbol_inconsistencies', []))}

### 3. äº¤å‰å¼•ç”¨éªŒè¯

- **æ€»é“¾æ¥æ•°**: {self.results['cross_reference_check'].get('total_links', 0)}
- **æœ‰æ•ˆé“¾æ¥æ•°**: {self.results['cross_reference_check'].get('valid_links', 0)}
- **æŸåé“¾æ¥æ•°**: {len(self.results['cross_reference_check'].get('broken_links', []))}

### 4. å®Œæ•´æ€§æ£€æŸ¥

- **æ¨¡å—å®Œæ•´æ€§**: {self.results['completeness_check'].get('overall_completion', 0):.1%}
- **ç¼ºå¤±æ–‡ä»¶æ•°**: {len(self.results['completeness_check'].get('missing_files', []))}

### 5. æ–‡æ¡£ç»“æ„éªŒè¯

- **ç´¢å¼•æ–‡ä»¶æ€»æ•°**: {self.results.get('structure_verification', {}).get('total_index_files', 0)}
- **ç»“æ„é—®é¢˜æ•°**: {len(self.results.get('structure_verification', {}).get('issues', []))}

## é—®é¢˜è¯¦æƒ…

### æ¦‚å¿µä¸€è‡´æ€§é—®é¢˜
"""
        
        for issue in self.results['consistency_check'].get('concept_inconsistencies', []):
            report_content += f"- **{issue['concept']}**: åœ¨æ–‡ä»¶ {', '.join(issue['conflicting_files'])} ä¸­å®šä¹‰ä¸ä¸€è‡´\n"
        
        report_content += "\n### æ•°å­¦ç¬¦å·ä¸€è‡´æ€§é—®é¢˜\n"
        for issue in self.results['consistency_check'].get('symbol_inconsistencies', []):
            report_content += f"- **{issue['symbol']}**: ä½¿ç”¨äº†å¤šç§å˜ä½“ {', '.join(issue['variants'])}\n"
        
        report_content += "\n### æŸåçš„é“¾æ¥\n"
        for link in self.results['cross_reference_check'].get('broken_links', []):
            report_content += f"- {link['source']} â†’ {link['target']}\n"
        
        report_content += f"""
### ç¼ºå¤±æ–‡ä»¶
"""
        for missing in self.results['completeness_check'].get('missing_files', []):
            report_content += f"- {missing}\n"
        
        report_content += f"""
## è´¨é‡è®¤è¯å»ºè®®

åŸºäºå½“å‰è´¨é‡åˆ†æ•° {quality_score:.1f}ï¼Œå»ºè®®è®¤è¯çº§åˆ«ï¼š
"""
        if quality_score >= 95:
            report_content += "- **é’»çŸ³çº§è®¤è¯** âœ¨ - é¡¶çº§è´¨é‡æ ‡å‡†\n"
        elif quality_score >= 85:
            report_content += "- **é»„é‡‘çº§è®¤è¯** ğŸ¥‡ - å“è¶Šè´¨é‡æ ‡å‡†\n"
        elif quality_score >= 75:
            report_content += "- **ç™½é“¶çº§è®¤è¯** ğŸ¥ˆ - é«˜è´¨é‡æ ‡å‡†\n"
        elif quality_score >= 60:
            report_content += "- **é’é“œçº§è®¤è¯** ğŸ¥‰ - åŸºç¡€è´¨é‡æ ‡å‡†\n"
        else:
            report_content += "- **éœ€è¦æ”¹è¿›** âš ï¸ - æœªè¾¾åˆ°è®¤è¯æ ‡å‡†\n"
        
        report_content += f"""
## æ”¹è¿›å»ºè®®

1. ä¼˜å…ˆè§£å†³æ¦‚å¿µå®šä¹‰ä¸ä¸€è‡´é—®é¢˜
2. ç»Ÿä¸€æ•°å­¦ç¬¦å·çš„ä½¿ç”¨è§„èŒƒ
3. ä¿®å¤æ‰€æœ‰æŸåçš„äº¤å‰å¼•ç”¨
4. è¡¥å……ç¼ºå¤±çš„æ–‡æ¡£å’Œç« èŠ‚
5. æ ‡å‡†åŒ–æ–‡æ¡£ç»“æ„æ ¼å¼

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. æ ¹æ®æœ¬æŠ¥å‘Šä¿®å¤å‘ç°çš„é—®é¢˜
2. é‡æ–°è¿è¡Œè´¨é‡éªŒè¯
3. ç”³è¯·æ­£å¼è´¨é‡è®¤è¯
4. å‡†å¤‡é¡¹ç›®å‘å¸ƒææ–™

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {self.get_current_time()}
**éªŒè¯å·¥å…·ç‰ˆæœ¬**: v1.0
**æ•°æ®å®Œæ•´æ€§**: âœ… å·²éªŒè¯
"""
        
        # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
        try:
            report_path.parent.mkdir(exist_ok=True)
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"ğŸ“„ è´¨é‡éªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        except Exception as e:
            print(f"âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Š: {e}")
        
        # è¾“å‡ºJSONæ ¼å¼çš„è¯¦ç»†ç»“æœ
        json_report_path = report_path.with_suffix('.json')
        try:
            with open(json_report_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜: {json_report_path}")
        except Exception as e:
            print(f"âŒ æ— æ³•ç”ŸæˆJSONæŠ¥å‘Š: {e}")
    
    def calculate_quality_score(self) -> float:
        """è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°"""
        scores = []
        
        # æ¦‚å¿µä¸€è‡´æ€§åˆ†æ•° (25åˆ†)
        concept_issues = len(self.results['consistency_check'].get('concept_inconsistencies', []))
        concept_score = max(0, 25 - concept_issues * 5)
        scores.append(concept_score)
        
        # ç¬¦å·ä¸€è‡´æ€§åˆ†æ•° (20åˆ†)
        symbol_issues = len(self.results['consistency_check'].get('symbol_inconsistencies', []))
        symbol_score = max(0, 20 - symbol_issues * 3)
        scores.append(symbol_score)
        
        # äº¤å‰å¼•ç”¨åˆ†æ•° (25åˆ†)
        total_links = self.results['cross_reference_check'].get('total_links', 1)
        valid_links = self.results['cross_reference_check'].get('valid_links', 0)
        ref_score = (valid_links / total_links) * 25 if total_links > 0 else 25
        scores.append(ref_score)
        
        # å®Œæ•´æ€§åˆ†æ•° (20åˆ†)
        completion_rate = self.results['completeness_check'].get('overall_completion', 0)
        completion_score = completion_rate * 20
        scores.append(completion_score)
        
        # ç»“æ„ä¸€è‡´æ€§åˆ†æ•° (10åˆ†)
        structure_issues = len(self.results.get('structure_verification', {}).get('issues', []))
        structure_score = max(0, 10 - structure_issues * 2)
        scores.append(structure_score)
        
        return sum(scores)
    
    def get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # ç¡®å®šåŸºç¡€è·¯å¾„
    if len(sys.argv) > 1:
        base_path = sys.argv[1]
    else:
        base_path = "."
    
    # åˆ›å»ºéªŒè¯å™¨å¹¶è¿è¡Œ
    verifier = QualityVerifier(base_path)
    verifier.run_full_verification()
    
    print("\nğŸ‰ è´¨é‡éªŒè¯å®Œæˆï¼")
    print(f"ğŸ“Š æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: RESTRUCTURE_WORKING/phase4_quality_verification_report.md")

if __name__ == "__main__":
    main() 