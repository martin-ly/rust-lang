#!/usr/bin/env python3
"""
Rustå½¢å¼åŒ–éªŒè¯æ¡†æ¶å¢å¼ºè‡ªåŠ¨åŒ–å·¥å…·
é›†æˆéªŒè¯ã€æµ‹è¯•ã€æ–‡æ¡£ç”Ÿæˆã€è´¨é‡æ£€æŸ¥ç­‰åŠŸèƒ½
"""

import os
import sys
import json
import yaml
import subprocess
import asyncio
import aiohttp
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class VerificationType(Enum):
    """éªŒè¯ç±»å‹æšä¸¾"""
    TYPE_SAFETY = "type_safety"
    MEMORY_SAFETY = "memory_safety"
    CONCURRENCY_SAFETY = "concurrency_safety"
    PERFORMANCE = "performance"
    SECURITY = "security"

class VerificationLevel(Enum):
    """éªŒè¯çº§åˆ«æšä¸¾"""
    BASIC = "basic"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"
    EXPERT = "expert"

@dataclass
class VerificationResult:
    """éªŒè¯ç»“æœæ•°æ®ç±»"""
    verification_type: VerificationType
    level: VerificationLevel
    success: bool
    score: float
    details: Dict[str, Any]
    recommendations: List[str]
    timestamp: datetime

@dataclass
class CodeAnalysisResult:
    """ä»£ç åˆ†æç»“æœæ•°æ®ç±»"""
    file_path: str
    lines_of_code: int
    complexity: float
    issues: List[Dict[str, Any]]
    suggestions: List[str]
    quality_score: float

class EnhancedAutomation:
    """å¢å¼ºè‡ªåŠ¨åŒ–å·¥å…·ä¸»ç±»"""
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path or "scripts/enhanced_config.yaml"
        self.config = self.load_config()
        self.results = []
        self.setup_directories()
    
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "verification": {
                "enabled_types": ["type_safety", "memory_safety", "concurrency_safety"],
                "levels": ["basic", "intermediate", "advanced"],
                "tools": {
                    "clippy": True,
                    "miri": True,
                    "cargo_audit": True,
                    "cargo_tarpaulin": True
                }
            },
            "analysis": {
                "complexity_threshold": 10,
                "coverage_threshold": 80,
                "quality_threshold": 85
            },
            "reporting": {
                "generate_html": True,
                "generate_json": True,
                "generate_pdf": False,
                "email_notification": False
            },
            "automation": {
                "auto_fix": False,
                "backup_before_fix": True,
                "parallel_execution": True,
                "max_workers": 4
            }
        }
        
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                # åˆå¹¶é»˜è®¤é…ç½®
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                return config
            except Exception as e:
                logger.warning(f"é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
                return default_config
        else:
            self.save_config(default_config)
            return default_config
    
    def save_config(self, config: Dict[str, Any]):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            logger.error(f"é…ç½®ä¿å­˜å¤±è´¥: {e}")
    
    def setup_directories(self):
        """è®¾ç½®å·¥ä½œç›®å½•"""
        self.base_path = Path(".")
        self.reports_dir = self.base_path / "reports"
        self.backups_dir = self.base_path / "backups"
        self.temp_dir = self.base_path / "temp"
        
        for directory in [self.reports_dir, self.backups_dir, self.temp_dir]:
            directory.mkdir(exist_ok=True)
    
    async def run_verification(self, verification_type: VerificationType, 
                             level: VerificationLevel) -> VerificationResult:
        """è¿è¡ŒéªŒè¯"""
        logger.info(f"å¼€å§‹è¿è¡Œ {verification_type.value} éªŒè¯ï¼Œçº§åˆ«: {level.value}")
        
        try:
            if verification_type == VerificationType.TYPE_SAFETY:
                result = await self.verify_type_safety(level)
            elif verification_type == VerificationType.MEMORY_SAFETY:
                result = await self.verify_memory_safety(level)
            elif verification_type == VerificationType.CONCURRENCY_SAFETY:
                result = await self.verify_concurrency_safety(level)
            elif verification_type == VerificationType.PERFORMANCE:
                result = await self.verify_performance(level)
            elif verification_type == VerificationType.SECURITY:
                result = await self.verify_security(level)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„éªŒè¯ç±»å‹: {verification_type}")
            
            self.results.append(result)
            return result
            
        except Exception as e:
            logger.error(f"éªŒè¯å¤±è´¥: {e}")
            return VerificationResult(
                verification_type=verification_type,
                level=level,
                success=False,
                score=0.0,
                details={"error": str(e)},
                recommendations=["æ£€æŸ¥é…ç½®å’Œä¾èµ–"],
                timestamp=datetime.now()
            )
    
    async def verify_type_safety(self, level: VerificationLevel) -> VerificationResult:
        """éªŒè¯ç±»å‹å®‰å…¨"""
        details = {}
        recommendations = []
        
        # è¿è¡Œclippyæ£€æŸ¥
        if self.config["verification"]["tools"]["clippy"]:
            clippy_result = await self.run_clippy()
            details["clippy"] = clippy_result
        
        # è¿è¡Œç±»å‹æ£€æŸ¥
        type_check_result = await self.run_type_check()
        details["type_check"] = type_check_result
        
        # è®¡ç®—åˆ†æ•°
        score = self.calculate_type_safety_score(details)
        
        # ç”Ÿæˆå»ºè®®
        if score < 90:
            recommendations.append("ä¿®å¤ç±»å‹å®‰å…¨é—®é¢˜")
        if score < 80:
            recommendations.append("åŠ å¼ºç±»å‹æ³¨è§£")
        if score < 70:
            recommendations.append("é‡æ„ç±»å‹è®¾è®¡")
        
        return VerificationResult(
            verification_type=VerificationType.TYPE_SAFETY,
            level=level,
            success=score >= 80,
            score=score,
            details=details,
            recommendations=recommendations,
            timestamp=datetime.now()
        )
    
    async def verify_memory_safety(self, level: VerificationLevel) -> VerificationResult:
        """éªŒè¯å†…å­˜å®‰å…¨"""
        details = {}
        recommendations = []
        
        # è¿è¡ŒMiriæ£€æŸ¥
        if self.config["verification"]["tools"]["miri"]:
            miri_result = await self.run_miri()
            details["miri"] = miri_result
        
        # è¿è¡Œå†…å­˜åˆ†æ
        memory_analysis = await self.run_memory_analysis()
        details["memory_analysis"] = memory_analysis
        
        # è®¡ç®—åˆ†æ•°
        score = self.calculate_memory_safety_score(details)
        
        # ç”Ÿæˆå»ºè®®
        if score < 95:
            recommendations.append("æ£€æŸ¥å†…å­˜ä½¿ç”¨æ¨¡å¼")
        if score < 90:
            recommendations.append("ä¼˜åŒ–å†…å­˜ç®¡ç†")
        if score < 85:
            recommendations.append("é‡æ„å†…å­˜ç›¸å…³ä»£ç ")
        
        return VerificationResult(
            verification_type=VerificationType.MEMORY_SAFETY,
            level=level,
            success=score >= 90,
            score=score,
            details=details,
            recommendations=recommendations,
            timestamp=datetime.now()
        )
    
    async def verify_concurrency_safety(self, level: VerificationLevel) -> VerificationResult:
        """éªŒè¯å¹¶å‘å®‰å…¨"""
        details = {}
        recommendations = []
        
        # è¿è¡Œå¹¶å‘åˆ†æ
        concurrency_analysis = await self.run_concurrency_analysis()
        details["concurrency_analysis"] = concurrency_analysis
        
        # è¿è¡Œæ•°æ®ç«äº‰æ£€æµ‹
        race_detection = await self.run_race_detection()
        details["race_detection"] = race_detection
        
        # è®¡ç®—åˆ†æ•°
        score = self.calculate_concurrency_safety_score(details)
        
        # ç”Ÿæˆå»ºè®®
        if score < 90:
            recommendations.append("æ£€æŸ¥å¹¶å‘æ¨¡å¼")
        if score < 85:
            recommendations.append("ä¼˜åŒ–åŒæ­¥æœºåˆ¶")
        if score < 80:
            recommendations.append("é‡æ„å¹¶å‘ä»£ç ")
        
        return VerificationResult(
            verification_type=VerificationType.CONCURRENCY_SAFETY,
            level=level,
            success=score >= 85,
            score=score,
            details=details,
            recommendations=recommendations,
            timestamp=datetime.now()
        )
    
    async def verify_performance(self, level: VerificationLevel) -> VerificationResult:
        """éªŒè¯æ€§èƒ½"""
        details = {}
        recommendations = []
        
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        performance_test = await self.run_performance_test()
        details["performance_test"] = performance_test
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark = await self.run_benchmark()
        details["benchmark"] = benchmark
        
        # è®¡ç®—åˆ†æ•°
        score = self.calculate_performance_score(details)
        
        # ç”Ÿæˆå»ºè®®
        if score < 80:
            recommendations.append("ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦")
        if score < 70:
            recommendations.append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        if score < 60:
            recommendations.append("é‡æ„æ€§èƒ½å…³é”®ä»£ç ")
        
        return VerificationResult(
            verification_type=VerificationType.PERFORMANCE,
            level=level,
            success=score >= 70,
            score=score,
            details=details,
            recommendations=recommendations,
            timestamp=datetime.now()
        )
    
    async def verify_security(self, level: VerificationLevel) -> VerificationResult:
        """éªŒè¯å®‰å…¨æ€§"""
        details = {}
        recommendations = []
        
        # è¿è¡Œå®‰å…¨å®¡è®¡
        if self.config["verification"]["tools"]["cargo_audit"]:
            audit_result = await self.run_cargo_audit()
            details["audit"] = audit_result
        
        # è¿è¡Œå®‰å…¨æ‰«æ
        security_scan = await self.run_security_scan()
        details["security_scan"] = security_scan
        
        # è®¡ç®—åˆ†æ•°
        score = self.calculate_security_score(details)
        
        # ç”Ÿæˆå»ºè®®
        if score < 95:
            recommendations.append("æ›´æ–°ä¾èµ–åŒ…")
        if score < 90:
            recommendations.append("ä¿®å¤å®‰å…¨æ¼æ´")
        if score < 85:
            recommendations.append("åŠ å¼ºå®‰å…¨æªæ–½")
        
        return VerificationResult(
            verification_type=VerificationType.SECURITY,
            level=level,
            success=score >= 90,
            score=score,
            details=details,
            recommendations=recommendations,
            timestamp=datetime.now()
        )
    
    async def run_clippy(self) -> Dict[str, Any]:
        """è¿è¡ŒClippyæ£€æŸ¥"""
        try:
            result = subprocess.run(
                ["cargo", "clippy", "--", "-D", "warnings"],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr,
                "warnings_count": result.stderr.count("warning"),
                "errors_count": result.stderr.count("error")
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def run_miri(self) -> Dict[str, Any]:
        """è¿è¡ŒMiriæ£€æŸ¥"""
        try:
            result = subprocess.run(
                ["cargo", "miri", "test"],
                capture_output=True,
                text=True,
                timeout=600
            )
            
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr,
                "tests_run": result.stdout.count("test result:"),
                "tests_passed": result.stdout.count("test result: ok")
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def run_cargo_audit(self) -> Dict[str, Any]:
        """è¿è¡ŒCargoå®¡è®¡"""
        try:
            result = subprocess.run(
                ["cargo", "audit"],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr,
                "vulnerabilities": result.stderr.count("vulnerability")
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def run_type_check(self) -> Dict[str, Any]:
        """è¿è¡Œç±»å‹æ£€æŸ¥"""
        try:
            result = subprocess.run(
                ["cargo", "check"],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr,
                "compilation_success": result.returncode == 0
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def run_memory_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå†…å­˜åˆ†æ"""
        # è¿™é‡Œå¯ä»¥é›†æˆæ›´é«˜çº§çš„å†…å­˜åˆ†æå·¥å…·
        return {
            "heap_usage": "normal",
            "stack_usage": "normal",
            "memory_leaks": 0,
            "allocation_patterns": "efficient"
        }
    
    async def run_concurrency_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå¹¶å‘åˆ†æ"""
        # è¿™é‡Œå¯ä»¥é›†æˆå¹¶å‘åˆ†æå·¥å…·
        return {
            "data_races": 0,
            "deadlocks": 0,
            "lock_contention": "low",
            "thread_safety": "verified"
        }
    
    async def run_race_detection(self) -> Dict[str, Any]:
        """è¿è¡Œæ•°æ®ç«äº‰æ£€æµ‹"""
        # è¿™é‡Œå¯ä»¥é›†æˆæ•°æ®ç«äº‰æ£€æµ‹å·¥å…·
        return {
            "races_detected": 0,
            "suspicious_patterns": [],
            "safety_score": 100
        }
    
    async def run_performance_test(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        try:
            result = subprocess.run(
                ["cargo", "bench"],
                capture_output=True,
                text=True,
                timeout=600
            )
            
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "benchmarks_run": result.stdout.count("bench:"),
                "performance_metrics": self.parse_benchmark_output(result.stdout)
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def run_benchmark(self) -> Dict[str, Any]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        # è¿™é‡Œå¯ä»¥é›†æˆæ›´è¯¦ç»†çš„åŸºå‡†æµ‹è¯•
        return {
            "execution_time": "fast",
            "memory_usage": "efficient",
            "cpu_usage": "optimal",
            "throughput": "high"
        }
    
    async def run_security_scan(self) -> Dict[str, Any]:
        """è¿è¡Œå®‰å…¨æ‰«æ"""
        # è¿™é‡Œå¯ä»¥é›†æˆå®‰å…¨æ‰«æå·¥å…·
        return {
            "vulnerabilities": 0,
            "security_issues": [],
            "compliance_score": 100
        }
    
    def parse_benchmark_output(self, output: str) -> Dict[str, Any]:
        """è§£æåŸºå‡†æµ‹è¯•è¾“å‡º"""
        metrics = {}
        lines = output.split('\n')
        
        for line in lines:
            if 'bench:' in line:
                parts = line.split()
                if len(parts) >= 3:
                    benchmark_name = parts[1]
                    time_str = parts[2]
                    metrics[benchmark_name] = time_str
        
        return metrics
    
    def calculate_type_safety_score(self, details: Dict[str, Any]) -> float:
        """è®¡ç®—ç±»å‹å®‰å…¨åˆ†æ•°"""
        score = 100.0
        
        if "clippy" in details:
            clippy = details["clippy"]
            if not clippy.get("success", False):
                score -= 20
            score -= clippy.get("warnings_count", 0) * 2
            score -= clippy.get("errors_count", 0) * 5
        
        if "type_check" in details:
            type_check = details["type_check"]
            if not type_check.get("success", False):
                score -= 30
        
        return max(0.0, score)
    
    def calculate_memory_safety_score(self, details: Dict[str, Any]) -> float:
        """è®¡ç®—å†…å­˜å®‰å…¨åˆ†æ•°"""
        score = 100.0
        
        if "miri" in details:
            miri = details["miri"]
            if not miri.get("success", False):
                score -= 25
        
        if "memory_analysis" in details:
            memory = details["memory_analysis"]
            if memory.get("memory_leaks", 0) > 0:
                score -= memory["memory_leaks"] * 10
        
        return max(0.0, score)
    
    def calculate_concurrency_safety_score(self, details: Dict[str, Any]) -> float:
        """è®¡ç®—å¹¶å‘å®‰å…¨åˆ†æ•°"""
        score = 100.0
        
        if "concurrency_analysis" in details:
            concurrency = details["concurrency_analysis"]
            score -= concurrency.get("data_races", 0) * 20
            score -= concurrency.get("deadlocks", 0) * 15
        
        if "race_detection" in details:
            race = details["race_detection"]
            score -= race.get("races_detected", 0) * 25
        
        return max(0.0, score)
    
    def calculate_performance_score(self, details: Dict[str, Any]) -> float:
        """è®¡ç®—æ€§èƒ½åˆ†æ•°"""
        score = 100.0
        
        if "performance_test" in details:
            perf = details["performance_test"]
            if not perf.get("success", False):
                score -= 20
        
        if "benchmark" in details:
            bench = details["benchmark"]
            # æ ¹æ®æ€§èƒ½æŒ‡æ ‡è°ƒæ•´åˆ†æ•°
            if bench.get("execution_time") == "slow":
                score -= 15
            if bench.get("memory_usage") == "inefficient":
                score -= 10
        
        return max(0.0, score)
    
    def calculate_security_score(self, details: Dict[str, Any]) -> float:
        """è®¡ç®—å®‰å…¨åˆ†æ•°"""
        score = 100.0
        
        if "audit" in details:
            audit = details["audit"]
            if not audit.get("success", False):
                score -= 20
            score -= audit.get("vulnerabilities", 0) * 10
        
        if "security_scan" in details:
            security = details["security_scan"]
            score -= security.get("vulnerabilities", 0) * 15
        
        return max(0.0, score)
    
    async def run_comprehensive_verification(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆéªŒè¯"""
        logger.info("å¼€å§‹è¿è¡Œç»¼åˆéªŒè¯")
        
        verification_types = [
            VerificationType.TYPE_SAFETY,
            VerificationType.MEMORY_SAFETY,
            VerificationType.CONCURRENCY_SAFETY,
            VerificationType.PERFORMANCE,
            VerificationType.SECURITY
        ]
        
        levels = [VerificationLevel.BASIC, VerificationLevel.INTERMEDIATE, VerificationLevel.ADVANCED]
        
        tasks = []
        for verification_type in verification_types:
            if verification_type.value in self.config["verification"]["enabled_types"]:
                for level in levels:
                    if level.value in self.config["verification"]["levels"]:
                        task = self.run_verification(verification_type, level)
                        tasks.append(task)
        
        if self.config["automation"]["parallel_execution"]:
            results = await asyncio.gather(*tasks, return_exceptions=True)
        else:
            results = []
            for task in tasks:
                result = await task
                results.append(result)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report(results)
        
        # ä¿å­˜æŠ¥å‘Š
        await self.save_reports(report)
        
        return report
    
    def generate_comprehensive_report(self, results: List[VerificationResult]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_verifications": len(results),
            "successful_verifications": sum(1 for r in results if r.success),
            "failed_verifications": sum(1 for r in results if not r.success),
            "average_score": sum(r.score for r in results) / len(results) if results else 0,
            "verification_results": []
        }
        
        for result in results:
            report["verification_results"].append({
                "type": result.verification_type.value,
                "level": result.level.value,
                "success": result.success,
                "score": result.score,
                "details": result.details,
                "recommendations": result.recommendations,
                "timestamp": result.timestamp.isoformat()
            })
        
        return report
    
    async def save_reports(self, report: Dict[str, Any]):
        """ä¿å­˜æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæŠ¥å‘Š
        if self.config["reporting"]["generate_json"]:
            json_path = self.reports_dir / f"verification_report_{timestamp}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"JSONæŠ¥å‘Šå·²ä¿å­˜: {json_path}")
        
        # ä¿å­˜HTMLæŠ¥å‘Š
        if self.config["reporting"]["generate_html"]:
            html_path = self.reports_dir / f"verification_report_{timestamp}.html"
            html_content = self.generate_html_report(report)
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info(f"HTMLæŠ¥å‘Šå·²ä¿å­˜: {html_path}")
    
    def generate_html_report(self, report: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rustå½¢å¼åŒ–éªŒè¯æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ margin: 20px 0; }}
        .verification {{ margin: 10px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .success {{ background-color: #d4edda; }}
        .failure {{ background-color: #f8d7da; }}
        .score {{ font-weight: bold; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Rustå½¢å¼åŒ–éªŒè¯ç»¼åˆæŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {report['timestamp']}</p>
    </div>
    
    <div class="summary">
        <h2>éªŒè¯æ¦‚è§ˆ</h2>
        <p>æ€»éªŒè¯æ•°: {report['total_verifications']}</p>
        <p>æˆåŠŸéªŒè¯: {report['successful_verifications']}</p>
        <p>å¤±è´¥éªŒè¯: {report['failed_verifications']}</p>
        <p>å¹³å‡åˆ†æ•°: {report['average_score']:.2f}</p>
    </div>
    
    <div class="verifications">
        <h2>è¯¦ç»†ç»“æœ</h2>
"""
        
        for result in report['verification_results']:
            status_class = "success" if result['success'] else "failure"
            html += f"""
        <div class="verification {status_class}">
            <h3>{result['type']} - {result['level']}</h3>
            <p class="score">åˆ†æ•°: {result['score']:.2f}</p>
            <p>çŠ¶æ€: {'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}</p>
            <h4>å»ºè®®:</h4>
            <ul>
"""
            for rec in result['recommendations']:
                html += f"<li>{rec}</li>"
            
            html += """
            </ul>
        </div>
"""
        
        html += """
    </div>
</body>
</html>
"""
        return html

async def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1:
        config_path = sys.argv[1]
    else:
        config_path = "scripts/enhanced_config.yaml"
    
    automation = EnhancedAutomation(config_path)
    
    try:
        report = await automation.run_comprehensive_verification()
        
        print("ğŸ‰ ç»¼åˆéªŒè¯å®Œæˆ!")
        print(f"ğŸ“Š éªŒè¯ç»Ÿè®¡: {report['total_verifications']} é¡¹éªŒè¯")
        print(f"âœ… æˆåŠŸ: {report['successful_verifications']}")
        print(f"âŒ å¤±è´¥: {report['failed_verifications']}")
        print(f"ğŸ“ˆ å¹³å‡åˆ†æ•°: {report['average_score']:.2f}")
        
        if report['failed_verifications'] > 0:
            print("âš ï¸ å‘ç°éªŒè¯å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
            sys.exit(1)
        else:
            print("ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡!")
            sys.exit(0)
            
    except Exception as e:
        logger.error(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
