#!/usr/bin/env python3
"""
Rustå½¢å¼åŒ–éªŒè¯æ¡†æ¶ç»¼åˆè‡ªåŠ¨åŒ–ç®¡ç†è„šæœ¬
é›†æˆéªŒè¯ã€è´¨é‡æ£€æŸ¥ã€æ–‡æ¡£ç”Ÿæˆã€æŠ¥å‘Šç”Ÿæˆç­‰åŠŸèƒ½
"""

import os
import sys
import subprocess
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import re
import shutil

class FrameworkAutomation:
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.scripts_dir = self.base_path / 'scripts'
        self.reports_dir = self.base_path / 'reports'
        self.config_file = self.scripts_dir / 'framework_config.yaml'
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.reports_dir.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.config = self.load_config()
    
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½æ¡†æ¶é…ç½®"""
        default_config = {
            "framework": {
                "version": "2.0",
                "name": "Rust Formal Verification Framework",
                "description": "Complete formal verification framework for Rust",
                "maintainer": "Rust Formal Verification Team"
            },
            "verification": {
                "enabled": True,
                "strict_mode": False,
                "check_types": True,
                "check_memory": True,
                "check_concurrency": True,
                "check_performance": True
            },
            "documentation": {
                "enabled": True,
                "auto_generate": True,
                "include_examples": True,
                "include_proofs": True,
                "format": ["markdown", "html", "pdf"]
            },
            "quality": {
                "enabled": True,
                "coverage_threshold": 90,
                "accuracy_threshold": 95,
                "completeness_threshold": 85,
                "performance_threshold": 90
            },
            "automation": {
                "enabled": True,
                "schedule": "daily",
                "backup": True,
                "notification": True,
                "reporting": True
            },
            "tools": {
                "clippy": {"enabled": True, "strict": True},
                "miri": {"enabled": True, "timeout": 600},
                "coq": {"enabled": True, "timeout": 300},
                "lean": {"enabled": True, "timeout": 300},
                "rustdoc": {"enabled": True, "private": True}
            }
        }
        
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                # åˆå¹¶é»˜è®¤é…ç½®
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                return config
            except Exception as e:
                print(f"é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
                return default_config
        else:
            # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
            self.save_config(default_config)
            return default_config
    
    def save_config(self, config: Dict[str, Any]):
        """ä¿å­˜é…ç½®"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            print(f"é…ç½®ä¿å­˜å¤±è´¥: {e}")
    
    def run_verification(self) -> Dict[str, Any]:
        """è¿è¡ŒéªŒè¯æµç¨‹"""
        print("ğŸ” è¿è¡ŒéªŒè¯æµç¨‹...")
        
        if not self.config['verification']['enabled']:
            return {'success': True, 'message': 'éªŒè¯å·²ç¦ç”¨'}
        
        results = {}
        
        # è¿è¡ŒClippyæ£€æŸ¥
        if self.config['tools']['clippy']['enabled']:
            results['clippy'] = self.run_clippy()
        
        # è¿è¡ŒMIRIæ£€æŸ¥
        if self.config['tools']['miri']['enabled']:
            results['miri'] = self.run_miri()
        
        # è¿è¡ŒCoqéªŒè¯
        if self.config['tools']['coq']['enabled']:
            results['coq'] = self.run_coq()
        
        # è¿è¡ŒLeanéªŒè¯
        if self.config['tools']['lean']['enabled']:
            results['lean'] = self.run_lean()
        
        # è¯„ä¼°æ•´ä½“ç»“æœ
        all_success = all(r.get('success', False) for r in results.values())
        
        return {
            'success': all_success,
            'results': results,
            'total_tools': len(results),
            'successful_tools': sum(1 for r in results.values() if r.get('success', False))
        }
    
    def run_clippy(self) -> Dict[str, Any]:
        """è¿è¡ŒClippyæ£€æŸ¥"""
        try:
            cmd = ['cargo', 'clippy']
            if self.config['tools']['clippy']['strict']:
                cmd.extend(['--', '-D', 'warnings'])
            
            result = subprocess.run(
                cmd,
                cwd=self.base_path,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                'success': result.returncode == 0,
                'output': result.stdout,
                'error': result.stderr,
                'exit_code': result.returncode
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'Clippyè¿è¡Œé”™è¯¯: {e}',
                'exit_code': -1
            }
    
    def run_miri(self) -> Dict[str, Any]:
        """è¿è¡ŒMIRIæ£€æŸ¥"""
        try:
            cmd = ['cargo', 'miri', 'test']
            result = subprocess.run(
                cmd,
                cwd=self.base_path,
                capture_output=True,
                text=True,
                timeout=self.config['tools']['miri']['timeout']
            )
            
            return {
                'success': result.returncode == 0,
                'output': result.stdout,
                'error': result.stderr,
                'exit_code': result.returncode
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'MIRIè¿è¡Œé”™è¯¯: {e}',
                'exit_code': -1
            }
    
    def run_coq(self) -> Dict[str, Any]:
        """è¿è¡ŒCoqéªŒè¯"""
        proofs_dir = self.base_path / 'proofs' / 'coq'
        if not proofs_dir.exists():
            return {'success': True, 'message': 'Coqè¯æ˜ç›®å½•ä¸å­˜åœ¨'}
        
        try:
            results = []
            for proof_file in proofs_dir.glob('*.v'):
                cmd = ['coqc', str(proof_file)]
                result = subprocess.run(
                    cmd,
                    cwd=proofs_dir,
                    capture_output=True,
                    text=True,
                    timeout=self.config['tools']['coq']['timeout']
                )
                
                results.append({
                    'file': proof_file.name,
                    'success': result.returncode == 0,
                    'output': result.stdout,
                    'error': result.stderr
                })
            
            overall_success = all(r['success'] for r in results)
            return {
                'success': overall_success,
                'results': results,
                'total_files': len(results),
                'successful_files': sum(1 for r in results if r['success'])
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'Coqè¿è¡Œé”™è¯¯: {e}',
                'results': []
            }
    
    def run_lean(self) -> Dict[str, Any]:
        """è¿è¡ŒLeanéªŒè¯"""
        proofs_dir = self.base_path / 'proofs' / 'lean'
        if not proofs_dir.exists():
            return {'success': True, 'message': 'Leanè¯æ˜ç›®å½•ä¸å­˜åœ¨'}
        
        try:
            results = []
            for proof_file in proofs_dir.glob('*.lean'):
                cmd = ['lean', str(proof_file)]
                result = subprocess.run(
                    cmd,
                    cwd=proofs_dir,
                    capture_output=True,
                    text=True,
                    timeout=self.config['tools']['lean']['timeout']
                )
                
                results.append({
                    'file': proof_file.name,
                    'success': result.returncode == 0,
                    'output': result.stdout,
                    'error': result.stderr
                })
            
            overall_success = all(r['success'] for r in results)
            return {
                'success': overall_success,
                'results': results,
                'total_files': len(results),
                'successful_files': sum(1 for r in results if r['success'])
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'Leanè¿è¡Œé”™è¯¯: {e}',
                'results': []
            }
    
    def generate_documentation(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æ¡£"""
        print("ğŸ“š ç”Ÿæˆæ–‡æ¡£...")
        
        if not self.config['documentation']['enabled']:
            return {'success': True, 'message': 'æ–‡æ¡£ç”Ÿæˆå·²ç¦ç”¨'}
        
        try:
            # ç”ŸæˆRustæ–‡æ¡£
            if self.config['tools']['rustdoc']['enabled']:
                cmd = ['cargo', 'doc']
                if self.config['tools']['rustdoc']['private']:
                    cmd.append('--document-private-items')
                
                result = subprocess.run(
                    cmd,
                    cwd=self.base_path,
                    capture_output=True,
                    text=True,
                    timeout=300
                )
                
                if result.returncode != 0:
                    return {
                        'success': False,
                        'error': f'æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {result.stderr}',
                        'exit_code': result.returncode
                    }
            
            # ç”Ÿæˆå…¶ä»–æ ¼å¼æ–‡æ¡£
            formats = self.config['documentation']['format']
            generated_formats = []
            
            for format_type in formats:
                if format_type == 'html':
                    # HTMLæ–‡æ¡£å·²é€šè¿‡rustdocç”Ÿæˆ
                    generated_formats.append('html')
                elif format_type == 'markdown':
                    # Markdownæ–‡æ¡£å·²å­˜åœ¨
                    generated_formats.append('markdown')
                elif format_type == 'pdf':
                    # PDFç”Ÿæˆéœ€è¦é¢å¤–å·¥å…·
                    generated_formats.append('pdf')
            
            return {
                'success': True,
                'generated_formats': generated_formats,
                'message': f'æ–‡æ¡£ç”ŸæˆæˆåŠŸï¼Œæ ¼å¼: {", ".join(generated_formats)}'
            }
        
        except Exception as e:
            return {
                'success': False,
                'error': f'æ–‡æ¡£ç”Ÿæˆé”™è¯¯: {e}',
                'exit_code': -1
            }
    
    def assess_quality(self, verification_results: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°è´¨é‡"""
        print("ğŸ“Š è¯„ä¼°è´¨é‡...")
        
        if not self.config['quality']['enabled']:
            return {'success': True, 'message': 'è´¨é‡è¯„ä¼°å·²ç¦ç”¨'}
        
        quality_metrics = {
            'verification_coverage': 0,
            'tool_success_rate': 0,
            'documentation_completeness': 0,
            'overall_quality': 0
        }
        
        # è®¡ç®—éªŒè¯è¦†ç›–ç‡
        if verification_results.get('success'):
            quality_metrics['verification_coverage'] = 100
        else:
            successful_tools = verification_results.get('successful_tools', 0)
            total_tools = verification_results.get('total_tools', 1)
            quality_metrics['verification_coverage'] = (successful_tools / total_tools) * 100
        
        # è®¡ç®—å·¥å…·æˆåŠŸç‡
        quality_metrics['tool_success_rate'] = quality_metrics['verification_coverage']
        
        # è®¡ç®—æ–‡æ¡£å®Œæ•´æ€§
        doc_files = list(self.base_path.rglob('*.md'))
        required_files = [
            'README.md',
            '00_MASTER_NAVIGATION.md',
            'LEARNING_PATHS.md',
            'VERIFICATION_GUIDE.md'
        ]
        
        existing_files = sum(1 for req_file in required_files 
                           if (self.base_path / req_file).exists())
        quality_metrics['documentation_completeness'] = (existing_files / len(required_files)) * 100
        
        # è®¡ç®—æ€»ä½“è´¨é‡
        quality_metrics['overall_quality'] = sum(quality_metrics.values()) / len(quality_metrics)
        
        # æ£€æŸ¥è´¨é‡é˜ˆå€¼
        thresholds = {
            'coverage': self.config['quality']['coverage_threshold'],
            'accuracy': self.config['quality']['accuracy_threshold'],
            'completeness': self.config['quality']['completeness_threshold'],
            'performance': self.config['quality']['performance_threshold']
        }
        
        quality_gates = {
            'coverage_gate': quality_metrics['verification_coverage'] >= thresholds['coverage'],
            'accuracy_gate': quality_metrics['tool_success_rate'] >= thresholds['accuracy'],
            'completeness_gate': quality_metrics['documentation_completeness'] >= thresholds['completeness'],
            'performance_gate': quality_metrics['overall_quality'] >= thresholds['performance']
        }
        
        all_gates_passed = all(quality_gates.values())
        
        return {
            'success': all_gates_passed,
            'metrics': quality_metrics,
            'thresholds': thresholds,
            'gates': quality_gates,
            'overall_quality': quality_metrics['overall_quality']
        }
    
    def generate_comprehensive_report(self, verification_results: Dict[str, Any], 
                                    doc_results: Dict[str, Any], 
                                    quality_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""# Rustå½¢å¼åŒ–éªŒè¯æ¡†æ¶ç»¼åˆæŠ¥å‘Š

## ğŸ“Š æ¡†æ¶æ¦‚è§ˆ

- **æ¡†æ¶åç§°**: {self.config['framework']['name']}
- **æ¡†æ¶ç‰ˆæœ¬**: {self.config['framework']['version']}
- **æŠ¥å‘Šæ—¶é—´**: {timestamp}
- **æ¡†æ¶è·¯å¾„**: {self.base_path}
- **æ•´ä½“çŠ¶æ€**: {'âœ… ä¼˜ç§€' if quality_results.get('overall_quality', 0) >= 90 else 'âš ï¸ éœ€è¦æ”¹è¿›' if quality_results.get('overall_quality', 0) >= 80 else 'âŒ éœ€è¦ä¿®å¤'}

## ğŸ” éªŒè¯ç»“æœ

### éªŒè¯æ¦‚è§ˆ
- **éªŒè¯çŠ¶æ€**: {'âœ… æˆåŠŸ' if verification_results.get('success') else 'âŒ å¤±è´¥'}
- **å·¥å…·æ€»æ•°**: {verification_results.get('total_tools', 0)}
- **æˆåŠŸå·¥å…·**: {verification_results.get('successful_tools', 0)}

### è¯¦ç»†ç»“æœ
"""
        
        # æ·»åŠ éªŒè¯å·¥å…·ç»“æœ
        for tool, result in verification_results.get('results', {}).items():
            status = 'âœ… é€šè¿‡' if result.get('success') else 'âŒ å¤±è´¥'
            report += f"- **{tool.upper()}**: {status}\n"
        
        report += f"""
## ğŸ“š æ–‡æ¡£ç”Ÿæˆ

### æ–‡æ¡£çŠ¶æ€
- **ç”ŸæˆçŠ¶æ€**: {'âœ… æˆåŠŸ' if doc_results.get('success') else 'âŒ å¤±è´¥'}
- **ç”Ÿæˆæ ¼å¼**: {', '.join(doc_results.get('generated_formats', []))}
- **è¯¦ç»†ä¿¡æ¯**: {doc_results.get('message', 'æœªæ‰§è¡Œ')}

## ğŸ“ˆ è´¨é‡è¯„ä¼°

### è´¨é‡æŒ‡æ ‡
- **éªŒè¯è¦†ç›–ç‡**: {quality_results.get('metrics', {}).get('verification_coverage', 0):.1f}%
- **å·¥å…·æˆåŠŸç‡**: {quality_results.get('metrics', {}).get('tool_success_rate', 0):.1f}%
- **æ–‡æ¡£å®Œæ•´æ€§**: {quality_results.get('metrics', {}).get('documentation_completeness', 0):.1f}%
- **æ€»ä½“è´¨é‡**: {quality_results.get('overall_quality', 0):.1f}/100

### è´¨é‡é—¨ç¦
"""
        
        # æ·»åŠ è´¨é‡é—¨ç¦ç»“æœ
        gates = quality_results.get('gates', {})
        for gate_name, passed in gates.items():
            status = 'âœ… é€šè¿‡' if passed else 'âŒ æœªé€šè¿‡'
            report += f"- **{gate_name}**: {status}\n"
        
        report += f"""
## ğŸ¯ æ”¹è¿›å»ºè®®

### çŸ­æœŸæ”¹è¿›
1. **éªŒè¯ä¼˜åŒ–**: æé«˜éªŒè¯å·¥å…·çš„æˆåŠŸç‡
2. **æ–‡æ¡£å®Œå–„**: è¡¥å……ç¼ºå¤±çš„æ–‡æ¡£å†…å®¹
3. **è´¨é‡æå‡**: æå‡æ•´ä½“è´¨é‡æŒ‡æ ‡

### é•¿æœŸæ”¹è¿›
1. **å·¥å…·æ‰©å±•**: å¢åŠ æ›´å¤šéªŒè¯å·¥å…·
2. **è‡ªåŠ¨åŒ–æå‡**: æé«˜è‡ªåŠ¨åŒ–ç¨‹åº¦
3. **æ ‡å‡†åˆ¶å®š**: åˆ¶å®šæ›´ä¸¥æ ¼çš„è´¨é‡æ ‡å‡†

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### é—®é¢˜åé¦ˆ
- **GitHub Issues**: é€šè¿‡GitHub Issuesæäº¤é—®é¢˜
- **æŠ€æœ¯è®¨è®º**: å‚ä¸æŠ€æœ¯è®¨è®ºå’Œé—®é¢˜è§£å†³
- **è´¡çŒ®å‚ä¸**: æ¬¢è¿æäº¤PRå’Œå‚ä¸å¼€å‘

### æ–‡æ¡£ç»´æŠ¤
- **å®šæœŸæ›´æ–°**: æ¯æœˆæ›´æ–°æ¡†æ¶å’Œæ–‡æ¡£
- **è´¨é‡æ£€æŸ¥**: æ¯å‘¨è¿è¡Œè‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥
- **ç”¨æˆ·åé¦ˆ**: åŠæ—¶å“åº”ç”¨æˆ·åé¦ˆå’Œå»ºè®®

---
> æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}
> æ¡†æ¶ç‰ˆæœ¬: {self.config['framework']['version']}
> è´¨é‡ç­‰çº§: {'ä¼˜ç§€' if quality_results.get('overall_quality', 0) >= 90 else 'è‰¯å¥½' if quality_results.get('overall_quality', 0) >= 80 else 'éœ€è¦æ”¹è¿›'}
"""
        
        return report
    
    def save_report(self, report: str, report_type: str = 'comprehensive'):
        """ä¿å­˜æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.reports_dir / f"{report_type}_report_{timestamp}.md"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
    
    def run_full_automation(self) -> bool:
        """è¿è¡Œå®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹"""
        print("ğŸš€ å¼€å§‹å®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹...")
        
        # 1. è¿è¡ŒéªŒè¯
        verification_results = self.run_verification()
        
        # 2. ç”Ÿæˆæ–‡æ¡£
        doc_results = self.generate_documentation()
        
        # 3. è¯„ä¼°è´¨é‡
        quality_results = self.assess_quality(verification_results)
        
        # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report(verification_results, doc_results, quality_results)
        self.save_report(report)
        
        # 5. æ£€æŸ¥æ•´ä½“çŠ¶æ€
        overall_success = (
            verification_results.get('success', False) and
            doc_results.get('success', False) and
            quality_results.get('success', False)
        )
        
        if overall_success:
            print("ğŸ‰ å®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹æˆåŠŸå®Œæˆ!")
        else:
            print("âš ï¸ è‡ªåŠ¨åŒ–æµç¨‹éƒ¨åˆ†å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Š")
        
        return overall_success

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python framework_automation.py [full|verify|doc|quality] [path]")
        print("  full    - è¿è¡Œå®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹")
        print("  verify  - åªè¿è¡ŒéªŒè¯æµç¨‹")
        print("  doc     - åªç”Ÿæˆæ–‡æ¡£")
        print("  quality - åªè¯„ä¼°è´¨é‡")
        sys.exit(1)
    
    mode = sys.argv[1]
    base_path = sys.argv[2] if len(sys.argv) > 2 else os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    automation = FrameworkAutomation(base_path)
    
    if mode == 'full':
        success = automation.run_full_automation()
    elif mode == 'verify':
        results = automation.run_verification()
        success = results.get('success', False)
    elif mode == 'doc':
        results = automation.generate_documentation()
        success = results.get('success', False)
    elif mode == 'quality':
        verification_results = automation.run_verification()
        results = automation.assess_quality(verification_results)
        success = results.get('success', False)
    else:
        print(f"æœªçŸ¥æ¨¡å¼: {mode}")
        sys.exit(1)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
