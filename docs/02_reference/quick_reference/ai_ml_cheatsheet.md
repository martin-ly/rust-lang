# ğŸ¤– Rust AI/ML é€ŸæŸ¥å¡ {#-rust-aiml-é€ŸæŸ¥å¡}

> **å¿«é€Ÿå‚è€ƒ** | [AI+Rust ç”Ÿæ€æŒ‡å—](../../05_guides/AI_RUST_ECOSYSTEM_GUIDE.md) | [AI è¾…åŠ©ç¼–ç¨‹](../../../guides/AI_ASSISTED_RUST_PROGRAMMING_GUIDE_2025.md)
> **åˆ›å»ºæ—¥æœŸ**: 2026-02-13
> **æœ€åæ›´æ–°**: 2026-02-13
> **Rust ç‰ˆæœ¬**: 1.93.1+ (Edition 2024)
> **çŠ¶æ€**: âœ… å·²å®Œæˆ

---

## ğŸ“‹ ç›®å½• {#-ç›®å½•}

- [ğŸ¤– Rust AI/ML é€ŸæŸ¥å¡ {#-rust-aiml-é€ŸæŸ¥å¡}](#-rust-aiml-é€ŸæŸ¥å¡--rust-aiml-é€ŸæŸ¥å¡)
  - [ğŸ“‹ ç›®å½• {#-ç›®å½•}](#-ç›®å½•--ç›®å½•)
  - [æ¡†æ¶é€‰å‹](#æ¡†æ¶é€‰å‹)
  - [Burn å¿«é€Ÿå…¥é—¨](#burn-å¿«é€Ÿå…¥é—¨)
    - [ç¤ºä¾‹ 1: å¼ é‡åŸºç¡€æ“ä½œ](#ç¤ºä¾‹-1-å¼ é‡åŸºç¡€æ“ä½œ)
    - [ç¤ºä¾‹ 2: ç®€å•ç¥ç»ç½‘ç»œ](#ç¤ºä¾‹-2-ç®€å•ç¥ç»ç½‘ç»œ)
    - [ç¤ºä¾‹ 3: æ¨¡å‹æ¨ç†](#ç¤ºä¾‹-3-æ¨¡å‹æ¨ç†)
  - [Candle å¿«é€Ÿå…¥é—¨](#candle-å¿«é€Ÿå…¥é—¨)
    - [ç¤ºä¾‹ 4: å¼ é‡æ“ä½œ](#ç¤ºä¾‹-4-å¼ é‡æ“ä½œ)
    - [ç¤ºä¾‹ 5: åŠ è½½ Hugging Face æ¨¡å‹](#ç¤ºä¾‹-5-åŠ è½½-hugging-face-æ¨¡å‹)
  - [LLM æ¨ç†](#llm-æ¨ç†)
    - [ç¤ºä¾‹ 6: ä½¿ç”¨ llm crate è¿›è¡Œæœ¬åœ°æ¨ç†](#ç¤ºä¾‹-6-ä½¿ç”¨-llm-crate-è¿›è¡Œæœ¬åœ°æ¨ç†)
    - [æ¡†æ¶é€‰å‹è¡¨](#æ¡†æ¶é€‰å‹è¡¨)
  - [ä¸ C01â€“C12 å…³è”](#ä¸-c01c12-å…³è”)
  - [ğŸ¯ ä½¿ç”¨åœºæ™¯ {#-ä½¿ç”¨åœºæ™¯}](#-ä½¿ç”¨åœºæ™¯--ä½¿ç”¨åœºæ™¯)
    - [åœºæ™¯ 1: å›¾åƒåˆ†ç±»æœåŠ¡](#åœºæ™¯-1-å›¾åƒåˆ†ç±»æœåŠ¡)
    - [åœºæ™¯ 2: å®æ—¶æ–‡æœ¬ç”Ÿæˆ](#åœºæ™¯-2-å®æ—¶æ–‡æœ¬ç”Ÿæˆ)
  - [ğŸ“ å½¢å¼åŒ–æ–¹æ³•é“¾æ¥ {#-å½¢å¼åŒ–æ–¹æ³•é“¾æ¥}](#-å½¢å¼åŒ–æ–¹æ³•é“¾æ¥--å½¢å¼åŒ–æ–¹æ³•é“¾æ¥)
    - [ç†è®ºåŸºç¡€](#ç†è®ºåŸºç¡€)
    - [å½¢å¼åŒ–å®šç†](#å½¢å¼åŒ–å®šç†)
  - [ğŸš« åä¾‹é€ŸæŸ¥ {#-åä¾‹é€ŸæŸ¥}](#-åä¾‹é€ŸæŸ¥--åä¾‹é€ŸæŸ¥)
    - [åä¾‹ 1: æ··æ·†ä¸åŒæ¡†æ¶çš„ API](#åä¾‹-1-æ··æ·†ä¸åŒæ¡†æ¶çš„-api)
    - [åä¾‹ 2: æœªæ ¹æ®åœºæ™¯é€‰æ‹©åç«¯](#åä¾‹-2-æœªæ ¹æ®åœºæ™¯é€‰æ‹©åç«¯)
    - [åä¾‹ 3: å¿½ç•¥ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§](#åä¾‹-3-å¿½ç•¥ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§)
    - [åä¾‹ 4: å†…å­˜æ³„æ¼ - å¾ªç¯å¼•ç”¨å¼ é‡ç¼“å­˜](#åä¾‹-4-å†…å­˜æ³„æ¼---å¾ªç¯å¼•ç”¨å¼ é‡ç¼“å­˜)
    - [åä¾‹ 5: è¾¹ç•Œæƒ…å†µ - ç©ºå¼ é‡æ“ä½œ](#åä¾‹-5-è¾¹ç•Œæƒ…å†µ---ç©ºå¼ é‡æ“ä½œ)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)
  - [ç›¸å…³ç¤ºä¾‹ä»£ç ](#ç›¸å…³ç¤ºä¾‹ä»£ç )

---

## æ¡†æ¶é€‰å‹

| æ¡†æ¶ | é€‚ç”¨åœºæ™¯ | ä¾èµ– |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Candle** | ç®€æ´ APIã€Hugging Faceã€æ¨ç† | candle-core, candle-nn |
| **llm** | æœ¬åœ° LLMã€CPU æ¨ç† | llm |
| **tch-rs** | PyTorch ç”Ÿæ€ã€LibTorch | tch |

---

## Burn å¿«é€Ÿå…¥é—¨

### ç¤ºä¾‹ 1: å¼ é‡åŸºç¡€æ“ä½œ

```toml
# Cargo.toml
[dependencies]
burn = "0.20"
burn-ndarray = "0.20"
```

```rust
use burn::tensor::{Tensor, backend::NdArray};

fn main() {
    type B = NdArray<f32>;

    // åˆ›å»ºå¼ é‡
    let x = Tensor::<B, 2>::from_floats([
        [1.0, 2.0],
        [3.0, 4.0],
    ]);
    let y = Tensor::<B, 2>::from_floats([
        [5.0, 6.0],
        [7.0, 8.0],
    ]);

    // å¼ é‡è¿ç®—
    let z = x + y;           // åŠ æ³•
    let w = x.matmul(y);     // çŸ©é˜µä¹˜æ³•
    let s = x.sum();         // æ±‚å’Œ

    println!("Sum: {:?}", s.into_scalar());
}
```

### ç¤ºä¾‹ 2: ç®€å•ç¥ç»ç½‘ç»œ

```rust
use burn::{
    module::Module,
    nn::{Linear, ReLU, Softmax},
    tensor::{Tensor, backend::NdArray},
};

#[derive(Module, Debug)]
struct Net<B: burn::tensor::backend::Backend> {
    linear1: Linear<B>,
    activation: ReLU,
    linear2: Linear<B>,
    softmax: Softmax,
}

impl<B: burn::tensor::backend::Backend> Net<B> {
    fn new(device: &B::Device) -> Self {
        Self {
            linear1: Linear::new(device, 784, 128),
            activation: ReLU::new(),
            linear2: Linear::new(device, 128, 10),
            softmax: Softmax::new(1),
        }
    }

    fn forward(&self, input: Tensor<B, 2>) -> Tensor<B, 2> {
        let x = self.linear1.forward(input);
        let x = self.activation.forward(x);
        let x = self.linear2.forward(x);
        self.softmax.forward(x)
    }
}
```

### ç¤ºä¾‹ 3: æ¨¡å‹æ¨ç†

```rust
use burn::tensor::{Tensor, backend::NdArray};

fn inference<B: burn::tensor::backend::Backend>(
    model: &impl burn::module::Module<B>,
    input: Tensor<B, 2>,
) -> Tensor<B, 2> {
    // å‰å‘ä¼ æ’­
    let output = model.forward(input);

    // è·å–é¢„æµ‹ç»“æœ
    let predictions = output.argmax(1);
    predictions
}
```

**æ–‡æ¡£**: [burn.dev](https://burn.dev/)

---

## Candle å¿«é€Ÿå…¥é—¨

### ç¤ºä¾‹ 4: å¼ é‡æ“ä½œ

```toml
# Cargo.toml
[dependencies]
candle-core = "0.8"
candle-nn = "0.8"
```

```rust
use candle_core::{Device, Result, Tensor};

fn main() -> Result<()> {
    let device = Device::Cpu;

    // åˆ›å»ºå¼ é‡
    let a = Tensor::new(&[[1.0f32, 2.0], [3.0, 4.0]], &device)?;
    let b = Tensor::new(&[[5.0f32, 6.0], [7.0, 8.0]], &device)?;

    // åŸºæœ¬è¿ç®—
    let c = (&a + &b)?;           // åŠ æ³•
    let d = a.matmul(&b)?;        // çŸ©é˜µä¹˜æ³•
    let e = a.mean(1)?;           // æŒ‰ç»´åº¦æ±‚å‡å€¼

    println!("Shape: {:?}", c.shape());
    println!("Values: {:?}", c.to_vec2::<f32>()?);

    Ok(())
}
```

### ç¤ºä¾‹ 5: åŠ è½½ Hugging Face æ¨¡å‹

```rust
use candle_core::{Device, Result};
use candle_nn::VarBuilder;
use hf_hub::{api::sync::Api, Repo, RepoType};

fn load_model(model_id: &str) -> Result<()> {
    let api = Api::new()?;
    let repo = api.repo(Repo::new(
        model_id.to_string(),
        RepoType::Model,
    ));

    // ä¸‹è½½æ¨¡å‹æ–‡ä»¶
    let weights = repo.get("model.safetensors")?;

    // åŠ è½½æƒé‡
    let device = Device::Cpu;
    let vb = unsafe {
        VarBuilder::from_mmaped_safetensors(&[weights], candle_core::DType::F32, &device)?
    };

    println!("Model loaded successfully!");
    Ok(())
}
```

**æ–‡æ¡£**: [Candle GitHub](https://github.com/huggingface/candle)

---

## LLM æ¨ç†

### ç¤ºä¾‹ 6: ä½¿ç”¨ llm crate è¿›è¡Œæœ¬åœ°æ¨ç†

```rust
use llm::Model;

fn llm_inference() -> anyhow::Result<()> {
    // åŠ è½½æ¨¡å‹
    let model_path = "path/to/model.gguf";
    let model = llm::load::<llm::models::Llama>(
        std::path::Path::new(model_path),
        llm::TokenizerSource::Embedded,
        Default::default(),
        llm::load_progress_callback_stdout,
    )?;

    // åˆ›å»ºæ¨ç†ä¼šè¯
    let mut session = model.start_session(Default::default());

    // æ¨ç†
    let prompt = "The capital of France is";
    let mut response = String::new();

    session.infer(
        model.as_ref(),
        &mut rand::thread_rng(),
        &llm::InferenceRequest {
            prompt: prompt.into(),
            parameters: &llm::InferenceParameters::default(),
            play_back_previous_tokens: false,
            maximum_token_count: Some(50),
        },
        &mut Default::default(),
        |t| {
            if let llm::InferenceResponse::GeneratedToken(token) = t {
                response.push_str(&token);
            }
            Ok(llm::InferenceFeedback::Continue)
        },
    )?;

    println!("Response: {}", response);
    Ok(())
}
```

### æ¡†æ¶é€‰å‹è¡¨

| åº“ | ç”¨é€” | é€‚ç”¨åœºæ™¯ |
| :--- | :--- | :--- |
| **llm** | å¤šæ¶æ„ã€InferenceSession | æœ¬åœ° CPU æ¨ç† |
| **mistral.rs** | é«˜æ€§èƒ½ã€é‡åŒ–ã€Vision | ç”Ÿäº§ç¯å¢ƒ |
| **lm.rs** | è½»é‡ã€CPU ä¼˜åŒ– | åµŒå…¥å¼è®¾å¤‡ |

---

## ä¸ C01â€“C12 å…³è”

| æ¨¡å— | AI/ML ä¸­çš„å…³è” |
| :--- | :--- |
| C01 æ‰€æœ‰æƒ | å¼ é‡ç”Ÿå‘½å‘¨æœŸã€é›¶æ‹·è´ |
| C02 ç±»å‹ç³»ç»Ÿ | æ³›å‹å¼ é‡ã€Trait æŠ½è±¡ |
| C05 çº¿ç¨‹ | å¤šçº¿ç¨‹è®­ç»ƒã€æ•°æ®å¹¶è¡Œ |
| C06 å¼‚æ­¥ | æµå¼æ¨ç† |
| C11 å® | æ¨¡å‹å®šä¹‰ DSL |

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯ {#-ä½¿ç”¨åœºæ™¯}

### åœºæ™¯ 1: å›¾åƒåˆ†ç±»æœåŠ¡

```rust
// ä½¿ç”¨ Candle æ„å»ºå›¾åƒåˆ†ç±»å¾®æœåŠ¡
use candle_core::{Device, Tensor};
use candle_nn::Module;

pub struct ImageClassifier {
    model: Box<dyn Module>,
    device: Device,
}

impl ImageClassifier {
    pub fn classify(&self, image_data: &[f32]) -> anyhow::Result<Vec<f32>> {
        let input = Tensor::from_slice(image_data, &[1, 3, 224, 224], &self.device)?;
        let output = self.model.forward(&input)?;
        let probs = candle_nn::ops::softmax(&output, 1)?;
        Ok(probs.to_vec1::<f32>()?)
    }
}
```

### åœºæ™¯ 2: å®æ—¶æ–‡æœ¬ç”Ÿæˆ

```rust
// ä½¿ç”¨ Burn å®ç°æµå¼æ–‡æœ¬ç”Ÿæˆ
use burn::tensor::backend::Backend;

async fn stream_generate<B: Backend>(
    model: &impl LanguageModel<B>,
    prompt: &str,
) -> impl Stream<Item = String> {
    // å¼‚æ­¥æµå¼ç”Ÿæˆæ–‡æœ¬
    stream! {
        let mut tokens = tokenize(prompt);
        for _ in 0..100 {
            let next_token = model.predict_next(&tokens).await;
            tokens.push(next_token);
            yield detokenize(&[next_token]);
        }
    }
}
```

---

## ğŸ“ å½¢å¼åŒ–æ–¹æ³•é“¾æ¥ {#-å½¢å¼åŒ–æ–¹æ³•é“¾æ¥}

### ç†è®ºåŸºç¡€

| æ¦‚å¿µ | å½¢å¼åŒ–æ–‡æ¡£ | æè¿° |
| :--- | :--- | :--- |
| **æ‰€æœ‰æƒä¸å†…å­˜å®‰å…¨** | [ownership_model](../../research_notes/formal_methods/ownership_model.md) | å¼ é‡å†…å­˜ç®¡ç†çš„å½¢å¼åŒ–ä¿è¯ |
| **ç±»å‹ç³»ç»Ÿ** | [type_system_foundations](../../research_notes/type_theory/type_system_foundations.md) | æ³›å‹å¼ é‡çš„ç±»å‹å®‰å…¨ |
| **Send/Sync** | [send_sync_formalization](../../research_notes/formal_methods/send_sync_formalization.md) | å¤šçº¿ç¨‹è®­ç»ƒçš„å®‰å…¨æ€§ |
| **ç”Ÿå‘½å‘¨æœŸ** | [lifetime_formalization](../../research_notes/formal_methods/lifetime_formalization.md) | æ¨¡å‹å¼•ç”¨æœ‰æ•ˆæ€§ |

### å½¢å¼åŒ–å®šç†

**å®šç† ML-T1ï¼ˆå¼ é‡å†…å­˜å®‰å…¨ï¼‰**: è‹¥å¼ é‡æ“ä½œæ»¡è¶³æ‰€æœ‰æƒè§„åˆ™ 1-8 å’Œå€Ÿç”¨è§„åˆ™ 5-8ï¼Œåˆ™å¼ é‡å†…å­˜è®¿é—®å®‰å…¨ã€‚

*è¯æ˜*: ç”± [ownership_model](../../research_notes/formal_methods/ownership_model.md) å®šç† T2/T3 å’Œ [borrow_checker_proof](../../research_notes/formal_methods/borrow_checker_proof.md) å®šç† T1ï¼Œå¼ é‡ä½œä¸ºå¤åˆç±»å‹ï¼Œå…¶å†…å­˜å®‰å…¨ç”±å†…éƒ¨å…ƒç´ çš„æ‰€æœ‰æƒä¿è¯ã€‚âˆ

---

## ğŸš« åä¾‹é€ŸæŸ¥ {#-åä¾‹é€ŸæŸ¥}

### åä¾‹ 1: æ··æ·†ä¸åŒæ¡†æ¶çš„ API

**é”™è¯¯ç¤ºä¾‹**:

```rust
// âŒ Burn ä¸ Candle çš„ Tensor åˆ›å»ºæ–¹å¼ä¸åŒï¼Œä¸å¯æ··ç”¨
use burn::tensor::Tensor as BurnTensor;
use candle_core::Tensor as CandleTensor;

fn bad() {
    // ä¸¤è€…ç±»å‹ä¸å…¼å®¹ï¼Œæ— æ³•ç›´æ¥è½¬æ¢
    let burn_t: BurnTensor<_, 2> = BurnTensor::zeros([3, 3]);
    // let candle_t: CandleTensor = burn_t;  // ç¼–è¯‘é”™è¯¯ï¼
}
```

**åŸå› **: Burnã€Candleã€tch-rs å„è‡ªæœ‰ç‹¬ç«‹ API å’Œç±»å‹ç³»ç»Ÿï¼Œä¸èƒ½æ··ç”¨ã€‚

**ä¿®æ­£**: é€‰å®šä¸€ä¸ªæ¡†æ¶åç»Ÿä¸€ä½¿ç”¨å…¶ APIï¼Œæˆ–é€šè¿‡ trait æŠ½è±¡éš”ç¦»ã€‚

```rust
// âœ… ä½¿ç”¨ trait æŠ½è±¡
pub trait TensorOps {
    fn add(&self, other: &Self) -> Self;
    fn matmul(&self, other: &Self) -> Self;
}

// ä¸ºä¸åŒæ¡†æ¶å®ç°
impl<B: Backend, const D: usize> TensorOps for Tensor<B, D> { ... }
```

---

### åä¾‹ 2: æœªæ ¹æ®åœºæ™¯é€‰æ‹©åç«¯

**é”™è¯¯ç¤ºä¾‹**:

```rust
// âŒ å¤§æ¨¡å‹æ¨ç†åœ¨ CPU ä¸Šè¿è¡Œï¼Œæœªè€ƒè™‘ GPU åŠ é€Ÿ
use candle_core::Device;

fn slow_inference() {
    let device = Device::Cpu;  // CPU æ¨ç†ææ…¢
    let model = load_model("llama-7b", &device).unwrap();
    // 7B å‚æ•°æ¨¡å‹åœ¨ CPU ä¸Šæ¨ç†å¯èƒ½éœ€æ•°åˆ†é’Ÿ
}
```

**åŸå› **: å¤§æ¨¡å‹åœ¨ CPU ä¸Šæ¨ç†å»¶è¿Ÿé«˜ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ GPU æˆ–é‡åŒ–ã€‚

**ä¿®æ­£**: ä½¿ç”¨ `Device::Cuda(0)` æˆ– `llm` çš„é‡åŒ–æ¨¡å‹ã€‚

```rust
// âœ… é€‰æ‹©åˆé€‚åç«¯
use candle_core::Device;

fn fast_inference() {
    // ä¼˜å…ˆä½¿ç”¨ GPU
    let device = Device::new_cuda(0)
        .unwrap_or(Device::Cpu);

    // æˆ–ä½¿ç”¨é‡åŒ–æ¨¡å‹
    let model = load_quantized_model("llama-7b-q4.gguf").unwrap();
}
```

---

### åä¾‹ 3: å¿½ç•¥ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§

**é”™è¯¯ç¤ºä¾‹**:

```toml
# âŒ æ··ç”¨ä¸å…¼å®¹çš„ burn ä¸ burn-ndarray ç‰ˆæœ¬
[dependencies]
burn = "0.18"
burn-ndarray = "0.20"  # ç‰ˆæœ¬ä¸ä¸€è‡´æ˜“å¯¼è‡´ç¼–è¯‘é”™è¯¯
```

**åŸå› **: burn ä¸ burn-ndarray éœ€åŒç‰ˆæœ¬ï¼Œå¦åˆ™ç¼–è¯‘å¤±è´¥ã€‚

**ä¿®æ­£**: ä¿æŒä¸»åº“ä¸åç«¯æ‰©å±•ç‰ˆæœ¬ä¸€è‡´ã€‚

```toml
# âœ… ä½¿ç”¨ workspace ç»Ÿä¸€ç‰ˆæœ¬
[workspace.dependencies]
burn = "0.20"
burn-ndarray = "0.20"
burn-cuda = "0.20"

[dependencies]
burn = { workspace = true }
burn-ndarray = { workspace = true }
```

---

### åä¾‹ 4: å†…å­˜æ³„æ¼ - å¾ªç¯å¼•ç”¨å¼ é‡ç¼“å­˜

**é”™è¯¯ç¤ºä¾‹**:

```rust
// âŒ å¾ªç¯å¼•ç”¨å¯¼è‡´å†…å­˜æ³„æ¼
use std::rc::Rc;
use std::cell::RefCell;

struct TensorCache {
    tensors: RefCell<Vec<Rc<TensorCache>>>,  // å¾ªç¯å¼•ç”¨
}

// a -> b -> a å¯¼è‡´å†…å­˜æ— æ³•é‡Šæ”¾
```

**åŸå› **: Rc å¾ªç¯å¼•ç”¨å¯¼è‡´å¼•ç”¨è®¡æ•°æ°¸ä¸ä¸ºé›¶ã€‚

**ä¿®æ­£**: ä½¿ç”¨ Weak æ‰“ç ´å¾ªç¯ã€‚

```rust
// âœ… ä½¿ç”¨ Weak æ‰“ç ´å¾ªç¯
use std::rc::{Rc, Weak};
use std::cell::RefCell;

struct TensorCache {
    tensors: RefCell<Vec<Weak<TensorCache>>>,  // Weak ä¸å¢åŠ å¼•ç”¨è®¡æ•°
}
```

---

### åä¾‹ 5: è¾¹ç•Œæƒ…å†µ - ç©ºå¼ é‡æ“ä½œ

**é”™è¯¯ç¤ºä¾‹**:

```rust
// âŒ æœªå¤„ç†ç©ºå¼ é‡
fn normalize(tensor: &Tensor) -> Tensor {
    tensor / tensor.sum()  // ç©ºå¼ é‡ sum ä¸º 0ï¼Œå¯¼è‡´é™¤é›¶
}
```

**åŸå› **: ç©ºå¼ é‡æˆ–é›¶å’Œå¯¼è‡´é™¤é›¶é”™è¯¯ã€‚

**ä¿®æ­£**: æ·»åŠ è¾¹ç•Œæ£€æŸ¥ã€‚

```rust
// âœ… è¾¹ç•Œæ£€æŸ¥
fn normalize(tensor: &Tensor) -> Option<Tensor> {
    let sum = tensor.sum().into_scalar();
    if sum == 0.0 {
        None
    } else {
        Some(tensor / sum)
    }
}
```

---

## ç›¸å…³æ–‡æ¡£

- [AI+Rust ç”Ÿæ€æŒ‡å—](../../05_guides/AI_RUST_ECOSYSTEM_GUIDE.md)
- [AI è¾…åŠ©ç¼–ç¨‹](../../../guides/AI_ASSISTED_RUST_PROGRAMMING_GUIDE_2025.md)
- [Burn](https://burn.dev/) | [Candle](https://github.com/huggingface/candle) | [llm](https://docs.rs/llm)

## ç›¸å…³ç¤ºä¾‹ä»£ç 

AI/ML ç¤ºä¾‹ä»£ç ä½äºæŒ‡å—ä¸å¤–éƒ¨ä»“åº“ï¼Œå¯ç›´æ¥å‚è€ƒï¼š

- [AI_RUST_ECOSYSTEM_GUIDE å…¥é—¨ç¤ºä¾‹](../../05_guides/AI_RUST_ECOSYSTEM_GUIDE.md) - Burn/Candle æœ€å°ç¤ºä¾‹ï¼ˆè§ã€Œå››ã€å…¥é—¨ç¤ºä¾‹ã€ï¼‰
- [Candle examples](https://github.com/huggingface/candle/tree/main/candle-examples)
- [llm ç¤ºä¾‹](https://github.com/rust-ml/llm/tree/main/examples)
