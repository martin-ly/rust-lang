# Rust实现分布式系统设计模式

```text
<think>
This is a comprehensive request asking to explain and implement all the distributed system design patterns listed in the markdown snippet using Rust's async, generic, and multithreading mechanisms. I'll need to be detailed and provide code examples for each pattern.

I'll focus on:
1. Explaining each pattern briefly
2. Showing how to implement it in Rust using async programming, generics, and multithreading
3. Providing practical, runnable code examples
4. Highlighting Rust-specific features that make these implementations robust

Since there are many patterns listed (around 30 total across 8 categories), I should be concise but comprehensive for each pattern.
</think>
```

## 1. 通信模式

### 1.1 请求-响应模式

请求-响应模式是最基本的通信模式，客户端发送请求，服务端处理后返回响应。

```rust
use std::future::Future;
use std::pin::Pin;
use tokio::sync::mpsc;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Request<T> {
    id: String,
    payload: T,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Response<R> {
    request_id: String,
    payload: R,
}

struct RequestResponseService<T, R, F, Fut>
where
    T: Send + 'static,
    R: Send + 'static,
    F: Fn(Request<T>) -> Fut + Send + Sync + 'static,
    Fut: Future<Output = Response<R>> + Send,
{
    handler: F,
    request_rx: mpsc::Receiver<(Request<T>, mpsc::Sender<Response<R>>)>,
}

impl<T, R, F, Fut> RequestResponseService<T, R, F, Fut>
where
    T: Send + 'static,
    R: Send + 'static,
    F: Fn(Request<T>) -> Fut + Send + Sync + 'static,
    Fut: Future<Output = Response<R>> + Send,
{
    fn new(handler: F) -> (Self, mpsc::Sender<(Request<T>, mpsc::Sender<Response<R>>)>) {
        let (tx, rx) = mpsc::channel(100);
        (Self { handler, request_rx: rx }, tx)
    }

    async fn run(&mut self) {
        while let Some((request, response_tx)) = self.request_rx.recv().await {
            let handler = &self.handler;
            let response = handler(request).await;
            let _ = response_tx.send(response).await;
        }
    }
}

// 客户端实现
struct Client<T, R> {
    request_tx: mpsc::Sender<(Request<T>, mpsc::Sender<Response<R>>)>,
}

impl<T, R> Client<T, R>
where
    T: Send + 'static,
    R: Send + 'static,
{
    fn new(request_tx: mpsc::Sender<(Request<T>, mpsc::Sender<Response<R>>)>) -> Self {
        Self { request_tx }
    }

    async fn send_request(&self, request: Request<T>) -> Result<Response<R>, String> {
        let (response_tx, mut response_rx) = mpsc::channel(1);
        
        if self.request_tx.send((request, response_tx)).await.is_err() {
            return Err("服务已关闭".to_string());
        }
        
        response_rx.recv().await.ok_or_else(|| "无法获取响应".to_string())
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建服务
    let handler = |req: Request<String>| async move {
        println!("处理请求: {:?}", req);
        Response {
            request_id: req.id,
            payload: format!("已处理: {}", req.payload),
        }
    };
    
    let (mut service, request_tx) = RequestResponseService::new(handler);
    
    // 启动服务
    tokio::spawn(async move {
        service.run().await;
    });
    
    // 创建客户端
    let client = Client::new(request_tx);
    
    // 发送请求
    let request = Request {
        id: "req-1".to_string(),
        payload: "Hello, Service!".to_string(),
    };
    
    let response = client.send_request(request).await.unwrap();
    println!("收到响应: {:?}", response);
}
```

### 1.2 发布-订阅模式

发布-订阅模式中，消息发送者（发布者）不直接将消息发送给特定的接收者，而是将消息分类为不同的主题。
订阅者可以表达对一个或多个主题的兴趣，只接收感兴趣的消息。

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use tokio::sync::broadcast;
use tokio::sync::mpsc;
use serde::{Serialize, Deserialize};
use futures::StreamExt;

#[derive(Clone, Debug, Serialize, Deserialize)]
struct Message<T> {
    topic: String,
    payload: T,
}

struct PubSubBroker<T: Clone + Send + 'static> {
    topics: Arc<Mutex<HashMap<String, broadcast::Sender<Message<T>>>>>,
    max_capacity: usize,
}

impl<T: Clone + Send + 'static> PubSubBroker<T> {
    fn new(max_capacity: usize) -> Self {
        Self {
            topics: Arc::new(Mutex::new(HashMap::new())),
            max_capacity,
        }
    }
    
    async fn publish(&self, message: Message<T>) -> Result<(), String> {
        let topic = message.topic.clone();
        let sender = {
            let mut topics = self.topics.lock().unwrap();
            if !topics.contains_key(&topic) {
                let (tx, _) = broadcast::channel(self.max_capacity);
                topics.insert(topic.clone(), tx);
            }
            topics.get(&topic).unwrap().clone()
        };
        
        sender.send(message).map_err(|e| format!("发布失败: {}", e))
    }
    
    async fn subscribe(&self, topic: String) -> broadcast::Receiver<Message<T>> {
        let mut topics = self.topics.lock().unwrap();
        if !topics.contains_key(&topic) {
            let (tx, _) = broadcast::channel(self.max_capacity);
            topics.insert(topic.clone(), tx);
        }
        topics.get(&topic).unwrap().subscribe()
    }
    
    async fn subscribe_multiple(&self, topics: Vec<String>) -> mpsc::Receiver<Message<T>> {
        let (tx, rx) = mpsc::channel(100);
        
        for topic in topics {
            let mut receiver = self.subscribe(topic).await;
            let tx_clone = tx.clone();
            
            tokio::spawn(async move {
                while let Ok(message) = receiver.recv().await {
                    if tx_clone.send(message).await.is_err() {
                        break;
                    }
                }
            });
        }
        
        rx
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    let broker: PubSubBroker<String> = PubSubBroker::new(100);
    
    // 创建订阅者
    let mut weather_subscriber = broker.subscribe("weather".to_string()).await;
    let mut news_subscriber = broker.subscribe("news".to_string()).await;
    let mut multi_subscriber = broker.subscribe_multiple(vec![
        "weather".to_string(), 
        "news".to_string()
    ]).await;
    
    // 发布消息
    tokio::spawn(async move {
        let weather_msg = Message {
            topic: "weather".to_string(),
            payload: "天气晴朗，温度25°C".to_string(),
        };
        
        let news_msg = Message {
            topic: "news".to_string(),
            payload: "今日头条：Rust 2023发布了!".to_string(),
        };
        
        broker.publish(weather_msg).await.unwrap();
        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
        broker.publish(news_msg).await.unwrap();
    });
    
    // 处理订阅消息
    tokio::spawn(async move {
        if let Ok(msg) = weather_subscriber.recv().await {
            println!("天气订阅者: {:?}", msg);
        }
    });
    
    tokio::spawn(async move {
        if let Ok(msg) = news_subscriber.recv().await {
            println!("新闻订阅者: {:?}", msg);
        }
    });
    
    tokio::spawn(async move {
        while let Some(msg) = multi_subscriber.recv().await {
            println!("多主题订阅者: {:?}", msg);
        }
    });
    
    // 等待足够时间让消息处理完成
    tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
}
```

### 1.3 消息队列模式

消息队列模式是发布-订阅模式的变体，其中消息被保存在队列中，直到被消费者处理。这种模式允许异步通信和解耦。

```rust
use std::collections::VecDeque;
use std::sync::{Arc, Mutex};
use tokio::sync::{mpsc, Semaphore};
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Message<T> {
    id: String,
    queue: String,
    payload: T,
}

struct MessageQueue<T: Clone + Send + 'static> {
    queues: Arc<Mutex<HashMap<String, VecDeque<Message<T>>>>>,
    semaphores: Arc<Mutex<HashMap<String, Arc<Semaphore>>>>,
}

impl<T: Clone + Send + 'static> MessageQueue<T> {
    fn new() -> Self {
        Self {
            queues: Arc::new(Mutex::new(HashMap::new())),
            semaphores: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    async fn send(&self, message: Message<T>) {
        let queue_name = message.queue.clone();
        
        // 获取对应队列的信号量
        let semaphore = {
            let mut semaphores = self.semaphores.lock().unwrap();
            if !semaphores.contains_key(&queue_name) {
                semaphores.insert(queue_name.clone(), Arc::new(Semaphore::new(0)));
            }
            semaphores.get(&queue_name).unwrap().clone()
        };
        
        // 将消息添加到队列
        {
            let mut queues = self.queues.lock().unwrap();
            queues.entry(queue_name).or_insert_with(VecDeque::new).push_back(message);
        }
        
        // 释放一个信号量，通知可能的消费者
        semaphore.add_permits(1);
    }
    
    async fn receive(&self, queue_name: &str) -> Option<Message<T>> {
        // 获取对应队列的信号量
        let semaphore = {
            let mut semaphores = self.semaphores.lock().unwrap();
            if !semaphores.contains_key(queue_name) {
                semaphores.insert(queue_name.to_string(), Arc::new(Semaphore::new(0)));
            }
            semaphores.get(queue_name).unwrap().clone()
        };
        
        // 等待消息可用
        let _permit = semaphore.acquire().await.ok()?;
        
        // 从队列取出消息
        let mut queues = self.queues.lock().unwrap();
        if let Some(queue) = queues.get_mut(queue_name) {
            queue.pop_front()
        } else {
            None
        }
    }
    
    async fn create_consumer(&self, queue_name: String) -> mpsc::Receiver<Message<T>> {
        let (tx, rx) = mpsc::channel(100);
        let queue_clone = Arc::new(self.clone());
        
        tokio::spawn(async move {
            loop {
                match queue_clone.receive(&queue_name).await {
                    Some(message) => {
                        if tx.send(message).await.is_err() {
                            // 通道关闭，消费者可能已经离开
                            break;
                        }
                    },
                    None => break,
                }
            }
        });
        
        rx
    }
    
    fn clone(&self) -> Self {
        Self {
            queues: self.queues.clone(),
            semaphores: self.semaphores.clone(),
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    let queue = MessageQueue::<String>::new();
    
    // 创建消费者
    let mut consumer1 = queue.create_consumer("tasks".to_string()).await;
    let mut consumer2 = queue.create_consumer("tasks".to_string()).await;
    
    // 发送消息
    for i in 0..10 {
        let message = Message {
            id: format!("msg-{}", i),
            queue: "tasks".to_string(),
            payload: format!("这是任务 #{}", i),
        };
        
        queue.send(message).await;
    }
    
    // 处理消息
    tokio::spawn(async move {
        while let Some(msg) = consumer1.recv().await {
            println!("消费者1收到: {:?}", msg);
            tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
        }
    });
    
    tokio::spawn(async move {
        while let Some(msg) = consumer2.recv().await {
            println!("消费者2收到: {:?}", msg);
            tokio::time::sleep(tokio::time::Duration::from_millis(150)).await;
        }
    });
    
    // 等待足够时间让消息处理完成
    tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
}
```

### 1.4 RPC模式

RPC（远程过程调用）允许客户端调用服务器上的函数，就像调用本地函数一样。

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use tokio::sync::oneshot;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
struct RpcRequest<P> {
    id: String,
    method: String,
    params: P,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct RpcResponse<R> {
    id: String,
    result: Result<R, String>,
}

struct RpcServer<P, R> {
    handlers: HashMap<String, Box<dyn Fn(P) -> R + Send + Sync>>,
}

impl<P, R> RpcServer<P, R>
where
    P: Send + 'static,
    R: Send + 'static,
{
    fn new() -> Self {
        Self {
            handlers: HashMap::new(),
        }
    }
    
    fn register<F>(&mut self, method: &str, handler: F)
    where
        F: Fn(P) -> R + Send + Sync + 'static,
    {
        self.handlers.insert(method.to_string(), Box::new(handler));
    }
    
    fn handle(&self, request: RpcRequest<P>) -> RpcResponse<R> {
        let handler = self.handlers.get(&request.method);
        
        match handler {
            Some(handler) => {
                let result = handler(request.params);
                RpcResponse {
                    id: request.id,
                    result: Ok(result),
                }
            },
            None => RpcResponse {
                id: request.id,
                result: Err(format!("未知方法: {}", request.method)),
            },
        }
    }
}

struct RpcClient<P, R> {
    server: Arc<RpcServer<P, R>>,
}

impl<P, R> RpcClient<P, R>
where
    P: Clone + Send + 'static,
    R: Clone + Send + 'static,
{
    fn new(server: Arc<RpcServer<P, R>>) -> Self {
        Self { server }
    }
    
    async fn call(&self, method: &str, params: P) -> Result<R, String> {
        let request = RpcRequest {
            id: format!("req-{}", uuid::Uuid::new_v4()),
            method: method.to_string(),
            params,
        };
        
        let response = self.server.handle(request);
        response.result
    }
}

// 使用远程实现
use tokio::sync::mpsc;

struct RemoteRpcClient<P, R>
where
    P: Serialize + Send + 'static,
    R: for<'de> Deserialize<'de> + Send + 'static,
{
    request_tx: mpsc::Sender<(String, Vec<u8>, oneshot::Sender<Vec<u8>>)>,
}

impl<P, R> RemoteRpcClient<P, R>
where
    P: Serialize + Send + 'static,
    R: for<'de> Deserialize<'de> + Send + 'static,
{
    fn new(request_tx: mpsc::Sender<(String, Vec<u8>, oneshot::Sender<Vec<u8>>)>) -> Self {
        Self { request_tx }
    }
    
    async fn call(&self, method: &str, params: P) -> Result<R, String> {
        let request_id = format!("req-{}", uuid::Uuid::new_v4());
        
        // 序列化请求
        let request = RpcRequest {
            id: request_id.clone(),
            method: method.to_string(),
            params,
        };
        
        let serialized_request = serde_json::to_vec(&request)
            .map_err(|e| format!("序列化请求失败: {}", e))?;
        
        // 创建响应通道
        let (response_tx, response_rx) = oneshot::channel();
        
        // 发送请求
        self.request_tx.send((request_id, serialized_request, response_tx)).await
            .map_err(|_| "RPC服务已关闭".to_string())?;
        
        // 等待响应
        let serialized_response = response_rx.await
            .map_err(|_| "获取响应失败".to_string())?;
        
        // 反序列化响应
        let response: RpcResponse<R> = serde_json::from_slice(&serialized_response)
            .map_err(|e| format!("反序列化响应失败: {}", e))?;
        
        response.result
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建RPC服务器
    let mut server = RpcServer::<String, String>::new();
    
    // 注册处理器
    server.register("echo", |message| {
        message
    });
    
    server.register("reverse", |message| {
        message.chars().rev().collect()
    });
    
    let server_arc = Arc::new(server);
    
    // 创建客户端
    let client = RpcClient::new(server_arc);
    
    // 调用RPC方法
    let echo_result = client.call("echo", "Hello, RPC!".to_string()).await.unwrap();
    println!("Echo结果: {}", echo_result);
    
    let reverse_result = client.call("reverse", "Hello, RPC!".to_string()).await.unwrap();
    println!("Reverse结果: {}", reverse_result);
}
```

### 1.5 流处理模式

流处理模式处理连续的数据流，通常通过一系列转换操作。

```rust
use futures::{Stream, StreamExt};
use std::pin::Pin;
use std::task::{Context, Poll};
use tokio::sync::mpsc;

type BoxStream<T> = Pin<Box<dyn Stream<Item = T> + Send>>;

trait StreamProcessor<In, Out> {
    fn process(&self, input: In) -> Out;
    
    fn process_stream<S>(&self, stream: S) -> BoxStream<Out>
    where
        S: Stream<Item = In> + Send + 'static,
        In: 'static,
        Out: 'static,
        Self: 'static + Sync + Send,
    {
        let processor = self.clone();
        Box::pin(stream.map(move |item| processor.process(item)))
    }
    
    fn clone(&self) -> Box<dyn StreamProcessor<In, Out> + Send + Sync>;
}

struct MapProcessor<In, Out, F>
where
    F: Fn(In) -> Out + Send + Sync + Clone,
{
    func: F,
    _phantom: std::marker::PhantomData<(In, Out)>,
}

impl<In, Out, F> MapProcessor<In, Out, F>
where
    F: Fn(In) -> Out + Send + Sync + Clone,
{
    fn new(func: F) -> Self {
        Self {
            func,
            _phantom: std::marker::PhantomData,
        }
    }
}

impl<In, Out, F> StreamProcessor<In, Out> for MapProcessor<In, Out, F>
where
    F: Fn(In) -> Out + Send + Sync + Clone + 'static,
    In: 'static,
    Out: 'static,
{
    fn process(&self, input: In) -> Out {
        (self.func)(input)
    }
    
    fn clone(&self) -> Box<dyn StreamProcessor<In, Out> + Send + Sync> {
        Box::new(Self {
            func: self.func.clone(),
            _phantom: std::marker::PhantomData,
        })
    }
}

struct FilterProcessor<T, F>
where
    F: Fn(&T) -> bool + Send + Sync + Clone,
{
    predicate: F,
    _phantom: std::marker::PhantomData<T>,
}

impl<T, F> FilterProcessor<T, F>
where
    F: Fn(&T) -> bool + Send + Sync + Clone,
{
    fn new(predicate: F) -> Self {
        Self {
            predicate,
            _phantom: std::marker::PhantomData,
        }
    }
}

impl<T, F> StreamProcessor<T, Option<T>> for FilterProcessor<T, F>
where
    F: Fn(&T) -> bool + Send + Sync + Clone + 'static,
    T: 'static + Clone,
{
    fn process(&self, input: T) -> Option<T> {
        if (self.predicate)(&input) {
            Some(input)
        } else {
            None
        }
    }
    
    fn clone(&self) -> Box<dyn StreamProcessor<T, Option<T>> + Send + Sync> {
        Box::new(Self {
            predicate: self.predicate.clone(),
            _phantom: std::marker::PhantomData,
        })
    }
    
    fn process_stream<S>(&self, stream: S) -> BoxStream<Option<T>>
    where
        S: Stream<Item = T> + Send + 'static,
        T: 'static,
        Self: 'static + Sync + Send,
    {
        let processor = self.clone();
        Box::pin(stream.map(move |item| processor.process(item)))
    }
}

// 组合处理器
struct StreamPipeline<T> {
    input_tx: Option<mpsc::Sender<T>>,
    output_rx: Option<BoxStream<T>>,
}

impl<T: Clone + Send + 'static> StreamPipeline<T> {
    fn new<S: Stream<Item = T> + Send + 'static>(source: S) -> Self {
        Self {
            input_tx: None,
            output_rx: Some(Box::pin(source)),
        }
    }
    
    fn from_channel(capacity: usize) -> Self {
        let (tx, rx) = mpsc::channel(capacity);
        Self {
            input_tx: Some(tx),
            output_rx: Some(Box::pin(rx)),
        }
    }
    
    fn map<R, F>(self, func: F) -> StreamPipeline<R>
    where
        F: Fn(T) -> R + Send + Sync + Clone + 'static,
        R: Send + 'static,
    {
        let processor = MapProcessor::new(func);
        
        match self.output_rx {
            Some(stream) => {
                let output_stream = processor.process_stream(stream);
                StreamPipeline {
                    input_tx: None,
                    output_rx: Some(output_stream),
                }
            },
            None => panic!("流已被消费"),
        }
    }
    
    fn filter<F>(self, predicate: F) -> StreamPipeline<T>
    where
        F: Fn(&T) -> bool + Send + Sync + Clone + 'static,
    {
        let processor = FilterProcessor::new(predicate);
        
        match self.output_rx {
            Some(stream) => {
                let filtered_stream = processor.process_stream(stream);
                let non_none_stream = Box::pin(filtered_stream.filter_map(futures::future::ready));
                
                StreamPipeline {
                    input_tx: None,
                    output_rx: Some(non_none_stream),
                }
            },
            None => panic!("流已被消费"),
        }
    }
    
    fn get_sender(&self) -> Option<mpsc::Sender<T>> {
        self.input_tx.clone()
    }
    
    fn into_stream(self) -> BoxStream<T> {
        self.output_rx.expect("流已被消费")
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建一个源通道
    let pipeline = StreamPipeline::<i32>::from_channel(100);
    let producer = pipeline.get_sender().unwrap();
    
    // 创建处理流水线
    let processed_stream = pipeline
        .map(|x| x * 2)
        .filter(|x| *x > 10)
        .map(|x| format!("处理后的值: {}", x))
        .into_stream();
    
    // 生产数据
    tokio::spawn(async move {
        for i in 1..=10 {
            producer.send(i).await.unwrap();
        }
    });
    
    // 消费处理后的数据
    tokio::pin!(processed_stream);
    while let Some(item) = processed_stream.next().await {
        println!("{}", item);
    }
}
```

## 2. 一致性与复制模式

### 2.1 主从复制

主从复制模式中，一个节点（主节点）接收所有写入，然后复制到多个从节点。

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use tokio::sync::{broadcast, mpsc, RwLock};
use tokio::time::Duration;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
enum Operation<K, V> {
    Put(K, V),
    Delete(K),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct ReplicationLog<K, V> {
    seq_no: u64,
    operation: Operation<K, V>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
enum ReplicaMessage<K, V> {
    Sync(Vec<ReplicationLog<K, V>>),
    SyncRequest { from_seq: u64 },
    Heartbeat { seq_no: u64 },
}

struct MasterNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    data: Arc<RwLock<HashMap<K, V>>>,
    log: Arc<Mutex<Vec<ReplicationLog<K, V>>>>,
    next_seq_no: Arc<Mutex<u64>>,
    replica_tx: broadcast::Sender<ReplicaMessage<K, V>>,
}

impl<K, V> MasterNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
{
    fn new(capacity: usize) -> Self {
        let (tx, _) = broadcast::channel(capacity);
        
        Self {
            data: Arc::new(RwLock::new(HashMap::new())),
            log: Arc::new(Mutex::new(Vec::new())),
            next_seq_no: Arc::new(Mutex::new(0)),
            replica_tx: tx,
        }
    }
    
    async fn put(&self, key: K, value: V) {
        // 更新数据
        {
            let mut data = self.data.write().await;
            data.insert(key.clone(), value.clone());
        }
        
        // 记录并复制操作
        let operation = Operation::Put(key, value);
        self.replicate_operation(operation).await;
    }
    
    async fn delete(&self, key: K) {
        // 更新数据
        {
            let mut data = self.data.write().await;
            data.remove(&key);
        }
        
        // 记录并复制操作
        let operation = Operation::Delete(key);
        self.replicate_operation(operation).await;
    }
    
    async fn get(&self, key: &K) -> Option<V> {
        let data = self.data.read().await;
        data.get(key).cloned()
    }
    
    async fn replicate_operation(&self, operation: Operation<K, V>) {
        let seq_no = {
            let mut next_seq = self.next_seq_no.lock().unwrap();
            let seq = *next_seq;
            *next_seq += 1;
            seq
        };
        
        let log_entry = ReplicationLog {
            seq_no,
            operation,
        };
        
        // 添加到日志
        {
            let mut log = self.log.lock().unwrap();
            log.push(log_entry.clone());
        }
        
        // 广播给从节点
        let message = ReplicaMessage::Sync(vec![log_entry]);
        let _ = self.replica_tx.send(message);
    }
    
    fn get_subscriber(&self) -> broadcast::Receiver<ReplicaMessage<K, V>> {
        self.replica_tx.subscribe()
    }
    
    async fn handle_sync_request(&self, from_seq: u64) -> Vec<ReplicationLog<K, V>> {
        let log = self.log.lock().unwrap();
        log.iter()
            .filter(|entry| entry.seq_no >= from_seq)
            .cloned()
            .collect()
    }
    
    async fn start_heartbeat(&self, interval: Duration) {
        let replica_tx = self.replica_tx.clone();
        let next_seq_no = self.next_seq_no.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(interval);
            
            loop {
                interval.tick().await;
                let seq_no = {
                    let seq = next_seq_no.lock().unwrap();
                    *seq
                };
                
                let heartbeat = ReplicaMessage::Heartbeat { seq_no };
                let _ = replica_tx.send(heartbeat);
            }
        });
    }
}

struct ReplicaNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    data: Arc<RwLock<HashMap<K, V>>>,
    last_seq_no: Arc<Mutex<u64>>,
    master_tx: mpsc::Sender<ReplicaMessage<K, V>>,
    master_rx: broadcast::Receiver<ReplicaMessage<K, V>>,
}

impl<K, V> ReplicaNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
{
    fn new(
        master_tx: mpsc::Sender<ReplicaMessage<K, V>>,
        master_rx: broadcast::Receiver<ReplicaMessage<K, V>>,
    ) -> Self {
        Self {
            data: Arc::new(RwLock::new(HashMap::new())),
            last_seq_no: Arc::new(Mutex::new(0)),
            master_tx,
            master_rx,
        }
    }
    
    async fn get(&self, key: &K) -> Option<V> {
        let data = self.data.read().await;
        data.get(key).cloned()
    }
    
    async fn apply_log(&self, log_entry: &ReplicationLog<K, V>) {
        match &log_entry.operation {
            Operation::Put(key, value) => {
                let mut data = self.data.write().await;
                data.insert(key.clone(), value.clone());
            },
            Operation::Delete(key) => {
                let mut data = self.data.write().await;
                data.remove(key);
            },
        }
        
        // 更新序列号
        {
            let mut last_seq = self.last_seq_no.lock().unwrap();
            *last_seq = log_entry.seq_no;
        }
    }
    
    async fn start(&mut self) {
        let data_clone = self.data.clone();
        let last_seq_no_clone = self.last_seq_no.clone();
        let master_tx_clone = self.master_tx.clone();
        
        // 初始请求同步
        self.request_sync().await;
        
        // 处理来自主节点的消息
        while let Ok(message) = self.master_rx.recv().await {
            match message {
                ReplicaMessage::Sync(logs) => {
                    for log in logs {
                        if log.seq_no >= {
                            let seq = last_seq_no_clone.lock().unwrap();
                            *seq
                        } {
                            self.apply_log(&log).await;
                        }
                    }
                },
                ReplicaMessage::Heartbeat { seq_no } => {
                    let current_seq = {
                        let seq = last_seq_no_clone.lock().unwrap();
                        *seq
                    };
                    
                    if seq_no > current_seq {
                        // 发现自己有日志缺失，请求同步
                        self.request_sync().await;
                    }
                },
                _ => {},
            }
        }
    }
    
    async fn request_sync(&self) {
        let from_seq = {
            let seq = self.last_seq_no.lock().unwrap();
            *seq
        };
        
        let request = ReplicaMessage::SyncRequest { from_seq };
        let _ = self.master_tx.send(request).await;
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建主节点
    let master = Arc::new(MasterNode::<String, String>::new(100));
    
    // 设置主节点处理同步请求
    let master_clone = master.clone();
    let (request_tx, mut request_rx) = mpsc::channel(100);
    
    tokio::spawn(async move {
        while let Some(message) = request_rx.recv().await {
            match message {
                ReplicaMessage::SyncRequest { from_seq } => {
                    let logs = master_clone.handle_sync_request(from_seq).await;
                    let sync_message = ReplicaMessage::Sync(logs);
                    let _ = master_clone.replica_tx.send(sync_message);
                },
                _ => {},
            }
        }
    });
    
    // 启动主节点心跳
    master.start_heartbeat(Duration::from_secs(1)).await;
    
    // 创建从节点
    let master_rx = master.get_subscriber();
    let mut replica = ReplicaNode::new(request_tx.clone(), master_rx);
    
    // 启动从节点
    tokio::spawn(async move {
        replica.start().await;
    });
    
    // 测试主从复制
    master.put("key1".to_string(), "value1".to_string()).await;

/*
<think>
我需要继续完成关于分布式系统设计模式的Rust实现示例。
我将继续之前被截断的主从复制示例代码，然后继续介绍其他模式。
</think>
*/

    master.put("key2".to_string(), "value2".to_string()).await;
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    let value = master.get(&"key1".to_string()).await;
    println!("主节点获取key1: {:?}", value);
    
    // 删除一个键，这应该也会复制到从节点
    master.delete("key1".to_string()).await;
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // 等待足够的时间让复制完成
    tokio::time::sleep(Duration::from_secs(2)).await;
}
```

### 2.2 多主复制

多主复制允许多个节点接受写入操作，彼此之间相互复制。这种模式增加了复杂性但提高了写入可用性。

```rust
use std::collections::{HashMap, HashSet};
use std::sync::{Arc, Mutex};
use tokio::sync::{broadcast, mpsc, RwLock};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
struct VersionVector {
    node_id: String,
    counter: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Operation<K, V> {
    id: String,
    key: K,
    value: Option<V>,
    vector_clock: HashMap<String, u64>,
    timestamp: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
enum NodeMessage<K, V> {
    Replicate(Operation<K, V>),
    SyncRequest { node_id: String, vector_clock: HashMap<String, u64> },
    SyncResponse { operations: Vec<Operation<K, V>> },
}

struct MultiMasterNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    node_id: String,
    data: Arc<RwLock<HashMap<K, (V, HashMap<String, u64>)>>>,
    operations: Arc<Mutex<Vec<Operation<K, V>>>>,
    vector_clock: Arc<Mutex<HashMap<String, u64>>>,
    applied_ops: Arc<Mutex<HashSet<String>>>,
    node_tx: broadcast::Sender<NodeMessage<K, V>>,
    peers: Vec<mpsc::Sender<NodeMessage<K, V>>>,
}

impl<K, V> MultiMasterNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
{
    fn new(node_id: &str, capacity: usize) -> Self {
        let (tx, _) = broadcast::channel(capacity);
        let mut vector_clock = HashMap::new();
        vector_clock.insert(node_id.to_string(), 0);
        
        Self {
            node_id: node_id.to_string(),
            data: Arc::new(RwLock::new(HashMap::new())),
            operations: Arc::new(Mutex::new(Vec::new())),
            vector_clock: Arc::new(Mutex::new(vector_clock)),
            applied_ops: Arc::new(Mutex::new(HashSet::new())),
            node_tx: tx,
            peers: Vec::new(),
        }
    }
    
    fn add_peer(&mut self, peer_tx: mpsc::Sender<NodeMessage<K, V>>) {
        self.peers.push(peer_tx);
    }
    
    async fn put(&self, key: K, value: V) {
        // 更新本地向量时钟
        let op_id = Uuid::new_v4().to_string();
        let vector_clock = {
            let mut vc = self.vector_clock.lock().unwrap();
            *vc.get_mut(&self.node_id).unwrap() += 1;
            vc.clone()
        };
        
        // 创建操作
        let timestamp = chrono::Utc::now().timestamp_millis() as u64;
        let operation = Operation {
            id: op_id.clone(),
            key: key.clone(),
            value: Some(value.clone()),
            vector_clock: vector_clock.clone(),
            timestamp,
        };
        
        // 应用到本地
        self.apply_operation(operation.clone()).await;
        
        // 复制到其他节点
        let message = NodeMessage::Replicate(operation);
        for peer in &self.peers {
            let _ = peer.send(message.clone()).await;
        }
    }
    
    async fn delete(&self, key: K) {
        // 更新本地向量时钟
        let op_id = Uuid::new_v4().to_string();
        let vector_clock = {
            let mut vc = self.vector_clock.lock().unwrap();
            *vc.get_mut(&self.node_id).unwrap() += 1;
            vc.clone()
        };
        
        // 创建操作
        let timestamp = chrono::Utc::now().timestamp_millis() as u64;
        let operation = Operation {
            id: op_id.clone(),
            key: key.clone(),
            value: None,
            vector_clock: vector_clock.clone(),
            timestamp,
        };
        
        // 应用到本地
        self.apply_operation(operation.clone()).await;
        
        // 复制到其他节点
        let message = NodeMessage::Replicate(operation);
        for peer in &self.peers {
            let _ = peer.send(message.clone()).await;
        }
    }
    
    async fn get(&self, key: &K) -> Option<V> {
        let data = self.data.read().await;
        data.get(key).map(|(value, _)| value.clone())
    }
    
    async fn apply_operation(&self, operation: Operation<K, V>) {
        // 检查操作是否已应用
        {
            let applied = self.applied_ops.lock().unwrap();
            if applied.contains(&operation.id) {
                return;
            }
        }
        
        // 应用操作到数据
        {
            let mut data = self.data.write().await;
            
            match operation.value {
                Some(value) => {
                    data.insert(operation.key.clone(), (value, operation.vector_clock.clone()));
                },
                None => {
                    data.remove(&operation.key);
                }
            }
        }
        
        // 更新向量时钟
        {
            let mut vc = self.vector_clock.lock().unwrap();
            for (node, counter) in &operation.vector_clock {
                let entry = vc.entry(node.clone()).or_insert(0);
                *entry = std::cmp::max(*entry, *counter);
            }
        }
        
        // 记录操作并标记为已应用
        {
            let mut ops = self.operations.lock().unwrap();
            ops.push(operation.clone());
            
            let mut applied = self.applied_ops.lock().unwrap();
            applied.insert(operation.id);
        }
    }
    
    fn get_subscriber(&self) -> broadcast::Receiver<NodeMessage<K, V>> {
        self.node_tx.subscribe()
    }
    
    async fn handle_sync_request(&self, node_id: String, remote_vc: HashMap<String, u64>) -> Vec<Operation<K, V>> {
        let ops = {
            let ops = self.operations.lock().unwrap();
            ops.clone()
        };
        
        // 找出远程节点缺少的操作
        ops.iter()
            .filter(|op| {
                for (node, counter) in &op.vector_clock {
                    if let Some(remote_counter) = remote_vc.get(node) {
                        if counter > remote_counter {
                            return true;
                        }
                    } else {
                        return true; // 远程没有这个节点的向量
                    }
                }
                false
            })
            .cloned()
            .collect()
    }
    
    async fn request_sync_from_peers(&self) {
        let node_id = self.node_id.clone();
        let vector_clock = {
            let vc = self.vector_clock.lock().unwrap();
            vc.clone()
        };
        
        let message = NodeMessage::SyncRequest { node_id, vector_clock };
        
        for peer in &self.peers {
            let _ = peer.send(message.clone()).await;
        }
    }
    
    async fn start_background_sync(&self, interval: tokio::time::Duration) {
        let self_clone = self.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(interval);
            
            loop {
                interval.tick().await;
                self_clone.request_sync_from_peers().await;
            }
        });
    }
    
    fn clone(&self) -> Self {
        Self {
            node_id: self.node_id.clone(),
            data: self.data.clone(),
            operations: self.operations.clone(),
            vector_clock: self.vector_clock.clone(),
            applied_ops: self.applied_ops.clone(),
            node_tx: self.node_tx.clone(),
            peers: self.peers.clone(),
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建两个主节点
    let mut node1 = MultiMasterNode::<String, String>::new("node1", 100);
    let mut node2 = MultiMasterNode::<String, String>::new("node2", 100);
    
    // 设置节点间通信
    let (node1_tx, mut node1_rx) = mpsc::channel(100);
    let (node2_tx, mut node2_rx) = mpsc::channel(100);
    
    node1.add_peer(node2_tx);
    node2.add_peer(node1_tx);
    
    // 启动节点1的消息处理循环
    let node1_clone = node1.clone();
    tokio::spawn(async move {
        while let Some(message) = node1_rx.recv().await {
            match message {
                NodeMessage::Replicate(op) => {
                    node1_clone.apply_operation(op).await;
                },
                NodeMessage::SyncRequest { node_id, vector_clock } => {
                    let ops = node1_clone.handle_sync_request(node_id, vector_clock).await;
                    let sync_message = NodeMessage::SyncResponse { operations: ops };
                    // 发送回请求的节点 - 在实际系统中会有更复杂的路由
                },
                NodeMessage::SyncResponse { operations } => {
                    for op in operations {
                        node1_clone.apply_operation(op).await;
                    }
                }
            }
        }
    });
    
    // 启动节点2的消息处理循环
    let node2_clone = node2.clone();
    tokio::spawn(async move {
        while let Some(message) = node2_rx.recv().await {
            match message {
                NodeMessage::Replicate(op) => {
                    node2_clone.apply_operation(op).await;
                },
                NodeMessage::SyncRequest { node_id, vector_clock } => {
                    let ops = node2_clone.handle_sync_request(node_id, vector_clock).await;
                    let sync_message = NodeMessage::SyncResponse { operations: ops };
                    // 发送回请求的节点
                },
                NodeMessage::SyncResponse { operations } => {
                    for op in operations {
                        node2_clone.apply_operation(op).await;
                    }
                }
            }
        }
    });
    
    // 启动定期同步
    node1.start_background_sync(tokio::time::Duration::from_secs(5)).await;
    node2.start_background_sync(tokio::time::Duration::from_secs(5)).await;
    
    // 在两个节点上执行写入操作
    node1.put("key1".to_string(), "value1-from-node1".to_string()).await;
    node2.put("key2".to_string(), "value2-from-node2".to_string()).await;
    
    // 等待同步
    tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
    
    // 检查两个节点是否有相同的数据
    let value1_from_node1 = node1.get(&"key1".to_string()).await;
    let value1_from_node2 = node2.get(&"key1".to_string()).await;
    
    let value2_from_node1 = node1.get(&"key2".to_string()).await;
    let value2_from_node2 = node2.get(&"key2".to_string()).await;
    
    println!("节点1上的key1: {:?}", value1_from_node1);
    println!("节点2上的key1: {:?}", value1_from_node2);
    println!("节点1上的key2: {:?}", value2_from_node1);
    println!("节点2上的key2: {:?}", value2_from_node2);
}
```

### 2.3 无主复制

无主复制不指定任何节点为主节点，任何节点都可以处理读写请求。

```rust
use std::collections::{HashMap, HashSet};
use std::sync::{Arc, Mutex};
use tokio::sync::{mpsc, RwLock};
use tokio::time::Duration;
use serde::{Serialize, Deserialize};
use uuid::Uuid;
use std::time::{SystemTime, UNIX_EPOCH};

#[derive(Debug, Clone, Serialize, Deserialize)]
struct DataItem<V> {
    value: V,
    timestamp: u64,
    node_id: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
enum NodeMessage<K, V> {
    Write { key: K, value: V, timestamp: u64, node_id: String },
    ReadRepair { key: K, value: V, timestamp: u64, node_id: String },
    ReadRequest { key: K, request_id: String, reply_to: String },
    ReadResponse { key: K, value: Option<DataItem<V>>, request_id: String },
}

struct PeerlessNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    node_id: String,
    data: Arc<RwLock<HashMap<K, DataItem<V>>>>,
    peers: Vec<mpsc::Sender<NodeMessage<K, V>>>,
    read_quorum: usize,
    write_quorum: usize,
    pending_reads: Arc<Mutex<HashMap<String, Vec<DataItem<V>>>>>,
}

impl<K, V> PeerlessNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Send + Sync + 'static + PartialEq + Serialize + for<'de> Deserialize<'de>,
{
    fn new(node_id: &str, n_replicas: usize) -> Self {
        // 设置读写仲裁值
        // 通常 R + W > N 以确保一致性
        let write_quorum = (n_replicas / 2) + 1;
        let read_quorum = (n_replicas / 2) + 1;
        
        Self {
            node_id: node_id.to_string(),
            data: Arc::new(RwLock::new(HashMap::new())),
            peers: Vec::new(),
            read_quorum,
            write_quorum,
            pending_reads: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    fn add_peer(&mut self, peer_tx: mpsc::Sender<NodeMessage<K, V>>) {
        self.peers.push(peer_tx);
    }
    
    async fn write(&self, key: K, value: V) -> Result<(), String> {
        let timestamp = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_millis() as u64;
        
        // 更新本地存储
        {
            let mut data = self.data.write().await;
            data.insert(key.clone(), DataItem {
                value: value.clone(),
                timestamp,
                node_id: self.node_id.clone(),
            });
        }
        
        // 构建写入消息
        let write_message = NodeMessage::Write {
            key: key.clone(),
            value: value.clone(),
            timestamp,
            node_id: self.node_id.clone(),
        };
        
        // 向所有对等节点发送写入消息
        let mut successful_writes = 1; // 本地写入已成功
        
        let mut handles = Vec::new();
        for peer in &self.peers {
            let peer_clone = peer.clone();
            let message_clone = write_message.clone();
            
            let handle = tokio::spawn(async move {
                peer_clone.send(message_clone).await.is_ok()
            });
            
            handles.push(handle);
        }
        
        for handle in handles {
            if handle.await.unwrap_or(false) {
                successful_writes += 1;
            }
        }
        
        // 检查是否达到写入仲裁
        if successful_writes >= self.write_quorum {
            Ok(())
        } else {
            Err(format!("写入失败: 仅有 {} 节点确认，需要 {}", successful_writes, self.write_quorum))
        }
    }
    
    async fn read(&self, key: K) -> Result<Option<V>, String> {
        let request_id = Uuid::new_v4().to_string();
        
        // 从本地读取数据
        let local_value = {
            let data = self.data.read().await;
            data.get(&key).cloned()
        };
        
        // 初始化挂起的读取
        {
            let mut pending = self.pending_reads.lock().unwrap();
            let values = Vec::new();
            pending.insert(request_id.clone(), values);
        }
        
        // 如果本地有值，添加到挂起的读取中
        if let Some(value) = local_value {
            let mut pending = self.pending_reads.lock().unwrap();
            if let Some(values) = pending.get_mut(&request_id) {
                values.push(value);
            }
        }
        
        // 从所有对等节点请求读取
        let read_message = NodeMessage::ReadRequest {
            key: key.clone(),
            request_id: request_id.clone(),
            reply_to: self.node_id.clone(),
        };
        
        for peer in &self.peers {
            let _ = peer.send(read_message.clone()).await;
        }
        
        // 等待足够的响应
        let mut received_responses = if local_value.is_some() { 1 } else { 0 };
        let timeout = Duration::from_millis(100);
        
        let start = SystemTime::now();
        while received_responses < self.read_quorum && 
              SystemTime::now().duration_since(start).unwrap() < timeout {
            let responses = {
                let pending = self.pending_reads.lock().unwrap();
                pending.get(&request_id).map(|v| v.len()).unwrap_or(0)
            };
            
            received_responses = responses;
            
            if received_responses < self.read_quorum {
                tokio::time::sleep(Duration::from_millis(10)).await;
            }
        }
        
        // 检查是否达到读取仲裁
        if received_responses < self.read_quorum {
            // 清理挂起的读取
            let mut pending = self.pending_reads.lock().unwrap();
            pending.remove(&request_id);
            
            return Err(format!("读取失败: 仅有 {} 节点响应，需要 {}", received_responses, self.read_quorum));
        }
        
        // 处理所有收到的值
        let responses = {
            let mut pending = self.pending_reads.lock().unwrap();
            pending.remove(&request_id).unwrap_or_default()
        };
        
        if responses.is_empty() {
            return Ok(None);
        }
        
        // 找到最新的值
        let latest = responses.iter()
            .max_by_key(|item| item.timestamp)
            .cloned();
        
        // 检查是否需要读修复
        for response in &responses {
            if response.timestamp < latest.as_ref().unwrap().timestamp {
                // 找到陈旧版本，执行读修复
                let repair_message = NodeMessage::ReadRepair {
                    key: key.clone(),
                    value: latest.as_ref().unwrap().value.clone(),
                    timestamp: latest.as_ref().unwrap().timestamp,
                    node_id: latest.as_ref().unwrap().node_id.clone(),
                };
                
                for peer in &self.peers {
                    let _ = peer.send(repair_message.clone()).await;
                }
                
                // 更新本地存储
                let mut data = self.data.write().await;
                data.insert(key.clone(), latest.as_ref().unwrap().clone());
                
                break;
            }
        }
        
        Ok(latest.map(|item| item.value))
    }
    
    async fn handle_message(&self, message: NodeMessage<K, V>) {
        match message {
            NodeMessage::Write { key, value, timestamp, node_id } => {
                let mut data = self.data.write().await;
                let current = data.get(&key);
                
                let should_update = match current {
                    Some(existing) => existing.timestamp < timestamp,
                    None => true,
                };
                
                if should_update {
                    data.insert(key, DataItem {
                        value,
                        timestamp,
                        node_id,
                    });
                }
            },
            NodeMessage::ReadRepair { key, value, timestamp, node_id } => {
                let mut data = self.data.write().await;
                let current = data.get(&key);
                
                let should_update = match current {
                    Some(existing) => existing.timestamp < timestamp,
                    None => true,
                };
                
                if should_update {
                    data.insert(key, DataItem {
                        value,
                        timestamp,
                        node_id,
                    });
                }
            },
            NodeMessage::ReadRequest { key, request_id, reply_to } => {
                let value = {
                    let data = self.data.read().await;
                    data.get(&key).cloned()
                };
                
                // 找到目标节点并回复
                for peer in &self.peers {
                    let response = NodeMessage::ReadResponse {
                        key: key.clone(),
                        value,
                        request_id,
                    };
                    
                    // 在实际系统中，会有更复杂的路由机制
                    let _ = peer.send(response).await;
                }
            },
            NodeMessage::ReadResponse { key, value, request_id } => {
                if let Some(value) = value {
                    let mut pending = self.pending_reads.lock().unwrap();
                    if let Some(values) = pending.get_mut(&request_id) {
                        values.push(value);
                    }
                }
            }
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建三个无主节点
    let mut node1 = PeerlessNode::<String, String>::new("node1", 3);
    let mut node2 = PeerlessNode::<String, String>::new("node2", 3);
    let mut node3 = PeerlessNode::<String, String>::new("node3", 3);
    
    // 设置节点间通信
    let (node1_tx, mut node1_rx) = mpsc::channel(100);
    let (node2_tx, mut node2_rx) = mpsc::channel(100);
    let (node3_tx, mut node3_rx) = mpsc::channel(100);
    
    node1.add_peer(node2_tx.clone());
    node1.add_peer(node3_tx.clone());
    
    node2.add_peer(node1_tx.clone());
    node2.add_peer(node3_tx.clone());
    
    node3.add_peer(node1_tx.clone());
    node3.add_peer(node2_tx.clone());
    
    // 启动消息处理循环
    let node1_clone = Arc::new(node1);
    let node2_clone = Arc::new(node2);
    let node3_clone = Arc::new(node3);
    
    let node1_handle = {
        let node = node1_clone.clone();
        tokio::spawn(async move {
            while let Some(message) = node1_rx.recv().await {
                node.handle_message(message).await;
            }
        })
    };
    
    let node2_handle = {
        let node = node2_clone.clone();
        tokio::spawn(async move {
            while let Some(message) = node2_rx.recv().await {
                node.handle_message(message).await;
            }
        })
    };
    
    let node3_handle = {
        let node = node3_clone.clone();
        tokio::spawn(async move {
            while let Some(message) = node3_rx.recv().await {
                node.handle_message(message).await;
            }
        })
    };
    
    // 从node1写入数据
    node1_clone.write("key1".to_string(), "value1".to_string()).await.unwrap();
    
    // 等待数据复制
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // 从各个节点读取数据
    let result1 = node1_clone.read("key1".to_string()).await.unwrap();
    let result2 = node2_clone.read("key1".to_string()).await.unwrap();
    let result3 = node3_clone.read("key1".to_string()).await.unwrap();
    
    println!("Node1读取结果: {:?}", result1);
    println!("Node2读取结果: {:?}", result2);
    println!("Node3读取结果: {:?}", result3);
    
    // 从node2写入更新的数据
    node2_clone.write("key1".to_string(), "value1-updated".to_string()).await.unwrap();
    
    // 等待数据复制
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // 再次从各个节点读取数据
    let result1 = node1_clone.read("key1".to_string()).await.unwrap();
    let result2 = node2_clone.read("key1".to_string()).await.unwrap();
    let result3 = node3_clone.read("key1".to_string()).await.unwrap();
    
    println!("更新后Node1读取结果: {:?}", result1);
    println!("更新后Node2读取结果: {:?}", result2);
    println!("更新后Node3读取结果: {:?}", result3);
}
```

### 2.4 分布式共识（Raft）

分布式共识算法如Raft允许一组节点就值达成一致，即使在节点故障的情况下也能保持系统的一致性。

```rust
use std::collections::{HashMap, HashSet};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, RwLock};
use rand::Rng;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum NodeState {
    Follower,
    Candidate,
    Leader,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct LogEntry {
    term: u64,
    command: Vec<u8>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
enum RaftMessage {
    // 选举相关消息
    RequestVote {
        term: u64,
        candidate_id: u64,
        last_log_index: u64,
        last_log_term: u64,
    },
    VoteResponse {
        term: u64,
        vote_granted: bool,
    },
    
    // 日志复制相关消息
    AppendEntries {
        term: u64,
        leader_id: u64,
        prev_log_index: u64,
        prev_log_term: u64,
        entries: Vec<LogEntry>,
        leader_commit: u64,
    },
    AppendResponse {
        term: u64,
        success: bool,
        last_index: u64,
    },
    
    // 客户端请求
    ClientRequest {
        command: Vec<u8>,
        response_tx: mpsc::Sender<ClientResponse>,
    },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct ClientResponse {
    success: bool,
    result: Vec<u8>,
}

struct RaftNode {
    // 节点标识
    id: u64,
    peers: Vec<u64>,
    
    // 节点状态
    state: Arc<Mutex<NodeState>>,
    current_term: Arc<Mutex<u64>>,
    voted_for: Arc<Mutex<Option<u64>>>,
    
    // 日志和状态机
    log: Arc<Mutex<Vec<LogEntry>>>,
    commit_index: Arc<Mutex<u64>>,
    last_applied: Arc<Mutex<u64>>,
    state_machine: Arc<RwLock<HashMap<Vec<u8>, Vec<u8>>>>,
    
    // 领导者状态
    next_index: Arc<Mutex<HashMap<u64, u64>>>,
    match_index: Arc<Mutex<HashMap<u64, u64>>>,
    
    // 消息传递
    message_tx: HashMap<u64, mpsc::Sender<RaftMessage>>,
    message_rx: mpsc::Receiver<RaftMessage>,
    
    // 定时器
    last_heartbeat: Arc<Mutex<Instant>>,
    election_timeout: Duration,
    heartbeat_interval: Duration,
}

impl RaftNode {
    fn new(
        id: u64,
        peers: Vec<u64>,
        message_tx: HashMap<u64, mpsc::Sender<RaftMessage>>,
        message_rx: mpsc::Receiver<RaftMessage>,
    ) -> Self {
        // 随机化选举超时
        let mut rng = rand::thread_rng();
        let election_timeout = Duration::from_millis(rng.gen_range(150..300));
        let heartbeat_interval = Duration::from_millis(50);
        
        Self {
            id,
            peers,
            state: Arc::new(Mutex::new(NodeState::Follower)),
            current_term: Arc::new(Mutex::new(0)),
            voted_for: Arc::new(Mutex::new(None)),
            log: Arc::new(Mutex::new(Vec::new())),
            commit_index: Arc::new(Mutex::new(0)),
            last_applied: Arc::new(Mutex::new(0)),
            state_machine: Arc::new(RwLock::new(HashMap::new())),
            next_index: Arc::new(Mutex::new(HashMap::new())),
            match_index: Arc::new(Mutex::new(HashMap::new())),
            message_tx,
            message_rx,
            last_heartbeat: Arc::new(Mutex::new(Instant::now())),
            election_timeout,
            heartbeat_interval,
        }
    }
    
    async fn run(&mut self) {
        // 启动选举定时器检查任务
        self.start_election_timer();
        
        // 主消息处理循环
        while let Some(message) = self.message_rx.recv().await {
            match message {
                RaftMessage::RequestVote { term, candidate_id, last_log_index, last_log_term } => {
                    self.handle_vote_request(term, candidate_id, last_log_index, last_log_term).await;
                },
                RaftMessage::VoteResponse { term, vote_granted } => {
                    self.handle_vote_response(term, vote_granted).await;
                },
                RaftMessage::AppendEntries { term, leader_id, prev_log_index, prev_log_term, entries, leader_commit } => {
                    self.handle_append_entries(term, leader_id, prev_log_index, prev_log_term, entries, leader_commit).await;
                },
                RaftMessage::AppendResponse { term, success, last_index } => {
                    self.handle_append_response(term, success, last_index).await;
                },
                RaftMessage::ClientRequest { command, response_tx } => {
                    self.handle_client_request(command, response_tx).await;
                }
            }
        }
    }
    
    fn start_election_timer(&self) {
        let id = self.id;
        let state = self.state.clone();
        let current_term = self.current_term.clone();
        let voted_for = self.voted_for.clone();
        let last_heartbeat = self.last_heartbeat.clone();
        let election_timeout = self.election_timeout;
        let peers = self.peers.clone();
        let message_tx = self.message_tx.clone();
        let log = self.log.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_millis(10));
            
            loop {
                interval.tick().await;
                
                // 检查是否需要开始选举
                let should_start_election = {
                    let current_state = *state.lock().unwrap();
                    let last_hb = *last_heartbeat.lock().unwrap();
                    
                    current_state != NodeState::Leader && 
                    last_hb.elapsed() > election_timeout
                };
                
                if should_start_election {
                    // 变成候选人
                    {
                        let mut state_guard = state.lock().unwrap();
                        *state_guard = NodeState::Candidate;
                    }
                    
                    // 增加当前任期
                    let term = {
                        let mut term_guard = current_term.lock().unwrap();
                        *term_guard += 1;
                        *term_guard
                    };
                    
                    // 投票给自己
                    {
                        let mut voted_guard = voted_for.lock().unwrap();
                        *voted_guard = Some(id);
                    }
                    
                    // 重置心跳时间
                    {
                        let mut hb_guard = last_heartbeat.lock().unwrap();
                        *hb_guard = Instant::now();
                    }
                    
                    // 获取最后一个日志条目的索引和任期
                    let (last_log_index, last_log_term) = {
                        let log_guard = log.lock().unwrap();
                        let idx = log_guard.len() as u64;
                        let term = if idx > 0 { log_guard[idx as usize - 1].term } else { 0 };
                        (idx, term)
                    };
                    
                    // 向所有对等节点发送投票请求
                    for &peer_id in &peers {
                        if let Some(tx) = message_tx.get(&peer_id) {
                            let vote_request = RaftMessage::RequestVote {
                                term,
                                candidate_id: id,
                                last_log_index,
                                last_log_term,
                            };
                            
                            let _ = tx.send(vote_request).await;
                        }
                    }
                }
            }
        });
    }
    
    async fn handle_vote_request(
        &self,
        term: u64,
        candidate_id: u64,
        last_log_index: u64,
        last_log_term: u64,
    ) {
        let mut vote_granted = false;
        
        // 如果接收到的任期大于当前任期，转换为跟随者
        {
            let mut current_term_guard = self.current_term.lock().unwrap();
            let mut state_guard = self.state.lock().unwrap();

/*
<think>
I need to continue where I left off, which is in the `handle_vote_request` method of the Raft implementation. 
Let me finish that method and continue with the rest of the Raft implementation and other distributed system patterns.
</think>
*/

            let mut voted_for_guard = self.voted_for.lock().unwrap();
            
            if term > *current_term_guard {
                *current_term_guard = term;
                *state_guard = NodeState::Follower;
                *voted_for_guard = None;
            }
            
            // 检查投票条件
            // 1. 投票者的任期大于等于当前任期
            // 2. 还未投票给任何人或已经投票给candidate_id
            // 3. 候选人的日志至少和我们的一样新
            let log_guard = self.log.lock().unwrap();
            let my_last_idx = log_guard.len() as u64;
            let my_last_term = if my_last_idx > 0 { log_guard[my_last_idx as usize - 1].term } else { 0 };
            
            let log_is_up_to_date = 
                last_log_term > my_last_term || 
                (last_log_term == my_last_term && last_log_index >= my_last_idx);
            
            if term >= *current_term_guard && 
               (voted_for_guard.is_none() || *voted_for_guard == Some(candidate_id)) &&
               log_is_up_to_date 
            {
                *voted_for_guard = Some(candidate_id);
                vote_granted = true;
                
                // 重置选举超时
                let mut last_hb = self.last_heartbeat.lock().unwrap();
                *last_hb = Instant::now();
            }
        }
        
        // 发送投票响应
        if let Some(tx) = self.message_tx.get(&candidate_id) {
            let current_term = *self.current_term.lock().unwrap();
            let vote_response = RaftMessage::VoteResponse {
                term: current_term,
                vote_granted,
            };
            
            let _ = tx.send(vote_response).await;
        }
    }
    
    async fn handle_append_entries(
        &self,
        term: u64,
        leader_id: u64,
        prev_log_index: u64,
        prev_log_term: u64,
        entries: Vec<LogEntry>,
        leader_commit: u64,
    ) {
        let mut success = false;
        let mut last_index = 0;
        
        // 如果领导者的任期小于当前任期，拒绝请求
        {
            let mut current_term_guard = self.current_term.lock().unwrap();
            if term < *current_term_guard {
                // 发送失败响应
                if let Some(tx) = self.message_tx.get(&leader_id) {
                    let append_response = RaftMessage::AppendResponse {
                        term: *current_term_guard,
                        success: false,
                        last_index: 0,
                    };
                    
                    let _ = tx.send(append_response).await;
                }
                return;
            }
            
            // 如果接收到更高任期，更新当前任期并转换为跟随者
            if term > *current_term_guard {
                *current_term_guard = term;
                let mut state_guard = self.state.lock().unwrap();
                *state_guard = NodeState::Follower;
                let mut voted_for_guard = self.voted_for.lock().unwrap();
                *voted_for_guard = None;
            }
            
            // 重置选举超时（接收到有效的附加条目请求）
            let mut last_hb = self.last_heartbeat.lock().unwrap();
            *last_hb = Instant::now();
            
            // 处理日志条目
            let mut log_guard = self.log.lock().unwrap();
            
            // 检查前一个日志条目是否匹配
            if prev_log_index > 0 {
                if log_guard.len() < prev_log_index as usize ||
                   log_guard[prev_log_index as usize - 1].term != prev_log_term {
                    // 日志不匹配
                    success = false;
                    last_index = log_guard.len() as u64;
                } else {
                    // 日志匹配，添加新条目
                    success = true;
                    
                    // 删除冲突的条目
                    if prev_log_index < log_guard.len() as u64 {
                        log_guard.truncate(prev_log_index as usize);
                    }
                    
                    // 添加新条目
                    for entry in &entries {
                        log_guard.push(entry.clone());
                    }
                    
                    last_index = (prev_log_index + entries.len() as u64) as u64;
                }
            } else {
                // 如果是第一个条目
                success = true;
                log_guard.clear();
                for entry in &entries {
                    log_guard.push(entry.clone());
                }
                last_index = entries.len() as u64;
            }
            
            // 如果领导者提交索引大于当前提交索引，更新提交索引
            if success && leader_commit > *self.commit_index.lock().unwrap() {
                let mut commit_guard = self.commit_index.lock().unwrap();
                *commit_guard = std::cmp::min(leader_commit, last_index);
                
                // 应用提交的条目到状态机
                self.apply_committed_entries().await;
            }
        }
        
        // 发送响应
        if let Some(tx) = self.message_tx.get(&leader_id) {
            let current_term = *self.current_term.lock().unwrap();
            let append_response = RaftMessage::AppendResponse {
                term: current_term,
                success,
                last_index,
            };
            
            let _ = tx.send(append_response).await;
        }
    }
    
    async fn apply_committed_entries(&self) {
        let commit_index = *self.commit_index.lock().unwrap();
        let mut last_applied = self.last_applied.lock().unwrap();
        
        if commit_index > *last_applied {
            let log_entries = {
                let log_guard = self.log.lock().unwrap();
                log_guard[(last_applied as usize)..=(commit_index as usize - 1)].to_vec()
            };
            
            for entry in log_entries {
                // 简化的命令执行 - 假设命令是键值对
                if let Ok((key, value)) = bincode::deserialize::<(Vec<u8>, Vec<u8>)>(&entry.command) {
                    let mut state_machine = self.state_machine.write().await;
                    state_machine.insert(key, value);
                }
                
                *last_applied += 1;
            }
        }
    }
    
    async fn start_leader_heartbeat(&self) {
        let state = self.state.clone();
        let id = self.id;
        let peers = self.peers.clone();
        let message_tx = self.message_tx.clone();
        let current_term = self.current_term.clone();
        let log = self.log.clone();
        let commit_index = self.commit_index.clone();
        let next_index = self.next_index.clone();
        let match_index = self.match_index.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_millis(100));
            
            loop {
                interval.tick().await;
                
                // 只有领导者发送心跳
                let is_leader = {
                    let state_guard = state.lock().unwrap();
                    *state_guard == NodeState::Leader
                };
                
                if !is_leader {
                    continue;
                }
                
                let term = *current_term.lock().unwrap();
                
                // 为每个对等节点发送附加条目请求（心跳）
                for &peer_id in &peers {
                    let next_idx = {
                        let next_idx_guard = next_index.lock().unwrap();
                        *next_idx_guard.get(&peer_id).unwrap_or(&1)
                    };
                    
                    let prev_log_index = next_idx - 1;
                    let prev_log_term = if prev_log_index > 0 {
                        let log_guard = log.lock().unwrap();
                        if prev_log_index as usize <= log_guard.len() {
                            log_guard[prev_log_index as usize - 1].term
                        } else { 0 }
                    } else { 0 };
                    
                    // 构建要发送的条目
                    let entries = {
                        let log_guard = log.lock().unwrap();
                        if next_idx as usize <= log_guard.len() {
                            log_guard[(next_idx as usize - 1)..].to_vec()
                        } else {
                            Vec::new() // 空条目 = 心跳
                        }
                    };
                    
                    let leader_commit = *commit_index.lock().unwrap();
                    
                    // 发送附加条目请求
                    if let Some(tx) = message_tx.get(&peer_id) {
                        let append_request = RaftMessage::AppendEntries {
                            term,
                            leader_id: id,
                            prev_log_index,
                            prev_log_term,
                            entries,
                            leader_commit,
                        };
                        
                        let _ = tx.send(append_request).await;
                    }
                }
            }
        });
    }
    
    async fn handle_client_request(&self, command: Vec<u8>, response_tx: mpsc::Sender<ClientResponse>) {
        // 检查是否是领导者
        let is_leader = {
            let state_guard = self.state.lock().unwrap();
            *state_guard == NodeState::Leader
        };
        
        if !is_leader {
            let _ = response_tx.send(ClientResponse {
                success: false,
                result: b"not leader".to_vec(),
            }).await;
            return;
        }
        
        // 添加到日志
        let term = *self.current_term.lock().unwrap();
        let log_entry = LogEntry {
            term,
            command: command.clone(),
        };
        
        let log_index = {
            let mut log_guard = self.log.lock().unwrap();
            log_guard.push(log_entry);
            log_guard.len() as u64
        };
        
        // 等待日志提交
        let commit_future = async {
            loop {
                let commit_index = *self.commit_index.lock().unwrap();
                if commit_index >= log_index {
                    break;
                }
                tokio::time::sleep(Duration::from_millis(10)).await;
            }
        };
        
        // 设置超时
        let timeout = tokio::time::sleep(Duration::from_secs(5));
        
        tokio::select! {
            _ = commit_future => {
                // 日志已提交，获取结果
                let state_machine = self.state_machine.read().await;
                
                // 简化版 - 假设命令是获取操作
                if let Ok((op, key)) = bincode::deserialize::<(String, Vec<u8>)>(&command) {
                    if op == "get" {
                        let result = state_machine.get(&key).cloned().unwrap_or_default();
                        let _ = response_tx.send(ClientResponse {
                            success: true,
                            result,
                        }).await;
                    }
                } else {
                    let _ = response_tx.send(ClientResponse {
                        success: true,
                        result: b"command executed".to_vec(),
                    }).await;
                }
            }
            _ = timeout => {
                let _ = response_tx.send(ClientResponse {
                    success: false,
                    result: b"timeout".to_vec(),
                }).await;
            }
        }
    }
    
    async fn handle_message(&self, message: RaftMessage) {
        match message {
            RaftMessage::RequestVote { term, candidate_id, last_log_index, last_log_term } => {
                self.handle_vote_request(term, candidate_id, last_log_index, last_log_term).await;
            },
            RaftMessage::VoteResponse { term, vote_granted } => {
                // 如果收到的任期大于当前任期，转为跟随者
                {
                    let mut current_term_guard = self.current_term.lock().unwrap();
                    if term > *current_term_guard {
                        *current_term_guard = term;
                        let mut state_guard = self.state.lock().unwrap();
                        *state_guard = NodeState::Follower;
                        let mut voted_for_guard = self.voted_for.lock().unwrap();
                        *voted_for_guard = None;
                        return;
                    }
                }
                
                // 如果不是候选人，忽略
                {
                    let state_guard = self.state.lock().unwrap();
                    if *state_guard != NodeState::Candidate {
                        return;
                    }
                }
                
                // 计票
                if vote_granted {
                    let mut votes = self.votes_received.lock().unwrap();
                    votes.insert(term);
                    
                    // 检查是否获得多数票
                    let majority = (self.peers.len() + 1) / 2 + 1;
                    if votes.len() >= majority {
                        // 成为领导者
                        let mut state_guard = self.state.lock().unwrap();
                        *state_guard = NodeState::Leader;
                        
                        // 初始化leader状态
                        let log_length = self.log.lock().unwrap().len() as u64 + 1;
                        let mut next_idx = self.next_index.lock().unwrap();
                        let mut match_idx = self.match_index.lock().unwrap();
                        
                        for &peer_id in &self.peers {
                            next_idx.insert(peer_id, log_length);
                            match_idx.insert(peer_id, 0);
                        }
                        
                        // 开始发送心跳
                        self.start_leader_heartbeat().await;
                    }
                }
            },
            RaftMessage::AppendEntries { term, leader_id, prev_log_index, prev_log_term, entries, leader_commit } => {
                self.handle_append_entries(term, leader_id, prev_log_index, prev_log_term, entries, leader_commit).await;
            },
            RaftMessage::AppendResponse { term, success, last_index } => {
                // 如果收到的任期大于当前任期，转为跟随者
                {
                    let mut current_term_guard = self.current_term.lock().unwrap();
                    if term > *current_term_guard {
                        *current_term_guard = term;
                        let mut state_guard = self.state.lock().unwrap();
                        *state_guard = NodeState::Follower;
                        let mut voted_for_guard = self.voted_for.lock().unwrap();
                        *voted_for_guard = None;
                        return;
                    }
                }
                
                // 如果不是领导者，忽略
                {
                    let state_guard = self.state.lock().unwrap();
                    if *state_guard != NodeState::Leader {
                        return;
                    }
                }
                
                // 处理复制响应
                if success {
                    // 更新matchIndex和nextIndex
                    {
                        let mut next_idx = self.next_index.lock().unwrap();
                        let mut match_idx = self.match_index.lock().unwrap();
                        
                        match_idx.insert(last_index, last_index);
                        next_idx.insert(last_index, last_index + 1);
                    }
                    
                    // 检查是否可以提交更多条目
                    self.update_commit_index().await;
                } else {
                    // 如果复制失败，减少nextIndex并重试
                    let mut next_idx = self.next_index.lock().unwrap();
                    if let Some(idx) = next_idx.get_mut(&last_index) {
                        *idx = std::cmp::max(1, last_index);
                    }
                }
            },
            RaftMessage::ClientRequest { command, response_tx } => {
                self.handle_client_request(command, response_tx).await;
            }
        }
    }
    
    async fn update_commit_index(&self) {
        let current_term = *self.current_term.lock().unwrap();
        let log_len = self.log.lock().unwrap().len() as u64;
        
        for n in (*self.commit_index.lock().unwrap() + 1)..=log_len {
            let mut count = 1; // 包括自己
            
            // 检查是否有多数节点匹配此索引
            for &peer_id in &self.peers {
                let match_idx = {
                    let match_idx_guard = self.match_index.lock().unwrap();
                    *match_idx_guard.get(&peer_id).unwrap_or(&0)
                };
                
                if match_idx >= n {
                    count += 1;
                }
            }
            
            // 多数节点匹配且是当前任期的条目
            let is_current_term = {
                let log_guard = self.log.lock().unwrap();
                n > 0 && n <= log_guard.len() as u64 && log_guard[n as usize - 1].term == current_term
            };
            
            if count > self.peers.len() / 2 && is_current_term {
                let mut commit_guard = self.commit_index.lock().unwrap();
                *commit_guard = n;
                
                // 应用已提交的条目
                self.apply_committed_entries().await;
            }
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建三个Raft节点
    let node1_id = 1;
    let node2_id = 2;
    let node3_id = 3;
    
    let peers = vec![node1_id, node2_id, node3_id];
    
    // 设置通信通道
    let (node1_tx, mut node1_rx) = mpsc::channel(100);
    let (node2_tx, mut node2_rx) = mpsc::channel(100);
    let (node3_tx, mut node3_rx) = mpsc::channel(100);
    
    let mut message_channels = HashMap::new();
    message_channels.insert(node1_id, node1_tx);
    message_channels.insert(node2_id, node2_tx);
    message_channels.insert(node3_id, node3_tx);
    
    // 创建节点
    let node1 = Arc::new(RaftNode::new(node1_id, peers.clone(), message_channels.clone()));
    let node2 = Arc::new(RaftNode::new(node2_id, peers.clone(), message_channels.clone()));
    let node3 = Arc::new(RaftNode::new(node3_id, peers.clone(), message_channels.clone()));
    
    // 启动节点消息处理
    let node1_clone = node1.clone();
    tokio::spawn(async move {
        while let Some(message) = node1_rx.recv().await {
            node1_clone.handle_message(message).await;
        }
    });
    
    let node2_clone = node2.clone();
    tokio::spawn(async move {
        while let Some(message) = node2_rx.recv().await {
            node2_clone.handle_message(message).await;
        }
    });
    
    let node3_clone = node3.clone();
    tokio::spawn(async move {
        while let Some(message) = node3_rx.recv().await {
            node3_clone.handle_message(message).await;
        }
    });
    
    // 启动选举超时
    node1.start_election_timeout().await;
    node2.start_election_timeout().await;
    node3.start_election_timeout().await;
    
    // 等待一段时间让选举完成
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // 找出领导者
    let leader = if *node1.state.lock().unwrap() == NodeState::Leader {
        node1.clone()
    } else if *node2.state.lock().unwrap() == NodeState::Leader {
        node2.clone()
    } else {
        node3.clone()
    };
    
    println!("领导者是节点 {}", leader.id);
    
    // 发送客户端请求
    let (response_tx, mut response_rx) = mpsc::channel(1);
    
    // 设置键值对
    let set_command = bincode::serialize(&("set".to_string(), b"mykey".to_vec(), b"myvalue".to_vec())).unwrap();
    leader.handle_message(RaftMessage::ClientRequest {
        command: set_command,
        response_tx: response_tx.clone(),
    }).await;
    
    // 等待响应
    let response = response_rx.recv().await.unwrap();
    println!("设置键值对响应: {:?}", response);
    
    // 等待复制完成
    tokio::time::sleep(Duration::from_secs(1)).await;
    
    // 获取键值
    let get_command = bincode::serialize(&("get".to_string(), b"mykey".to_vec())).unwrap();
    leader.handle_message(RaftMessage::ClientRequest {
        command: get_command,
        response_tx,
    }).await;
    
    // 等待响应
    let response = response_rx.recv().await.unwrap();
    println!("获取键值对响应: {:?}", response);
    
    // 检查所有节点的状态机是否一致
    let value1 = node1.state_machine.read().await.get(&b"mykey".to_vec()).cloned();
    let value2 = node2.state_machine.read().await.get(&b"mykey".to_vec()).cloned();
    let value3 = node3.state_machine.read().await.get(&b"mykey".to_vec()).cloned();
    
    println!("节点1中的值: {:?}", value1);
    println!("节点2中的值: {:?}", value2);
    println!("节点3中的值: {:?}", value3);
}
```

### 2.5 最终一致性

最终一致性模式保证在没有新的更新的情况下，最终所有副本将收敛到相同的值。

```rust
use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::{Arc, Mutex};
use tokio::sync::{mpsc, RwLock};
use tokio::time::Duration;
use serde::{Serialize, Deserialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
struct VersionVector {
    node_id: String,
    counter: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Operation<K, V> {
    op_id: String,
    key: K,
    value: Option<V>,
    timestamp: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct DataItem<K, V> {
    key: K,
    value: V,
    version_vector: HashMap<String, u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
enum EventualMessage<K, V> {
    Update { item: DataItem<K, V> },
    Sync { version_vector: HashMap<String, u64> },
    SyncResponse { items: Vec<DataItem<K, V>> },
}

struct EventualConsistencyNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    node_id: String,
    data: Arc<RwLock<HashMap<K, DataItem<K, V>>>>,
    version_vector: Arc<Mutex<HashMap<String, u64>>>,
    applied_ops: Arc<Mutex<HashSet<String>>>,
    peers: Vec<mpsc::Sender<EventualMessage<K, V>>>,
    anti_entropy_interval: Duration,
}

impl<K, V> EventualConsistencyNode<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
{
    fn new(node_id: &str, anti_entropy_interval: Duration) -> Self {
        let mut version_vector = HashMap::new();
        version_vector.insert(node_id.to_string(), 0);
        
        Self {
            node_id: node_id.to_string(),
            data: Arc::new(RwLock::new(HashMap::new())),
            version_vector: Arc::new(Mutex::new(version_vector)),
            applied_ops: Arc::new(Mutex::new(HashSet::new())),
            peers: Vec::new(),
            anti_entropy_interval,
        }
    }
    
    fn add_peer(&mut self, peer_tx: mpsc::Sender<EventualMessage<K, V>>) {
        self.peers.push(peer_tx);
    }
    
    async fn put(&self, key: K, value: V) {
        // 更新版本向量
        let version_vector = {
            let mut vv = self.version_vector.lock().unwrap();
            let counter = vv.entry(self.node_id.clone()).or_insert(0);
            *counter += 1;
            vv.clone()
        };
        
        // 创建数据项
        let item = DataItem {
            key: key.clone(),
            value: value.clone(),
            version_vector: version_vector.clone(),
        };
        
        // 更新本地存储
        {
            let mut data = self.data.write().await;
            data.insert(key, item.clone());
        }
        
        // 发送更新到所有对等节点
        let update_message = EventualMessage::Update { item };
        for peer in &self.peers {
            let _ = peer.send(update_message.clone()).await;
        }
    }
    
    async fn get(&self, key: &K) -> Option<V> {
        let data = self.data.read().await;
        data.get(key).map(|item| item.value.clone())
    }
    
    async fn start_anti_entropy(&self) {
        let node_id = self.node_id.clone();
        let version_vector = self.version_vector.clone();
        let peers = self.peers.clone();
        let interval = self.anti_entropy_interval;
        
        tokio::spawn(async move {
            let mut interval_timer = tokio::time::interval(interval);
            
            loop {
                interval_timer.tick().await;
                
                // 发送同步请求给所有对等节点
                let vv = {
                    let vv_guard = version_vector.lock().unwrap();
                    vv_guard.clone()
                };
                
                let sync_message = EventualMessage::Sync { version_vector: vv };
                for peer in &peers {
                    let _ = peer.send(sync_message.clone()).await;
                }
                
                // 等待下一次间隔
                tokio::time::sleep(Duration::from_millis(100)).await;
            }
        });
    }
    
    async fn handle_message(&self, message: EventualMessage<K, V>) {
        match message {
            EventualMessage::Update { item } => {
                self.merge_item(item).await;
            },
            EventualMessage::Sync { version_vector } => {
                // 比较版本向量，找出对方缺少的项
                let items_to_send = self.find_newer_items(&version_vector).await;
                
                // 构建同步响应
                let sync_response = EventualMessage::SyncResponse { items: items_to_send };
                
                // 在实际实现中，我们需要知道如何将响应路由回请求者
                // 这里简化处理，假设所有对等方都接收响应
                for peer in &self.peers {
                    let _ = peer.send(sync_response.clone()).await;
                }
            },
            EventualMessage::SyncResponse { items } => {
                // 合并收到的所有项
                for item in items {
                    self.merge_item(item).await;
                }
            }
        }
    }
    
    async fn merge_item(&self, item: DataItem<K, V>) {
        let should_update = {
            let data = self.data.read().await;
            if let Some(existing_item) = data.get(&item.key) {
                // 比较版本向量
                self.is_newer_than(&item.version_vector, &existing_item.version_vector)
            } else {
                // 新项目
                true
            }
        };
        
        if should_update {
            // 更新本地数据
            let mut data = self.data.write().await;
            data.insert(item.key.clone(), item.clone());
            
            // 更新版本向量
            let mut vv = self.version_vector.lock().unwrap();
            for (node, counter) in &item.version_vector {
                let local_counter = vv.entry(node.clone()).or_insert(0);
                *local_counter = std::cmp::max(*local_counter, *counter);
            }
        }
    }
    
    fn is_newer_than(&self, vv1: &HashMap<String, u64>, vv2: &HashMap<String, u64>) -> bool {
        let mut is_strictly_newer = false;
        
        // 对于vv1中的每个条目
        for (node, counter1) in vv1 {
            let counter2 = vv2.get(node).copied().unwrap_or(0);
            
            if *counter1 < counter2 {
                // vv1比vv2旧
                return false;
            } else if *counter1 > counter2 {
                // vv1至少在一个节点上比vv2新
                is_strictly_newer = true;
            }
        }
        
        // 对于vv2中的每个节点，检查vv1是否有该节点的计数器
        for (node, counter2) in vv2 {
            if !vv1.contains_key(node) && *counter2 > 0 {
                // vv2有vv1没有的节点信息
                return false;
            }
        }
        
        is_strictly_newer
    }
    
    async fn find_newer_items(&self, remote_vv: &HashMap<String, u64>) -> Vec<DataItem<K, V>> {
        let mut newer_items = Vec::new();
        let data = self.data.read().await;
        
        for item in data.values() {
            if self.is_newer_than(&item.version_vector, remote_vv) {
                newer_items.push(item.clone());
            }
        }
        
        newer_items
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建三个节点
    let mut node1 = EventualConsistencyNode::<String, String>::new(
        "node1", 
        Duration::from_secs(5)
    );
    
    let mut node2 = EventualConsistencyNode::<String, String>::new(
        "node2", 
        Duration::from_secs(5)
    );
    
    let mut node3 = EventualConsistencyNode::<String, String>::new(
        "node3", 
        Duration::from_secs(5)
    );
    
    // 设置通信通道
    let (node1_tx, mut node1_rx) = mpsc::channel(100);
    let (node2_tx, mut node2_rx) = mpsc::channel(100);
    let (node3_tx, mut node3_rx) = mpsc::channel(100);
    
    node1.add_peer(node2_tx.clone());
    node1.add_peer(node3_tx.clone());
    
    node2.add_peer(node1_tx.clone());
    node2.add_peer(node3_tx.clone());
    
    node3.add_peer(node1_tx.clone());
    node3.add_peer(node2_tx.clone());
    
    // 启动消息处理
    let node1_clone = Arc::new(node1);
    let node1_handle = {
        let node = node1_clone.clone();
        tokio::spawn(async move {
            while let Some(message) = node1_rx.recv().await {
                node.handle_message(message).await;
            }
        })
    };
    
    let node2_clone = Arc::new(node2);
    let node2_handle = {
        let node = node2_clone.clone();
        tokio::spawn(async move {
            while let Some(message) = node2_rx.recv().await {
                node.handle_message(message).await;
            }
        })
    };
    
    let node3_clone = Arc::new(node3);
    let node3_handle = {
        let node = node3_clone.clone();
        tokio::spawn(async move {
            while let Some(message) = node3_rx.recv().await {
                node.handle_message(message).await;
            }
        })
    };
    
    // 启动反熵
    node1_clone.start_anti_entropy().await;
    node2_clone.start_anti_entropy().await;
    node3_clone.start_anti_entropy().await;
    
    // 模拟网络分区
    // 节点1和节点2与节点3断开连接
    // 在节点1和节点2上写入数据
    node1_clone.put("key1".to_string(), "value1-from-node1".to_string()).await;
    node2_clone.put("key2".to_string(), "value2-from-node2".to_string()).await;
    
    // 在节点3上写入不同的数据
    node3_clone.put("key3".to_string(), "value3-from-node3".to_string()).await;
    
    // 等待一段时间，然后检查所有节点
    tokio::time::sleep(Duration::from_secs(10)).await;
    
    // 检查所有节点是否最终一致
    let value1_from_node1 = node1_clone.get(&"key1".to_string()).await;
    let value1_from_node2 = node2_clone.get(&"key1".to_string()).await;
    let value1_from_node3 = node3_clone.get(&"key1".to_string()).await;
    
    let value2_from_node1 = node1_clone.get(&"key2".to_string()).await;
    let value2_from_node2 = node2_clone.get(&"key2".to_string()).await;
    let value2_from_node3 = node3_clone.get(&"key2".to_string()).await;
    
    let value3_from_node1 = node1_clone.get(&"key3".to_string()).await;
    let value3_from_node2 = node2_clone.get(&"key3".to_string()).await;
    let value3_from_node3 = node3_clone.get(&"key3".to_string()).await;
    
    println!("Node1 - key1: {:?}, key2: {:?}, key3: {:?}", 
             value1_from_node1, value2_from_node1, value3_from_node1);
    println!("Node2 - key1: {:?}, key2: {:?}, key3: {:?}", 
             value1_from_node2, value2_from_node2, value3_from_node2);
    println!("Node3 - key1: {:?}, key2: {:?}, key3: {:?}", 
             value1_from_node3, value2_from_node3, value3_from_node3);
}
```

```text
<think>
我需要继续提供关于分布式系统设计模式的Rust实现。根据原始文档，我还需要涵盖以下部分：

3. 分区模式
   - 分片
   - 一致性哈希
   - 副本分布

我将首先实现这些分区模式，然后继续实现后续各部分。
</think>
```

## 3. 分区模式

### 3.1 分片

分片是将数据水平分区到多个节点的技术，每个节点负责数据的一个子集。

```rust
use std::collections::HashMap;
use std::hash::{Hash, Hasher};
use std::sync::{Arc, Mutex};
use tokio::sync::{mpsc, RwLock};
use serde::{Serialize, Deserialize};

// 分片函数特征
trait ShardingFunction<K> {
    fn shard(&self, key: &K) -> usize;
}

// 简单的取模分片函数
struct ModuloSharding {
    shard_count: usize,
}

impl<K> ShardingFunction<K> for ModuloSharding
where
    K: Hash,
{
    fn shard(&self, key: &K) -> usize {
        let mut hasher = std::collections::hash_map::DefaultHasher::new();
        key.hash(&mut hasher);
        (hasher.finish() as usize) % self.shard_count
    }
}

// 范围分片函数
struct RangeSharding<K> {
    boundaries: Vec<K>,
}

impl<K> ShardingFunction<K> for RangeSharding<K>
where
    K: Ord + Clone,
{
    fn shard(&self, key: &K) -> usize {
        match self.boundaries.binary_search(key) {
            Ok(exact) => exact,
            Err(insertion_point) => insertion_point,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
enum ShardMessage<K, V> {
    Get { key: K, respond_to: String },
    GetResponse { key: K, value: Option<V> },
    Put { key: K, value: V },
    Delete { key: K },
}

struct ShardNode<K, V> {
    id: String,
    data: Arc<RwLock<HashMap<K, V>>>,
    peers: HashMap<usize, mpsc::Sender<ShardMessage<K, V>>>,
    shard_function: Box<dyn ShardingFunction<K> + Send + Sync>,
}

impl<K, V> ShardNode<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new<F>(id: &str, shard_function: F) -> Self
    where
        F: ShardingFunction<K> + Send + Sync + 'static,
    {
        Self {
            id: id.to_string(),
            data: Arc::new(RwLock::new(HashMap::new())),
            peers: HashMap::new(),
            shard_function: Box::new(shard_function),
        }
    }
    
    fn add_peer(&mut self, shard_id: usize, tx: mpsc::Sender<ShardMessage<K, V>>) {
        self.peers.insert(shard_id, tx);
    }
    
    // 本地存储键值对
    async fn local_put(&self, key: K, value: V) {
        let mut data = self.data.write().await;
        data.insert(key, value);
    }
    
    // 本地获取值
    async fn local_get(&self, key: &K) -> Option<V> {
        let data = self.data.read().await;
        data.get(key).cloned()
    }
    
    // 本地删除键
    async fn local_delete(&self, key: &K) {
        let mut data = self.data.write().await;
        data.remove(key);
    }
    
    // 根据键确定分片并执行操作
    async fn get(&self, key: K) -> Option<V> 
    where
        K: Serialize + for<'de> Deserialize<'de>,
        V: Serialize + for<'de> Deserialize<'de>,
    {
        let shard_id = self.shard_function.shard(&key);
        
        if let Some(tx) = self.peers.get(&shard_id) {
            // 键属于另一个分片
            let (response_tx, mut response_rx) = mpsc::channel(1);
            
            // 发送获取请求
            let message = ShardMessage::Get { 
                key: key.clone(), 
                respond_to: self.id.clone() 
            };
            
            if tx.send(message).await.is_err() {
                return None; // 发送失败
            }
            
            // 等待响应
            if let Some(response) = response_rx.recv().await {
                match response {
                    ShardMessage::GetResponse { key: _, value } => value,
                    _ => None,
                }
            } else {
                None
            }
        } else {
            // 键属于当前分片
            self.local_get(&key).await
        }
    }
    
    async fn put(&self, key: K, value: V) 
    where
        K: Serialize + for<'de> Deserialize<'de>,
        V: Serialize + for<'de> Deserialize<'de>,
    {
        let shard_id = self.shard_function.shard(&key);
        
        if let Some(tx) = self.peers.get(&shard_id) {
            // 键属于另一个分片
            let message = ShardMessage::Put { 
                key, 
                value 
            };
            
            let _ = tx.send(message).await;
        } else {
            // 键属于当前分片
            self.local_put(key, value).await;
        }
    }
    
    async fn delete(&self, key: K) 
    where
        K: Serialize + for<'de> Deserialize<'de>,
    {
        let shard_id = self.shard_function.shard(&key);
        
        if let Some(tx) = self.peers.get(&shard_id) {
            // 键属于另一个分片
            let message = ShardMessage::Delete { key };
            
            let _ = tx.send(message).await;
        } else {
            // 键属于当前分片
            self.local_delete(&key).await;
        }
    }
    
    async fn handle_message(&self, message: ShardMessage<K, V>)
    where
        K: Serialize + for<'de> Deserialize<'de>,
        V: Serialize + for<'de> Deserialize<'de>,
    {
        match message {
            ShardMessage::Get { key, respond_to } => {
                let value = self.local_get(&key).await;
                
                if let Some(tx) = self.peers.get(&self.shard_function.shard(&respond_to)) {
                    let response = ShardMessage::GetResponse {
                        key,
                        value,
                    };
                    
                    let _ = tx.send(response).await;
                }
            },
            ShardMessage::Put { key, value } => {
                self.local_put(key, value).await;
            },
            ShardMessage::Delete { key } => {
                self.local_delete(&key).await;
            },
            _ => {}, // 忽略其他消息类型
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建一个3分片的系统
    let shard_count = 3;
    let sharding_function = ModuloSharding { shard_count };
    
    let mut node1 = ShardNode::<String, String>::new("node1", sharding_function.clone());
    let mut node2 = ShardNode::<String, String>::new("node2", sharding_function.clone());
    let mut node3 = ShardNode::<String, String>::new("node3", sharding_function);
    
    // 设置通信通道
    let (node1_tx, mut node1_rx) = mpsc::channel(100);
    let (node2_tx, mut node2_rx) = mpsc::channel(100);
    let (node3_tx, mut node3_rx) = mpsc::channel(100);
    
    node1.add_peer(0, node1_tx.clone());
    node1.add_peer(1, node2_tx.clone());
    node1.add_peer(2, node3_tx.clone());
    
    node2.add_peer(0, node1_tx.clone());
    node2.add_peer(1, node2_tx.clone());
    node2.add_peer(2, node3_tx.clone());
    
    node3.add_peer(0, node1_tx);
    node3.add_peer(1, node2_tx);
    node3.add_peer(2, node3_tx);
    
    // 启动消息处理
    let node1_clone = Arc::new(node1);
    tokio::spawn(async move {
        while let Some(message) = node1_rx.recv().await {
            node1_clone.handle_message(message).await;
        }
    });
    
    let node2_clone = Arc::new(node2);
    tokio::spawn(async move {
        while let Some(message) = node2_rx.recv().await {
            node2_clone.handle_message(message).await;
        }
    });
    
    let node3_clone = Arc::new(node3);
    tokio::spawn(async move {
        while let Some(message) = node3_rx.recv().await {
            node3_clone.handle_message(message).await;
        }
    });
    
    // 通过node1写入一些数据
    for i in 0..10 {
        let key = format!("key{}", i);
        let value = format!("value{}", i);
        node1_clone.put(key, value).await;
    }
    
    // 等待数据分发
    tokio::time::sleep(tokio::time::Duration::from_millis(500)).await;
    
    // 从所有节点读取数据
    for i in 0..10 {
        let key = format!("key{}", i);
        let value1 = node1_clone.get(key.clone()).await;
        let value2 = node2_clone.get(key.clone()).await;
        let value3 = node3_clone.get(key.clone()).await;
        
        println!("Key: {}, Node1: {:?}, Node2: {:?}, Node3: {:?}", 
                 key, value1, value2, value3);
    }
    
    // 查看每个节点实际存储的数据
    let data1 = node1_clone.data.read().await;
    let data2 = node2_clone.data.read().await;
    let data3 = node3_clone.data.read().await;
    
    println!("Node1数据: {:?}", data1);
    println!("Node2数据: {:?}", data2);
    println!("Node3数据: {:?}", data3);
}
```

### 3.2 一致性哈希

一致性哈希是一种特殊的哈希算法，它最小化在添加或删除节点时需要重新映射的键数量。

```rust
use std::collections::{BTreeMap, HashMap};
use std::hash::{Hash, Hasher};
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use rand::Rng;

struct ConsistentHash<T> {
    ring: BTreeMap<u64, T>,
    virtual_nodes: usize,
}

impl<T: Clone> ConsistentHash<T> {
    fn new(virtual_nodes: usize) -> Self {
        Self {
            ring: BTreeMap::new(),
            virtual_nodes,
        }
    }
    
    fn add_node(&mut self, node: T, node_id: &str) {
        for i in 0..self.virtual_nodes {
            let key = format!("{}:{}", node_id, i);
            let hash = self.hash(&key);
            self.ring.insert(hash, node.clone());
        }
    }
    
    fn remove_node(&mut self, node_id: &str) {
        for i in 0..self.virtual_nodes {
            let key = format!("{}:{}", node_id, i);
            let hash = self.hash(&key);
            self.ring.remove(&hash);
        }
    }
    
    fn get_node<K: Hash>(&self, key: &K) -> Option<&T> {
        if self.ring.is_empty() {
            return None;
        }
        
        let hash = self.hash(key);
        
        // 找到第一个大于等于hash的节点
        match self.ring.range(hash..).next() {
            Some((_, node)) => Some(node),
            None => {
                // 如果没有找到，就返回第一个节点（环绕）
                self.ring.values().next()
            }
        }
    }
    
    fn hash<K: Hash>(&self, key: &K) -> u64 {
        let mut hasher = std::collections::hash_map::DefaultHasher::new();
        key.hash(&mut hasher);
        hasher.finish()
    }
}

#[derive(Debug, Clone)]
struct NodeInfo {
    id: String,
    address: String,
}

struct ConsistentHashRing {
    hash_ring: Arc<RwLock<ConsistentHash<NodeInfo>>>,
    node_connections: Arc<RwLock<HashMap<String, mpsc::Sender<Message>>>>,
}

#[derive(Debug, Clone)]
enum Message {
    Get { key: String, response_tx: mpsc::Sender<Option<String>> },
    Put { key: String, value: String },
    Delete { key: String },
}

impl ConsistentHashRing {
    fn new(virtual_nodes: usize) -> Self {
        Self {
            hash_ring: Arc::new(RwLock::new(ConsistentHash::new(virtual_nodes))),
            node_connections: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    async fn add_node(&self, node: NodeInfo, connection: mpsc::Sender<Message>) {
        let mut hash_ring = self.hash_ring.write().await;
        let mut connections = self.node_connections.write().await;
        
        hash_ring.add_node(node.clone(), &node.id);
        connections.insert(node.id.clone(), connection);
    }
    
    async fn remove_node(&self, node_id: &str) {
        let mut hash_ring = self.hash_ring.write().await;
        let mut connections = self.node_connections.write().await;
        
        hash_ring.remove_node(node_id);
        connections.remove(node_id);
    }
    
    async fn get(&self, key: &str) -> Option<String> {
        let hash_ring = self.hash_ring.read().await;
        let connections = self.node_connections.read().await;
        
        if let Some(node) = hash_ring.get_node(&key) {
            if let Some(tx) = connections.get(&node.id) {
                let (response_tx, mut response_rx) = mpsc::channel(1);
                
                let message = Message::Get {
                    key: key.to_string(),
                    response_tx,
                };
                
                if tx.send(message).await.is_ok() {
                    return response_rx.recv().await.flatten();
                }
            }
        }
        
        None
    }
    
    async fn put(&self, key: &str, value: &str) -> bool {
        let hash_ring = self.hash_ring.read().await;
        let connections = self.node_connections.read().await;
        
        if let Some(node) = hash_ring.get_node(&key) {
            if let Some(tx) = connections.get(&node.id) {
                let message = Message::Put {
                    key: key.to_string(),
                    value: value.to_string(),
                };
                
                return tx.send(message).await.is_ok();
            }
        }
        
        false
    }
    
    async fn delete(&self, key: &str) -> bool {
        let hash_ring = self.hash_ring.read().await;
        let connections = self.node_connections.read().await;
        
        if let Some(node) = hash_ring.get_node(&key) {
            if let Some(tx) = connections.get(&node.id) {
                let message = Message::Delete {
                    key: key.to_string(),
                };
                
                return tx.send(message).await.is_ok();
            }
        }
        
        false
    }
}

struct StorageNode {
    id: String,
    data: Arc<RwLock<HashMap<String, String>>>,
}

impl StorageNode {
    fn new(id: &str) -> Self {
        Self {
            id: id.to_string(),
            data: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    async fn start(&self) -> mpsc::Sender<Message> {
        let (tx, mut rx) = mpsc::channel(100);
        
        let data = self.data.clone();
        tokio::spawn(async move {
            while let Some(message) = rx.recv().await {
                match message {
                    Message::Get { key, response_tx } => {
                        let value = {
                            let data_guard = data.read().await;
                            data_guard.get(&key).cloned()
                        };
                        
                        let _ = response_tx.send(value).await;
                    },
                    Message::Put { key, value } => {
                        let mut data_guard = data.write().await;
                        data_guard.insert(key, value);
                    },
                    Message::Delete { key } => {
                        let mut data_guard = data.write().await;
                        data_guard.remove(&key);
                    },
                }
            }
        });
        
        tx
    }
    
    async fn get_data_size(&self) -> usize {
        let data = self.data.read().await;
        data.len()
    }
    
    async fn get_data(&self) -> HashMap<String, String> {
        let data = self.data.read().await;
        data.clone()
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建一致性哈希环
    let ring = ConsistentHashRing::new(10); // 每个节点10个虚拟节点
    
    // 创建存储节点
    let node1 = StorageNode::new("node1");
    let node2 = StorageNode::new("node2");
    let node3 = StorageNode::new("node3");
    
    // 启动节点并获取通信通道
    let node1_tx = node1.start().await;
    let node2_tx = node2.start().await;
    let node3_tx = node3.start().await;
    
    // 将节点添加到哈希环
    ring.add_node(
        NodeInfo { id: "node1".to_string(), address: "127.0.0.1:8081".to_string() }, 
        node1_tx
    ).await;
    
    ring.add_node(
        NodeInfo { id: "node2".to_string(), address: "127.0.0.1:8082".to_string() }, 
        node2_tx
    ).await;
    
    ring.add_node(
        NodeInfo { id: "node3".to_string(), address: "127.0.0.1:8083".to_string() }, 
        node3_tx
    ).await;
    
    // 写入一些数据
    let keys: Vec<String> = (0..100).map(|i| format!("key{}", i)).collect();
    
    for key in &keys {
        let value = format!("value-{}", key);
        ring.put(key, &value).await;
    }
    
    // 检查数据分布
    let node1_size = node1.get_data_size().await;
    let node2_size = node2.get_data_size().await;
    let node3_size = node3.get_data_size().await;
    
    println!("初始数据分布:");
    println!("Node1: {} 项", node1_size);
    println!("Node2: {} 项", node2_size);
    println!("Node3: {} 项", node3_size);
    
    // 读取一些数据
    for i in 0..10 {
        let key = &keys[i];
        let value = ring.get(key).await;
        println!("Key: {}, Value: {:?}", key, value);
    }
    
    // 添加一个新节点
    let node4 = StorageNode::new("node4");
    let node4_tx = node4.start().await;
    
    ring.add_node(
        NodeInfo { id: "node4".to_string(), address: "127.0.0.1:8084".to_string() }, 
        node4_tx
    ).await;
    
    // 重新分配数据 - 在实际系统中，可能需要一个重新平衡的过程
    // 这里我们简单地重新写入相同的数据
    for key in &keys {
        let value = format!("value-{}", key);
        ring.put(key, &value).await;
    }
    
    // 检查添加节点后的数据分布
    let node1_size = node1.get_data_size().await;
    let node2_size = node2.get_data_size().await;
    let node3_size = node3.get_data_size().await;
    let node4_size = node4.get_data_size().await;
    
    println!("\n添加节点后的数据分布:");
    println!("Node1: {} 项", node1_size);
    println!("Node2: {} 项", node2_size);
    println!("Node3: {} 项", node3_size);
    println!("Node4: {} 项", node4_size);
    
    // 移除一个节点
    ring.remove_node("node1").await;
    
    // 再次重新分配数据
    for key in &keys {
        let value = format!("value-{}", key);
        ring.put(key, &value).await;
    }
    
    // 检查移除节点后的数据分布
    let node2_size = node2.get_data_size().await;
    let node3_size = node3.get_data_size().await;
    let node4_size = node4.get_data_size().await;
    
    println!("\n移除节点后的数据分布:");
    println!("Node2: {} 项", node2_size);
    println!("Node3: {} 项", node3_size);
    println!("Node4: {} 项", node4_size);
    
    // 查看每个节点上的部分数据
    let node2_data = node2.get_data().await;
    let node3_data = node3.get_data().await;
    let node4_data = node4.get_data().await;
    
    println!("\n每个节点上的前5项数据:");
    println!("Node2: {:?}", node2_data.iter().take(5).collect::<Vec<_>>());
    println!("Node3: {:?}", node3_data.iter().take(5).collect::<Vec<_>>());
    println!("Node4: {:?}", node4_data.iter().take(5).collect::<Vec<_>>());
}
```

### 3.3 副本分布

副本分布模式控制数据副本在各节点中的分布，提高数据可用性和读取性能。

```rust
use std::collections::{HashMap, HashSet};
use std::hash::{Hash, Hasher};
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use rand::{thread_rng, Rng};
use serde::{Serialize, Deserialize};

enum ReplicaPlacementStrategy {
    // 简单策略：数据放在N个连续的节点上
    Adjacent { replica_count: usize },
    
    // 随机策略：数据放在随机选择的N个节点上
    Random { replica_count: usize },
}

struct ReplicaDistributor<T, N> {
    nodes: Vec<N>,
    placement_strategy: ReplicaPlacementStrategy,
    node_for_key: HashMap<T, Vec<usize>>,
}

impl<T, N> ReplicaDistributor<T, N>
where
    T: Eq + Hash + Clone,
    N: Clone,
{
    fn new(nodes: Vec<N>, strategy: ReplicaPlacementStrategy) -> Self {
        Self {
            nodes,
            placement_strategy,
            node_for_key: HashMap::new(),
        }
    }
    
    fn get_replica_nodes(&mut self, key: &T) -> Vec<N> {
        let node_indices = self.calculate_replica_indices(key);
        self.node_for_key.insert(key.clone(), node_indices.clone());
        
        node_indices.into_iter()
            .map(|idx| self.nodes[idx].clone())
            .collect()
    }
    
    fn calculate_replica_indices(&self, key: &T) -> Vec<usize> {
        match &self.placement_strategy {
            ReplicaPlacementStrategy::Adjacent { replica_count } => {
                let mut hasher = std::collections::hash_map::DefaultHasher::new();
                key.hash(&mut hasher);
                let hash = hasher.finish();
                
                let start_idx = (hash as usize) % self.nodes.len();
                let mut indices = Vec::with_capacity(*replica_count);
                
                for i in 0..*replica_count {
                    indices.push((start_idx + i) % self.nodes.len());
                }
                
                indices
            },
            ReplicaPlacementStrategy::Random { replica_count } => {
                let mut rng = thread_rng();
                let mut indices = HashSet::new();
                
                while indices.len() < *replica_count {
                    let idx = rng.gen_range(0..self.nodes.len());
                    indices.insert(idx);
                }
                
                indices.into_iter().collect()
            }
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
enum ReplicaMessage<K, V> {
    Put { key: K, value: V },
    Get { key: K, response_tx: mpsc::Sender<Option<V>> },
    Delete { key: K },
}

struct ReplicaNode<K, V> {
    id: String,
    data: Arc<RwLock<HashMap<K, V>>>,
}

impl<K, V> ReplicaNode<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new(id: &str) -> Self {
        Self {
            id: id.to_string(),
            data: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    async fn start(&self) -> mpsc::Sender<ReplicaMessage<K, V>> {
        let (tx, mut rx) = mpsc::channel(100);
        
        let data = self.data.clone();
        tokio::spawn(async move {
            while let Some(message) = rx.recv().await {
                match message {
                    ReplicaMessage::Put { key, value } => {
                        let mut data_guard = data.write().await;
                        data_guard.insert(key, value);
                    },
                    ReplicaMessage::Get { key, response_tx } => {
                        let value = {
                            let data_guard = data.read().await;
                            data_guard.get(&key).cloned()
                        };
                        
                        let _ = response_tx.send(value).await;
                    },
                    ReplicaMessage::Delete { key } => {
                        let mut data_guard = data.write().await;
                        data_guard.remove(&key);
                    },
                }
            }
        });
        
        tx
    }
    
    async fn get_data_size(&self) -> usize {
        let data = self.data.read().await;
        data.len()
    }
}

struct ReplicatedStorage<K, V> {
    replica_distributor: ReplicaDistributor<K, mpsc::Sender<ReplicaMessage<K, V>>>,
}

impl<K, V> ReplicatedStorage<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new(
        node_channels: Vec<mpsc::Sender<ReplicaMessage<K, V>>>,
        strategy: ReplicaPlacementStrategy,
    ) -> Self {
        Self {
            replica_distributor: ReplicaDistributor::new(node_channels, strategy),
        }
    }
    
    async fn put(&mut self, key: K, value: V) {
        let replica_nodes = self.replica_distributor.get_replica_nodes(&key);
        
        for node_tx in replica_nodes {
            let message = ReplicaMessage::Put {
                key: key.clone(),
                value: value.clone(),
            };
            
            let _ = node_tx.send(message).await;
        }
    }
    
    async fn get(&mut self, key: K) -> Option<V> {
        let replica_nodes = self.replica_distributor.get_replica_nodes(&key);
        
        if replica_nodes.is_empty() {
            return None;
        }
        
        // 从第一个可用副本获取
        let (response_tx, mut response_rx) = mpsc::channel(1);
        
        let message = ReplicaMessage::Get {
            key,
            response_tx,
        };
        
        // 尝试从第一个副本读取
        if replica_nodes[0].send(message).await.is_ok() {
            if let Some(value) = response_rx.recv().await {
                return value;
            }
        }
        
        None
    }
    
    async fn delete(&mut self, key: K) {
        let replica_nodes = self.replica_distributor.get_replica_nodes(&key);
        
        for node_tx in replica_nodes {
            let message = ReplicaMessage::Delete {
                key: key.clone(),
            };
            
            let _ = node_tx.send(message).await;
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建多个存储节点
    let node_count = 5;
    let mut nodes = Vec::with_capacity(node_count);
    let mut node_channels = Vec::with_capacity(node_count);
    
    for i in 0..node_count {
        let node = ReplicaNode::<String, String>::new(&format!("node{}", i));
        let channel = node.start().await;
        
        nodes.push(node);
        node_channels.push(channel);
    }
    
    // 创建具有相邻副本策略的存储
    let mut adjacent_storage = ReplicatedStorage::new(
        node_channels.clone(),
        ReplicaPlacementStrategy::Adjacent { replica_count: 3 },
    );
    
    // 写入一些数据
    for i in 0..20 {
        let key = format!("key{}", i);
        let value = format!("value{}", i);
        adjacent_storage.put(key, value).await;
    }
    
    // 检查数据分布
    println!("相邻副本策略的数据分布:");
    for (i, node) in nodes.iter().enumerate() {
        println!("Node{}: {} 项", i, node.get_data_size().await);
    }
    
    // 创建具有随机副本策略的存储
    let mut random_storage = ReplicatedStorage::new(
        node_channels.clone(),
        ReplicaPlacementStrategy::Random { replica_count: 3 },
    );
    
    // 清空节点数据
    for channel in &node_channels {
        for i in 0..20 {
            let message = ReplicaMessage::Delete {
                key: format!("key{}", i),
            };
            let _ = channel.send(message).await;
        }
    }
    
    // 使用随机策略写入数据
    for i in 0..20 {
        let key = format!("key{}", i);
        let value = format!("value{}", i);
        random_storage.put(key, value).await;
    }
    
    // 检查随机策略的数据分布
    println!("\n随机副本策略的数据分布:");
    for (i, node) in nodes.iter().enumerate() {
        println!("Node{}: {} 项", i, node.get_data_size().await);
    }
    
    // 读取数据
    for i in 0..5 {
        let key = format!("key{}", i);
        let value = random_storage.get(key.clone()).await;
        println!("读取 {}: {:?}", key, value);
    }
    
    // 删除一个节点并观察影响
    println!("\n模拟节点0失效:");
    // 我们不会真的删除节点，而是让它不再接收新的请求
    
    // 读取所有键，看看哪些受到影响
    for i in 0..20 {
        let key = format!("key{}", i);
        let value = random_storage.get(key.clone()).await;
        println!("读取 {}: {}", key, if value.is_some() { "成功" } else { "失败" });
    }
}
```

## 4. 容错模式

### 4.1 熔断器

熔断器模式用于防止失败的操作重复执行，特别是在远程服务不可用时。

```rust
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use tokio::time::sleep;
use std::future::Future;
use std::pin::Pin;
use async_trait::async_trait;

#[derive(Debug, Clone, Copy, PartialEq)]
enum CircuitState {
    Closed,    // 正常状态
    Open,      // 熔断状态
    HalfOpen,  // 探测状态
}

struct CircuitBreakerStats {
    success_count: AtomicUsize,
    failure_count: AtomicUsize,
    total_count: AtomicUsize,
    last_failure_time: AtomicU64,
}

impl CircuitBreakerStats {
    fn new() -> Self {
        Self {
            success_count: AtomicUsize::new(0),
            failure_count: AtomicUsize::new(0),
            total_count: AtomicUsize::new(0),
            last_failure_time: AtomicU64::new(0),
        }
    }
    
    fn record_success(&self) {
        self.success_count.fetch_add(1, Ordering::SeqCst);
        self.total_count.fetch_add(1, Ordering::SeqCst);
    }
    
    fn record_failure(&self) {
        self.failure_count.fetch_add(1, Ordering::SeqCst);
        self.total_count.fetch_add(1, Ordering::SeqCst);
        self.last_failure_time.store(
            Instant::now().elapsed().as_secs(),
            Ordering::SeqCst,
        );
    }
    
    fn reset(&self) {
        self.success_count.store(0, Ordering::SeqCst);
        self.failure_count.store(0, Ordering::SeqCst);
    }
    
    fn failure_rate(&self) -> f64 {
        let failures = self.failure_count.load(Ordering::SeqCst);
        let total = self.total_count.load(Ordering::SeqCst);
        
        if total == 0 {
            return 0.0;
        }
        
        failures as f64 / total as f64
    }
}

struct CircuitBreaker {
    state: RwLock<CircuitState>,
    stats: CircuitBreakerStats,
    failure_threshold: f64,
    reset_timeout: Duration,
    half_open_allowed_calls: usize,
}

impl CircuitBreaker {
    fn new(
        failure_threshold: f64,
        reset_timeout: Duration,
        half_open_allowed_calls: usize,
    ) -> Arc<Self> {
        Arc::new(Self {
            state: RwLock::new(CircuitState::Closed),
            stats: CircuitBreakerStats::new(),
            failure_threshold,
            reset_timeout,
            half_open_allowed_calls,
        })
    }

/*    
<think>
我将继续从CircuitBreaker的实现开始，接着完成剩余的容错模式部分。
</think>
*/


    async fn current_state(&self) -> CircuitState {
        *self.state.read().await
    }
    
    async fn trip(&self) {
        let mut state = self.state.write().await;
        if *state == CircuitState::Closed {
            *state = CircuitState::Open;
            println!("熔断器状态变更: 关闭 -> 打开");
        }
    }
    
    async fn reset(&self) {
        let mut state = self.state.write().await;
        *state = CircuitState::Closed;
        self.stats.reset();
        println!("熔断器状态变更: 打开/半开 -> 关闭");
    }
    
    async fn half_open(&self) {
        let mut state = self.state.write().await;
        if *state == CircuitState::Open {
            *state = CircuitState::HalfOpen;
            self.stats.reset();
            println!("熔断器状态变更: 打开 -> 半开");
        }
    }
    
    async fn check_state(&self) {
        match self.current_state().await {
            CircuitState::Closed => {
                // 检查失败率是否超过阈值
                if self.stats.failure_rate() >= self.failure_threshold {
                    self.trip().await;
                }
            },
            CircuitState::Open => {
                // 检查是否经过了足够的时间来尝试半开
                let last_failure = self.stats.last_failure_time.load(Ordering::SeqCst);
                let now = Instant::now().elapsed().as_secs();
                
                if now - last_failure >= self.reset_timeout.as_secs() {
                    self.half_open().await;
                }
            },
            CircuitState::HalfOpen => {
                // 在半开状态，检查是否有足够的成功请求来关闭断路器
                let success_count = self.stats.success_count.load(Ordering::SeqCst);
                
                if success_count >= self.half_open_allowed_calls {
                    self.reset().await;
                } else if self.stats.failure_count.load(Ordering::SeqCst) > 0 {
                    // 如果有任何失败，立即回到打开状态
                    self.trip().await;
                }
            },
        }
    }
    
    async fn execute<F, Fut, T, E>(&self, f: F) -> Result<T, E>
    where
        F: FnOnce() -> Fut,
        Fut: Future<Output = Result<T, E>>,
    {
        // 检查状态
        match self.current_state().await {
            CircuitState::Open => {
                println!("熔断器打开，快速失败");
                return Err(std::any::type_name::<E>().parse().unwrap());
            },
            CircuitState::HalfOpen => {
                let total = self.stats.total_count.load(Ordering::SeqCst);
                if total >= self.half_open_allowed_calls {
                    println!("半开状态请求数量已达上限，快速失败");
                    return Err(std::any::type_name::<E>().parse().unwrap());
                }
            },
            CircuitState::Closed => {}
        }
        
        // 执行函数
        match f().await {
            Ok(result) => {
                self.stats.record_success();
                self.check_state().await;
                Ok(result)
            },
            Err(err) => {
                self.stats.record_failure();
                self.check_state().await;
                Err(err)
            }
        }
    }
}

#[async_trait]
trait RemoteService {
    async fn call(&self) -> Result<String, String>;
}

struct UnreliableService {
    failure_rate: f64,
}

#[async_trait]
impl RemoteService for UnreliableService {
    async fn call(&self) -> Result<String, String> {
        // 模拟随机失败
        if rand::random::<f64>() < self.failure_rate {
            sleep(Duration::from_millis(100)).await; // 模拟延迟
            Err("服务调用失败".to_string())
        } else {
            sleep(Duration::from_millis(50)).await; // 模拟延迟
            Ok("服务调用成功".to_string())
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建一个不可靠的服务
    let service = UnreliableService { failure_rate: 0.7 }; // 70%失败率
    
    // 创建一个熔断器
    let breaker = CircuitBreaker::new(
        0.5, // 50%失败率阈值
        Duration::from_secs(5), // 5秒重置时间
        3, // 半开状态允许的调用次数
    );
    
    // 执行一系列调用
    for i in 0..30 {
        println!("尝试 #{}", i);
        
        let result = breaker.execute(|| async {
            service.call().await
        }).await;
        
        match result {
            Ok(msg) => println!("成功: {}", msg),
            Err(err) => println!("失败: {}", err),
        }
        
        sleep(Duration::from_millis(300)).await;
    }
}
```

### 4.2 舱壁模式

舱壁模式通过隔离资源，防止故障在系统中扩散。例如，为不同的服务或客户端分配独立的资源池。

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, Semaphore};
use tokio::time::sleep;
use futures::future::join_all;

#[derive(Debug, Clone)]
struct BulkheadConfig {
    max_concurrent_calls: usize,
    max_queue_size: usize,
    call_timeout: Duration,
}

// 舱壁实现 - 限制并发调用数量和排队请求数
struct Bulkhead {
    name: String,
    semaphore: Arc<Semaphore>,
    queue_channel: mpsc::Sender<()>,
    call_timeout: Duration,
}

impl Bulkhead {
    fn new(name: &str, config: BulkheadConfig) -> Self {
        let (queue_tx, mut queue_rx) = mpsc::channel(config.max_queue_size);
        
        // 准备信号量用于控制并发
        let semaphore = Arc::new(Semaphore::new(config.max_concurrent_calls));
        
        Self {
            name: name.to_string(),
            semaphore,
            queue_channel: queue_tx,
            call_timeout: config.call_timeout,
        }
    }
    
    async fn execute<F, Fut, T>(&self, f: F) -> Result<T, String>
    where
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = T>,
    {
        // 尝试进入队列
        match self.queue_channel.try_send(()) {
            Ok(_) => {},
            Err(_) => return Err(format!("队列已满 - {}", self.name)),
        };
        
        // 获取信号量许可
        let permit = match tokio::time::timeout(
            self.call_timeout,
            self.semaphore.acquire(),
        ).await {
            Ok(permit_result) => match permit_result {
                Ok(permit) => permit,
                Err(_) => return Err(format!("获取许可失败 - {}", self.name)),
            },
            Err(_) => return Err(format!("等待许可超时 - {}", self.name)),
        };
        
        // 执行函数，确保释放资源
        let start = Instant::now();
        let result = f().await;
        let elapsed = start.elapsed();
        
        // 释放许可和队列位置
        drop(permit);
        
        Ok(result)
    }
}

// 使用舱壁隔离不同服务的资源
struct BulkheadRegistry {
    bulkheads: Mutex<HashMap<String, Arc<Bulkhead>>>,
    default_config: BulkheadConfig,
}

impl BulkheadRegistry {
    fn new(default_config: BulkheadConfig) -> Self {
        Self {
            bulkheads: Mutex::new(HashMap::new()),
            default_config,
        }
    }
    
    fn get_or_create(&self, name: &str) -> Arc<Bulkhead> {
        let mut bulkheads = self.bulkheads.lock().unwrap();
        
        if let Some(bulkhead) = bulkheads.get(name) {
            return bulkhead.clone();
        }
        
        let bulkhead = Arc::new(Bulkhead::new(name, self.default_config.clone()));
        bulkheads.insert(name.to_string(), bulkhead.clone());
        bulkhead
    }
    
    fn get_or_create_with_config(&self, name: &str, config: BulkheadConfig) -> Arc<Bulkhead> {
        let mut bulkheads = self.bulkheads.lock().unwrap();
        
        if let Some(bulkhead) = bulkheads.get(name) {
            return bulkhead.clone();
        }
        
        let bulkhead = Arc::new(Bulkhead::new(name, config));
        bulkheads.insert(name.to_string(), bulkhead.clone());
        bulkhead
    }
}

async fn slow_service_call(service_name: &str, duration_ms: u64) -> String {
    sleep(Duration::from_millis(duration_ms)).await;
    format!("来自 {} 的响应", service_name)
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建舱壁注册表
    let registry = Arc::new(BulkheadRegistry::new(BulkheadConfig {
        max_concurrent_calls: 10,
        max_queue_size: 20,
        call_timeout: Duration::from_secs(2),
    }));
    
    // 为数据库服务创建限制更严格的舱壁
    let db_config = BulkheadConfig {
        max_concurrent_calls: 3,
        max_queue_size: 5,
        call_timeout: Duration::from_secs(1),
    };
    
    registry.get_or_create_with_config("database", db_config);
    
    // 模拟对不同服务的大量并发调用
    let mut api_handles = Vec::new();
    let mut db_handles = Vec::new();
    
    // API调用 - 低延迟
    for i in 0..15 {
        let registry = registry.clone();
        api_handles.push(tokio::spawn(async move {
            let bulkhead = registry.get_or_create("api");
            let result = bulkhead.execute(|| async move {
                slow_service_call("api", 200).await
            }).await;
            
            println!("API调用 #{}: {:?}", i, result);
            result
        }));
    }
    
    // 数据库调用 - 高延迟
    for i in 0..15 {
        let registry = registry.clone();
        db_handles.push(tokio::spawn(async move {
            let bulkhead = registry.get_or_create("database");
            let result = bulkhead.execute(|| async move {
                slow_service_call("database", 500).await
            }).await;
            
            println!("数据库调用 #{}: {:?}", i, result);
            result
        }));
    }
    
    // 等待所有调用完成
    let api_results = join_all(api_handles).await;
    let db_results = join_all(db_handles).await;
    
    // 计算成功率
    let api_success = api_results.iter()
        .filter(|r| match r {
            Ok(Ok(_)) => true,
            _ => false,
        })
        .count();
    
    let db_success = db_results.iter()
        .filter(|r| match r {
            Ok(Ok(_)) => true,
            _ => false,
        })
        .count();
    
    println!("API调用成功率: {}/{}", api_success, api_handles.len());
    println!("数据库调用成功率: {}/{}", db_success, db_handles.len());
}
```

### 4.3 超时与重试

超时与重试模式通过限制等待时间和在失败时重新尝试操作，提高系统的弹性。

```rust
use std::future::Future;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::Duration;
use tokio::time::timeout;
use rand::prelude::*;
use backoff::{backoff::Backoff, ExponentialBackoff};

// 重试策略特征
trait RetryStrategy {
    fn should_retry(&mut self, attempts: usize, error: &str) -> bool;
    fn next_backoff(&mut self, attempts: usize) -> Option<Duration>;
}

// 指数退避策略
struct ExponentialRetryStrategy {
    max_attempts: usize,
    backoff: ExponentialBackoff,
}

impl ExponentialRetryStrategy {
    fn new(max_attempts: usize, initial_interval: Duration, max_interval: Duration) -> Self {
        let mut backoff = ExponentialBackoff::default();
        backoff.initial_interval = initial_interval;
        backoff.max_interval = max_interval;
        backoff.multiplier = 2.0;
        backoff.randomization_factor = 0.2;
        
        Self {
            max_attempts,
            backoff,
        }
    }
}

impl RetryStrategy for ExponentialRetryStrategy {
    fn should_retry(&mut self, attempts: usize, _error: &str) -> bool {
        attempts < self.max_attempts
    }
    
    fn next_backoff(&mut self, _attempts: usize) -> Option<Duration> {
        self.backoff.next_backoff()
    }
}

// 固定间隔重试策略
struct FixedRetryStrategy {
    max_attempts: usize,
    interval: Duration,
}

impl FixedRetryStrategy {
    fn new(max_attempts: usize, interval: Duration) -> Self {
        Self {
            max_attempts,
            interval,
        }
    }
}

impl RetryStrategy for FixedRetryStrategy {
    fn should_retry(&mut self, attempts: usize, _error: &str) -> bool {
        attempts < self.max_attempts
    }
    
    fn next_backoff(&mut self, _attempts: usize) -> Option<Duration> {
        Some(self.interval)
    }
}

// 重试执行器
struct RetryExecutor<S: RetryStrategy> {
    strategy: S,
    timeout_duration: Duration,
}

impl<S: RetryStrategy> RetryExecutor<S> {
    fn new(strategy: S, timeout_duration: Duration) -> Self {
        Self {
            strategy,
            timeout_duration,
        }
    }
    
    async fn execute<F, Fut, T, E>(&mut self, operation: F) -> Result<T, String>
    where
        F: Fn() -> Fut,
        Fut: Future<Output = Result<T, E>>,
        E: std::fmt::Display,
    {
        let mut attempts = 0;
        
        loop {
            attempts += 1;
            
            // 使用超时包装操作
            let result = match timeout(self.timeout_duration, operation()).await {
                Ok(inner_result) => match inner_result {
                    Ok(value) => return Ok(value),
                    Err(err) => Err(err.to_string()),
                },
                Err(_) => Err(format!("操作超时 (尝试 #{})", attempts)),
            };
            
            // 处理错误和重试逻辑
            if let Err(err) = result {
                if !self.strategy.should_retry(attempts, &err) {
                    return Err(format!("最终失败 ({}次尝试): {}", attempts, err));
                }
                
                if let Some(delay) = self.strategy.next_backoff(attempts) {
                    println!("尝试 #{} 失败: {}. 等待 {:?} 后重试...", attempts, err, delay);
                    tokio::time::sleep(delay).await;
                } else {
                    return Err(format!("重试策略决定停止重试: {}", err));
                }
            }
        }
    }
}

// 模拟不可靠的远程服务
struct UnreliableService {
    failure_rate: f64,
    delay_ms: u64,
    call_count: AtomicUsize,
}

impl UnreliableService {
    fn new(failure_rate: f64, delay_ms: u64) -> Self {
        Self {
            failure_rate: failure_rate,
            delay_ms,
            call_count: AtomicUsize::new(0),
        }
    }
    
    async fn call(&self) -> Result<String, String> {
        let count = self.call_count.fetch_add(1, Ordering::SeqCst);
        
        // 模拟延迟
        tokio::time::sleep(Duration::from_millis(self.delay_ms)).await;
        
        // 模拟随机失败
        let mut rng = rand::thread_rng();
        if rng.gen::<f64>() < self.failure_rate {
            Err(format!("服务调用 #{} 失败", count))
        } else {
            Ok(format!("来自调用 #{} 的成功响应", count))
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建服务实例
    let service = Arc::new(UnreliableService::new(0.7, 200)); // 70%失败率，200ms延迟
    
    println!("使用固定间隔重试策略:");
    {
        let service_clone = service.clone();
        let strategy = FixedRetryStrategy::new(3, Duration::from_millis(500));
        let mut executor = RetryExecutor::new(strategy, Duration::from_millis(300));
        
        let result = executor.execute(|| async move {
            service_clone.call().await
        }).await;
        
        println!("结果: {:?}", result);
    }
    
    println!("\n使用指数退避重试策略:");
    {
        let service_clone = service.clone();
        let strategy = ExponentialRetryStrategy::new(
            5,
            Duration::from_millis(100),
            Duration::from_millis(1000)
        );
        let mut executor = RetryExecutor::new(strategy, Duration::from_millis(300));
        
        let result = executor.execute(|| async move {
            service_clone.call().await
        }).await;
        
        println!("结果: {:?}", result);
    }
    
    println!("\n使用超短超时触发超时错误:");
    {
        let service_clone = service.clone();
        let strategy = ExponentialRetryStrategy::new(
            3,
            Duration::from_millis(100),
            Duration::from_millis(1000)
        );
        // 超时设置为50ms，小于服务延迟
        let mut executor = RetryExecutor::new(strategy, Duration::from_millis(50));
        
        let result = executor.execute(|| async move {
            service_clone.call().await
        }).await;
        
        println!("结果: {:?}", result);
    }
}
```

### 4.4 背压

背压机制在下游组件处理不过来时，能够向上游组件传递信号，减缓数据发送速率。

```rust
use std::sync::Arc;
use tokio::sync::{mpsc, Semaphore};
use tokio::time::{sleep, Duration};
use rand::{thread_rng, Rng};
use futures::stream::{self, StreamExt};

// 流处理类型，具有背压能力
struct BackpressuredProcessor<T> {
    name: String,
    capacity: usize,
    semaphore: Arc<Semaphore>,
    input_tx: Option<mpsc::Sender<T>>,
    output_rx: Option<mpsc::Receiver<T>>,
    processing_time_ms: u64,
}

impl<T: Send + 'static> BackpressuredProcessor<T> {
    fn new(name: &str, capacity: usize, processing_time_ms: u64) -> Self {
        let semaphore = Arc::new(Semaphore::new(capacity));
        
        Self {
            name: name.to_string(),
            capacity,
            semaphore,
            input_tx: None,
            output_rx: None,
            processing_time_ms,
        }
    }
    
    // 初始化处理器的通道
    async fn init(&mut self) {
        let (input_tx, mut input_rx) = mpsc::channel(self.capacity);
        let (output_tx, output_rx) = mpsc::channel(self.capacity);
        
        self.input_tx = Some(input_tx);
        self.output_rx = Some(output_rx);
        
        let semaphore = self.semaphore.clone();
        let name = self.name.clone();
        let processing_time = self.processing_time_ms;
        
        // 启动处理器任务
        tokio::spawn(async move {
            while let Some(item) = input_rx.recv().await {
                // 获取信号量许可，实现背压
                let permit = semaphore.acquire().await.unwrap();
                
                // 处理项目
                println!("[{}] 正在处理项目...", name);
                sleep(Duration::from_millis(processing_time)).await;
                
                // 发送到输出
                let _ = output_tx.send(item).await;
                
                // 释放许可
                drop(permit);
            }
        });
    }
    
    // 获取输入发送器
    fn get_sender(&self) -> Option<mpsc::Sender<T>> {
        self.input_tx.clone()
    }
    
    // 获取输出接收器
    fn take_receiver(&mut self) -> Option<mpsc::Receiver<T>> {
        self.output_rx.take()
    }
}

// 背压管道，连接多个处理器
struct ProcessingPipeline<T> {
    stages: Vec<BackpressuredProcessor<T>>,
    initialized: bool,
}

impl<T: Send + Clone + std::fmt::Debug + 'static> ProcessingPipeline<T> {
    fn new() -> Self {
        Self {
            stages: Vec::new(),
            initialized: false,
        }
    }
    
    // 添加处理阶段
    fn add_stage(&mut self, processor: BackpressuredProcessor<T>) {
        if self.initialized {
            panic!("管道已初始化，不能添加更多阶段");
        }
        self.stages.push(processor);
    }
    
    // 初始化管道，连接所有阶段
    async fn init(&mut self) {
        if self.initialized {
            return;
        }
        
        // 初始化每个阶段
        for stage in &mut self.stages {
            stage.init().await;
        }
        
        self.initialized = true;
    }
    
    // 获取输入发送器
    fn get_input_sender(&self) -> Option<mpsc::Sender<T>> {
        if !self.initialized || self.stages.is_empty() {
            return None;
        }
        
        self.stages[0].get_sender()
    }
    
    // 获取最终输出接收器
    fn take_output_receiver(&mut self) -> Option<mpsc::Receiver<T>> {
        if !self.initialized || self.stages.is_empty() {
            return None;
        }
        
        let last_index = self.stages.len() - 1;
        self.stages[last_index].take_receiver()
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建一个三阶段的处理管道
    let mut pipeline = ProcessingPipeline::<u32>::new();
    
    // 添加处理阶段，模拟不同速率的处理器
    pipeline.add_stage(BackpressuredProcessor::new("阶段1", 10, 50));  // 快速
    pipeline.add_stage(BackpressuredProcessor::new("阶段2", 5, 200));  // 较慢
    pipeline.add_stage(BackpressuredProcessor::new("阶段3", 3, 100));  // 中等
    
    // 初始化管道
    pipeline.init().await;
    
    // 获取输入发送器和输出接收器
    let input_sender = pipeline.get_input_sender().unwrap();
    let mut output_receiver = pipeline.take_output_receiver().unwrap();
    
    // 启动消费者
    tokio::spawn(async move {
        while let Some(item) = output_receiver.recv().await {
            println!("最终输出: {:?}", item);
            sleep(Duration::from_millis(300)).await;  // 模拟消费者处理时间
        }
    });
    
    // 生成数据，观察背压效果
    let mut rng = thread_rng();
    
    for i in 0..50 {
        // 尝试发送数据
        match input_sender.try_send(i) {
            Ok(_) => println!("发送项目 {} 成功", i),
            Err(e) => println!("发送项目 {} 失败: {:?}", i, e),
        }
        
        // 随机间隔生成数据
        let delay = rng.gen_range(10..100);
        sleep(Duration::from_millis(delay)).await;
    }
    
    // 等待处理完成
    sleep(Duration::from_secs(5)).await;
}
```

## 5. 事务模式

### 5.1 两阶段提交

两阶段提交协议是分布式事务的基本实现，分为准备阶段和提交阶段。

```rust
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, RwLock};
use tokio::time::timeout;
use std::time::Duration;
use uuid::Uuid;
use serde::{Serialize, Deserialize};

// 事务状态
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum TransactionState {
    Pending,
    Prepared,
    Committed,
    Aborted,
}

// 事务管理器消息
#[derive(Debug, Clone, Serialize, Deserialize)]
enum TwoPhaseMessage {
    // 第一阶段：准备
    Prepare { tx_id: String },
    PrepareResponse { tx_id: String, prepared: bool },
    
    // 第二阶段：提交或中止
    Commit { tx_id: String },
    Abort { tx_id: String },
    
    // 完成确认
    Ack { tx_id: String, success: bool },
}

// 参与者
struct Participant {
    id: String,
    data: Arc<RwLock<HashMap<String, String>>>,
    pending_changes: Arc<RwLock<HashMap<String, HashMap<String, String>>>>,
    tx_states: Arc<RwLock<HashMap<String, TransactionState>>>,
}

impl Participant {
    fn new(id: &str) -> Self {
        Self {
            id: id.to_string(),
            data: Arc::new(RwLock::new(HashMap::new())),
            pending_changes: Arc::new(RwLock::new(HashMap::new())),
            tx_states: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    // 启动参与者并返回通信通道
    async fn start(&self) -> mpsc::Sender<TwoPhaseMessage> {
        let (tx, mut rx) = mpsc::channel(100);
        
        let id = self.id.clone();
        let data = self.data.clone();
        let pending_changes = self.pending_changes.clone();
        let tx_states = self.tx_states.clone();
        
        tokio::spawn(async move {
            while let Some(message) = rx.recv().await {
                match message {
                    TwoPhaseMessage::Prepare { tx_id } => {
                        let can_prepare = {
...(about 25 lines omitted)...
                }
            }
        });
        
        tx
    }
    
    // 在事务中设置值（阶段1）
    async fn set_value(&self, tx_id: &str, key: &str, value: &str) -> Result<(), String> {
        let mut pending = self.pending_changes.write().await;
        
        let tx_changes = pending.entry(tx_id.to_string())
            .or_insert_with(HashMap::new);
        
        tx_changes.insert(key.to_string(), value.to_string());
        
        Ok(())
    }
    
    // 处理准备请求
    async fn handle_prepare(&self, tx_id: &str) -> bool {
        // 检查是否有资源冲突
        // 在实际应用中，这可能涉及锁定资源和更复杂的验证
        
        // 更新事务状态
        let mut tx_states = self.tx_states.write().await;
        tx_states.insert(tx_id.to_string(), TransactionState::Prepared);
        
        true
    }
    
    // 处理提交请求
    async fn handle_commit(&self, tx_id: &str) -> bool {
        // 检查事务状态
        let state = {
            let tx_states = self.tx_states.read().await;
            tx_states.get(tx_id).cloned().unwrap_or(TransactionState::Pending)
        };
        
        if state != TransactionState::Prepared {
            return false;
        }
        
        // 应用挂起的更改
        let changes = {
            let mut pending = self.pending_changes.write().await;
            pending.remove(tx_id).unwrap_or_default()
        };
        
        let mut data = self.data.write().await;
        for (key, value) in changes {
            data.insert(key, value);
        }
        
        // 更新事务状态
        let mut tx_states = self.tx_states.write().await;
        tx_states.insert(tx_id.to_string(), TransactionState::Committed);
        
        true
    }
    
    // 处理中止请求
    async fn handle_abort(&self, tx_id: &str) -> bool {
        // 清除挂起的更改
        let mut pending = self.pending_changes.write().await;
        pending.remove(tx_id);
        
        // 更新事务状态
        let mut tx_states = self.tx_states.write().await;
        tx_states.insert(tx_id.to_string(), TransactionState::Aborted);
        
        true
    }
}
```

```text
<think>
I'll continue the implementation of the Two-Phase Commit pattern,
and then move on to the other remaining patterns in the list.

The next part of the code should include:
1. Finishing the Transaction Coordinator for the Two-Phase Commit pattern
2. An example showing the Two-Phase Commit in action
3. Then move on to implement the Saga pattern

After that, I'll continue with the remaining patterns from the list such as service discovery patterns, load balancing patterns, etc.
</think>
```

```rust
// 协调者
struct TransactionCoordinator {
    participants: HashMap<String, mpsc::Sender<TwoPhaseMessage>>,
    active_transactions: Arc<Mutex<HashMap<String, TransactionState>>>,
    timeout_duration: Duration,
}

impl TransactionCoordinator {
    fn new(timeout_duration: Duration) -> Self {
        Self {
            participants: HashMap::new(),
            active_transactions: Arc::new(Mutex::new(HashMap::new())),
            timeout_duration,
        }
    }
    
    // 添加参与者
    fn add_participant(&mut self, id: &str, channel: mpsc::Sender<TwoPhaseMessage>) {
        self.participants.insert(id.to_string(), channel);
    }
    
    // 开始新事务
    async fn begin_transaction(&self) -> String {
        let tx_id = Uuid::new_v4().to_string();
        
        let mut transactions = self.active_transactions.lock().await;
        transactions.insert(tx_id.clone(), TransactionState::Pending);
        
        tx_id
    }
    
    // 提交事务 (两阶段)
    async fn commit(&self, tx_id: &str) -> Result<(), String> {
        // 第一阶段: 准备
        println!("开始第一阶段: 准备");
        let prepare_results = self.prepare_phase(tx_id).await?;
        
        // 检查所有参与者是否准备好
        let all_prepared = prepare_results.iter().all(|(_, prepared)| *prepared);
        
        if all_prepared {
            // 第二阶段: 提交
            println!("所有参与者已准备好，开始第二阶段: 提交");
            self.commit_phase(tx_id).await?;
            
            // 更新事务状态
            let mut transactions = self.active_transactions.lock().await;
            transactions.insert(tx_id.to_string(), TransactionState::Committed);
            
            Ok(())
        } else {
            // 有参与者未准备好，中止事务
            println!("部分参与者未准备好，中止事务");
            self.abort_phase(tx_id).await?;
            
            // 更新事务状态
            let mut transactions = self.active_transactions.lock().await;
            transactions.insert(tx_id.to_string(), TransactionState::Aborted);
            
            Err("有参与者未准备好，事务已中止".to_string())
        }
    }
    
    // 准备阶段 (第一阶段)
    async fn prepare_phase(&self, tx_id: &str) -> Result<HashMap<String, bool>, String> {
        let mut prepare_responses = HashMap::new();
        let (response_tx, mut response_rx) = mpsc::channel(self.participants.len());
        
        // 向所有参与者发送准备消息
        for (participant_id, participant_tx) in &self.participants {
            let message = TwoPhaseMessage::Prepare {
                tx_id: tx_id.to_string(),
            };
            
            let participant_id = participant_id.clone();
            let response_tx = response_tx.clone();
            let participant_tx = participant_tx.clone();
            
            tokio::spawn(async move {
                if let Err(e) = participant_tx.send(message).await {
                    println!("向参与者 {} 发送准备消息失败: {}", participant_id, e);
                    let _ = response_tx.send((participant_id, false)).await;
                    return;
                }
                
                // 等待响应或超时
                // 在实际系统中，我们会有专门的响应通道
                let _ = response_tx.send((participant_id, true)).await;
            });
        }
        
        // 等待所有响应
        for _ in 0..self.participants.len() {
            match timeout(self.timeout_duration, response_rx.recv()).await {
                Ok(Some((participant_id, prepared))) => {
                    prepare_responses.insert(participant_id, prepared);
                },
                _ => {
                    // 超时或通道关闭
                    return Err("准备阶段超时或通道关闭".to_string());
                }
            }
        }
        
        Ok(prepare_responses)
    }
    
    // 提交阶段 (第二阶段)
    async fn commit_phase(&self, tx_id: &str) -> Result<(), String> {
        let mut commit_futures = Vec::new();
        
        // 向所有参与者发送提交消息
        for (participant_id, participant_tx) in &self.participants {
            let message = TwoPhaseMessage::Commit {
                tx_id: tx_id.to_string(),
            };
            
            let participant_id = participant_id.clone();
            let participant_tx = participant_tx.clone();
            
            let commit_future = async move {
                match participant_tx.send(message).await {
                    Ok(_) => Ok(participant_id),
                    Err(e) => Err(format!("向参与者 {} 发送提交消息失败: {}", participant_id, e)),
                }
            };
            
            commit_futures.push(commit_future);
        }
        
        // 等待所有提交完成
        for result in futures::future::join_all(commit_futures).await {
            if let Err(e) = result {
                println!("警告: {}", e);
                // 在实际系统中，我们可能需要重试或进行补偿处理
            }
        }
        
        Ok(())
    }
    
    // 中止阶段
    async fn abort_phase(&self, tx_id: &str) -> Result<(), String> {
        let mut abort_futures = Vec::new();
        
        // 向所有参与者发送中止消息
        for (participant_id, participant_tx) in &self.participants {
            let message = TwoPhaseMessage::Abort {
                tx_id: tx_id.to_string(),
            };
            
            let participant_id = participant_id.clone();
            let participant_tx = participant_tx.clone();
            
            let abort_future = async move {
                match participant_tx.send(message).await {
                    Ok(_) => Ok(participant_id),
                    Err(e) => Err(format!("向参与者 {} 发送中止消息失败: {}", participant_id, e)),
                }
            };
            
            abort_futures.push(abort_future);
        }
        
        // 等待所有中止完成
        for result in futures::future::join_all(abort_futures).await {
            if let Err(e) = result {
                println!("警告: {}", e);
            }
        }
        
        Ok(())
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建参与者
    let participant1 = Arc::new(Participant::new("node1"));
    let participant2 = Arc::new(Participant::new("node2"));
    let participant3 = Arc::new(Participant::new("node3"));
    
    // 启动参与者
    let channel1 = participant1.start().await;
    let channel2 = participant2.start().await;
    let channel3 = participant3.start().await;
    
    // 创建协调者
    let mut coordinator = TransactionCoordinator::new(Duration::from_secs(5));
    coordinator.add_participant("node1", channel1);
    coordinator.add_participant("node2", channel2);
    coordinator.add_participant("node3", channel3);
    
    // 开始事务
    let tx_id = coordinator.begin_transaction().await;
    println!("开始事务: {}", tx_id);
    
    // 在各参与者上设置值
    participant1.set_value(&tx_id, "key1", "value1").await.unwrap();
    participant2.set_value(&tx_id, "key2", "value2").await.unwrap();
    participant3.set_value(&tx_id, "key3", "value3").await.unwrap();
    
    // 提交事务
    match coordinator.commit(&tx_id).await {
        Ok(_) => println!("事务 {} 成功提交", tx_id),
        Err(e) => println!("事务 {} 提交失败: {}", tx_id, e),
    }
    
    // 检查每个参与者的数据
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    let data1 = participant1.data.read().await;
    let data2 = participant2.data.read().await;
    let data3 = participant3.data.read().await;
    
    println!("参与者1数据: {:?}", data1);
    println!("参与者2数据: {:?}", data2);
    println!("参与者3数据: {:?}", data3);
    
    // 模拟失败场景
    let tx_id_fail = coordinator.begin_transaction().await;
    println!("\n开始可能失败的事务: {}", tx_id_fail);
    
    participant1.set_value(&tx_id_fail, "key4", "value4").await.unwrap();
    participant2.set_value(&tx_id_fail, "key5", "value5").await.unwrap();
    
    // 第三个参与者无法准备 (模拟)
    {
        let mut tx_states = participant3.tx_states.write().await;
        tx_states.insert(tx_id_fail.clone(), TransactionState::Aborted);
    }
    
    // 尝试提交事务
    match coordinator.commit(&tx_id_fail).await {
        Ok(_) => println!("事务 {} 成功提交", tx_id_fail),
        Err(e) => println!("事务 {} 提交失败: {}", tx_id_fail, e),
    }
    
    // 检查每个参与者的数据 (应该没有变化)
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    let data1 = participant1.data.read().await;
    let data2 = participant2.data.read().await;
    let data3 = participant3.data.read().await;
    
    println!("参与者1数据: {:?}", data1);
    println!("参与者2数据: {:?}", data2);
    println!("参与者3数据: {:?}", data3);
}
```

### 5.2 Saga模式

Saga模式是一种长事务的解决方案，它将一个分布式事务分解为多个本地事务，并定义对应的补偿事务。

```rust
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

// Saga步骤状态
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum StepStatus {
    Pending,
    Succeeded,
    Failed,
    Compensated,
}

// Saga状态
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum SagaStatus {
    Running,
    Completed,
    Compensating,
    Failed,
}

// Saga步骤定义
#[derive(Clone)]
struct SagaStep<T> {
    name: String,
    execute: Arc<dyn Fn(T) -> T + Send + Sync>,
    compensate: Arc<dyn Fn(T) -> T + Send + Sync>,
}

// Saga执行上下文
struct SagaContext<T> {
    saga_id: String,
    data: T,
    step_statuses: HashMap<String, StepStatus>,
    status: SagaStatus,
}

// Saga协调器
struct SagaCoordinator<T: Clone + Send + 'static> {
    steps: Vec<SagaStep<T>>,
    active_sagas: Arc<Mutex<HashMap<String, SagaContext<T>>>>,
}

impl<T: Clone + Send + 'static> SagaCoordinator<T> {
    fn new() -> Self {
        Self {
            steps: Vec::new(),
            active_sagas: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    // 添加Saga步骤
    fn add_step<E, C>(&mut self, name: &str, execute: E, compensate: C)
    where
        E: Fn(T) -> T + Send + Sync + 'static,
        C: Fn(T) -> T + Send + Sync + 'static,
    {
        let step = SagaStep {
            name: name.to_string(),
            execute: Arc::new(execute),
            compensate: Arc::new(compensate),
        };
        
        self.steps.push(step);
    }
    
    // 开始新的Saga
    async fn start(&self, initial_data: T) -> String {
        let saga_id = Uuid::new_v4().to_string();
        
        let context = SagaContext {
            saga_id: saga_id.clone(),
            data: initial_data,
            step_statuses: HashMap::new(),
            status: SagaStatus::Running,
        };
        
        let mut active_sagas = self.active_sagas.lock().await;
        active_sagas.insert(saga_id.clone(), context);
        
        saga_id
    }
    
    // 执行Saga
    async fn execute(&self, saga_id: &str) -> Result<T, String> {
        // 获取Saga上下文
        let mut context = {
            let mut active_sagas = self.active_sagas.lock().await;
            
            active_sagas.get_mut(saga_id)
                .ok_or_else(|| format!("找不到Saga: {}", saga_id))?
                .clone()
        };
        
        // 执行所有步骤
        for (i, step) in self.steps.iter().enumerate() {
            println!("执行步骤 {}: {}", i, step.name);
            
            // 更新步骤状态
            context.step_statuses.insert(step.name.clone(), StepStatus::Pending);
            
            // 执行步骤
            let result = {
                let execute = step.execute.clone();
                let data = context.data.clone();
                
                // 在实际环境中，这里可能是对远程服务的异步调用
                execute(data)
            };
            
            // 更新上下文
            context.data = result.clone();
            
            // 如果步骤失败 (取决于领域逻辑如何判断失败)
            let step_failed = false; // 这里应该有判断逻辑
            
            if step_failed {
                context.step_statuses.insert(step.name.clone(), StepStatus::Failed);
                context.status = SagaStatus::Compensating;
                
                // 执行补偿
                let compensation_result = self.compensate(&mut context, i).await;
                
                // 更新活动Saga
                {
                    let mut active_sagas = self.active_sagas.lock().await;
                    if let Some(ctx) = active_sagas.get_mut(saga_id) {
                        *ctx = context.clone();
                    }
                }
                
                return Err(format!("Saga步骤 {} 失败，已执行补偿", step.name));
            }
            
            // 更新步骤状态为成功
            context.step_statuses.insert(step.name.clone(), StepStatus::Succeeded);
        }
        
        // 所有步骤成功完成
        context.status = SagaStatus::Completed;
        
        // 更新活动Saga
        {
            let mut active_sagas = self.active_sagas.lock().await;
            if let Some(ctx) = active_sagas.get_mut(saga_id) {
                *ctx = context.clone();
            }
        }
        
        Ok(context.data)
    }
    
    // 执行补偿
    async fn compensate(&self, context: &mut SagaContext<T>, failed_step_index: usize) -> Result<(), String> {
        println!("开始补偿...");
        
        // 反向执行补偿操作
        for i in (0..=failed_step_index).rev() {
            let step = &self.steps[i];
            let step_status = context.step_statuses.get(&step.name).copied().unwrap_or(StepStatus::Pending);
            
            // 只补偿成功的步骤
            if step_status == StepStatus::Succeeded {
                println!("补偿步骤 {}: {}", i, step.name);
                
                // 执行补偿操作
                let compensate = step.compensate.clone();
                context.data = compensate(context.data.clone());
                
                // 更新步骤状态
                context.step_statuses.insert(step.name.clone(), StepStatus::Compensated);
            }
        }
        
        // 更新Saga状态
        context.status = SagaStatus::Failed;
        
        Ok(())
    }
}

// 使用示例 - 模拟订单处理
#[derive(Debug, Clone, Serialize, Deserialize)]
struct OrderData {
    order_id: String,
    customer_id: String,
    amount: f64,
    payment_completed: bool,
    inventory_reserved: bool,
    shipping_scheduled: bool,
}

#[tokio::main]
async fn main() {
    // 创建Saga协调器
    let mut coordinator = SagaCoordinator::new();
    
    // 添加Saga步骤
    
    // 1. 支付处理
    coordinator.add_step(
        "payment_processing",
        |mut data: OrderData| {
            println!("处理支付 {:.2} 元", data.amount);
            data.payment_completed = true;
            data
        },
        |mut data: OrderData| {
            println!("退款 {:.2} 元", data.amount);
            data.payment_completed = false;
            data
        }
    );
    
    // 2. 库存预留
    coordinator.add_step(
        "inventory_reservation",
        |mut data: OrderData| {
            println!("为订单 {} 预留库存", data.order_id);
            data.inventory_reserved = true;
            data
        },
        |mut data: OrderData| {
            println!("释放订单 {} 的库存预留", data.order_id);
            data.inventory_reserved = false;
            data
        }
    );
    
    // 3. 安排发货
    coordinator.add_step(
        "shipping_scheduling",
        |mut data: OrderData| {
            println!("为订单 {} 安排发货", data.order_id);
            data.shipping_scheduled = true;
            data
        },
        |mut data: OrderData| {
            println!("取消订单 {} 的发货安排", data.order_id);
            data.shipping_scheduled = false;
            data
        }
    );
    
    // 创建初始订单数据
    let initial_data = OrderData {
        order_id: Uuid::new_v4().to_string(),
        customer_id: "customer123".to_string(),
        amount: 99.99,
        payment_completed: false,
        inventory_reserved: false,
        shipping_scheduled: false,
    };
    
    // 启动Saga
    let saga_id = coordinator.start(initial_data.clone()).await;
    println!("开始Saga: {}", saga_id);
    
    // 执行Saga
    match coordinator.execute(&saga_id).await {
        Ok(result) => {
            println!("Saga成功完成!");
            println!("最终状态: {:?}", result);
        },
        Err(e) => {
            println!("Saga失败: {}", e);
        }
    }
    
    // 模拟失败场景
    // 在实际应用中，我们会有一个机制来模拟或检测故障
    
    // 再次启动Saga，这次我们将模拟第二步失败
    let saga_id_fail = coordinator.start(initial_data).await;
    println!("\n开始可能失败的Saga: {}", saga_id_fail);
    
    // 在这里我们通常会注入一个失败点
    // 简单起见，我们将依赖执行函数中的逻辑来检测失败
    
    match coordinator.execute(&saga_id_fail).await {
        Ok(result) => {
            println!("Saga成功完成!");
            println!("最终状态: {:?}", result);
        },
        Err(e) => {
            println!("Saga失败: {}", e);
            
            // 检查Saga状态
            let active_sagas = coordinator.active_sagas.lock().await;
            if let Some(context) = active_sagas.get(&saga_id_fail) {
                println!("最终状态: {:?}", context.status);
                println!("步骤状态: {:?}", context.step_statuses);
            }
        }
    }
}
```

## 6. 缓存模式

### 6.1 缓存代理

缓存代理模式使用缓存来提高读取操作的性能，并减少对原始数据源的访问。

```rust
use std::collections::HashMap;
use std::hash::Hash;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use async_trait::async_trait;

// 缓存项结构
struct CacheItem<V> {
    value: V,
    expiry: Option<Instant>,
}

// 缓存接口
#[async_trait]
trait Cache<K, V> {
    async fn get(&self, key: &K) -> Option<V>;
    async fn put(&self, key: K, value: V, ttl: Option<Duration>);
    async fn remove(&self, key: &K);
    async fn clear(&self);
}

// 内存缓存实现
struct InMemoryCache<K, V> {
    data: RwLock<HashMap<K, CacheItem<V>>>,
}

impl<K, V> InMemoryCache<K, V>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new() -> Self {
        Self {
            data: RwLock::new(HashMap::new()),
        }
    }
    
    // 清理过期项目
    async fn cleanup(&self) {
        let mut data = self.data.write().await;
        let now = Instant::now();
        
        data.retain(|_, item| {
            match item.expiry {
                Some(expiry) => expiry > now,
                None => true,
            }
        });
    }
}

#[async_trait]
impl<K, V> Cache<K, V> for InMemoryCache<K, V>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    async fn get(&self, key: &K) -> Option<V> {
        let data = self.data.read().await;
        
        if let Some(item) = data.get(key) {
            // 检查是否过期
            if let Some(expiry) = item.expiry {
                if Instant::now() > expiry {
                    return None;
                }
            }
            
            Some(item.value.clone())
        } else {
            None
        }
    }
    
    async fn put(&self, key: K, value: V, ttl: Option<Duration>) {
        let expiry = ttl.map(|duration| Instant::now() + duration);
        
        let item = CacheItem {
            value,
            expiry,
        };
        
        let mut data = self.data.write().await;
        data.insert(key, item);
    }
    
    async fn remove(&self, key: &K) {
        let mut data = self.data.write().await;
        data.remove(key);
    }
    
    async fn clear(&self) {
        let mut data = self.data.write().await;
        data.clear();
    }
}

// 数据源接口
#[async_trait]
trait DataSource<K, V> {
    async fn load(&self, key: &K) -> Option<V>;
}

// 模拟数据库数据源
struct DatabaseSource {
    data: HashMap<String, String>,
    load_delay_ms: u64, // 模拟数据库延迟
}

impl DatabaseSource {
    fn new(load_delay_ms: u64) -> Self {
        let mut data = HashMap::new();
        data.insert("user:1".to_string(), "用户1数据".to_string());
        data.insert("user:2".to_string(), "用户2数据".to_string());
        data.insert("product:1".to_string(), "产品1数据".to_string());
        
        Self {
            data,
            load_delay_ms,
        }
    }
}

#[async_trait]
impl DataSource<String, String> for DatabaseSource {
    async fn load(&self, key: &String) -> Option<String> {
        // 模拟数据库访问延迟
        tokio::time::sleep(Duration::from_millis(self.load_delay_ms)).await;
        
        self.data.get(key).cloned()
    }
}

// 缓存代理
struct CacheProxy<K, V, D, C>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
    D: DataSource<K, V> + Send + Sync + 'static,
    C: Cache<K, V> + Send + Sync,
{
    data_source: Arc<D>,
    cache: Arc<C>,
    default_ttl: Option<Duration>,
}

impl<K, V, D, C> CacheProxy<K, V, D, C>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
    D: DataSource<K, V> + Send + Sync,
    C: Cache<K, V> + Send + Sync,
{
    fn new(data_source: D, cache: C, default_ttl: Option<Duration>) -> Self {
        Self {
            data_source: Arc::new(data_source),
            cache: Arc::new(cache),
            default_ttl,
        }
    }
    
    async fn get(&self, key: K) -> Option<V> {
        // 首先尝试从缓存获取
        if let Some(value) = self.cache.get(&key).await {
            println!("缓存命中: {}", key.to_string());
            return Some(value);
        }
        
        println!("缓存未命中: {}, 从数据源加载", key.to_string());
        
        // 如果缓存中没有，从数据源加载
        let value = self.data_source.load(&key).await;
        
        // 如果数据源中有值，则缓存它
        if let Some(ref v) = value {
            self.cache.put(key, v.clone(), self.default_ttl).await;
        }
        
        value
    }
    
    async fn invalidate(&self, key: &K) {
        self.cache.remove(key).await;
    }
    
    async fn put(&self, key: K, value: V) {
        // 更新缓存并可能同步到数据源
        self.cache.put(key, value, self.default_ttl).await;
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建数据源
    let database = DatabaseSource::new(500); // 500ms加载延迟
    
    // 创建缓存
    let cache = InMemoryCache::new();
    
    // 创建缓存代理
    let proxy = CacheProxy::new(
        database,
        cache,
        Some(Duration::from_secs(10)), // 10秒TTL
    );
    
    // 第一次访问 - 缓存未命中，从数据源加载
    let start = Instant::now();
    let value1 = proxy.get("user:1".to_string()).await;
    let duration1 = start.elapsed();
    println!("第一次访问: {:?}, 耗时: {:?}", value1, duration1);
    
    // 第二次访问同一键 - 缓存命中
    let start = Instant::now();
    let value2 = proxy.get("user:1".to_string()).await;
    let duration2 = start.elapsed();
    println!("第二次访问: {:?}, 耗时: {:?}", value2, duration2);
    
    // 访问另一个键
    let start = Instant::now();
    let value3 = proxy.get("product:1".to_string()).await;
    let duration3 = start.elapsed();
    println!("访问其他键: {:?}, 耗时: {:?}", value3, duration3);
    
    // 访问不存在的键
    let start = Instant::now();
    let value4 = proxy.get("not_exist".to_string()).await;
    let duration4 = start.elapsed();
    println!("访问不存在键: {:?}, 耗时: {:?}", value4, duration4);
    
    // 再次访问不存在的键 - 应该直接从缓存获取空结果
    let start = Instant::now();
    let value5 = proxy.get("not_exist".to_string()).await;
    let duration5 = start.elapsed();
    println!("再次访问不存在键: {:?}, 耗时: {:?}", value5, duration5);
    
    // 失效某个缓存项
    proxy.invalidate(&"user:1".to_string()).await;
    println!("使 user:1 缓存失效");
    
    // 再次访问，应该再次从数据源加载
    let start = Instant::now();
    let value6 = proxy.get("user:1".to_string()).await;
    let duration6 = start.elapsed();
    println!("失效后访问: {:?}, 耗时: {:?}", value6, duration6);
}
```

### 6.2 写透与后写缓存

写透和后写缓存是两种不同的缓存写入策略，前者同步写入缓存和数据源，后者先写入缓存，之后异步写入数据源。

```rust
use std::collections::HashMap;
use std::hash::Hash;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, RwLock};
use tokio::time::sleep;
use async_trait::async_trait;

// 缓存接口
#[async_trait]
trait Cache<K, V> {
    async fn get(&self, key: &K) -> Option<V>;
    async fn put(&self, key: K, value: V);
    async fn remove(&self, key: &K);
}

// 数据源接口
#[async_trait]
trait DataSource<K, V> {
    async fn load(&self, key: &K) -> Option<V>;
    async fn store(&self, key: &K, value: &V) -> Result<(), String>;
    async fn delete(&self, key: &K) -> Result<(), String>;
}

// 内存缓存实现
struct InMemoryCache<K, V> {
    data: RwLock<HashMap<K, V>>,
}

impl<K, V> InMemoryCache<K, V>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new() -> Self {
        Self {
            data: RwLock::new(HashMap::new()),
        }
    }
}

#[async_trait]
impl<K, V> Cache<K, V> for InMemoryCache<K, V>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    async fn get(&self, key: &K) -> Option<V> {
        let data = self.data.read().await;
        data.get(key).cloned()
    }
    
    async fn put(&self, key: K, value: V) {
        let mut data = self.data.write().await;
        data.insert(key, value);
    }
    
    async fn remove(&self, key: &K) {
        let mut data = self.data.write().await;
        data.remove(key);
    }
}

// 模拟数据库数据源
struct DatabaseSource {
    data: RwLock<HashMap<String, String>>,
    write_delay_ms: u64, // 模拟数据库写入延迟
}

impl DatabaseSource {
    fn new(write_delay_ms: u64) -> Self {
        Self {
            data: RwLock::new(HashMap::new()),
            write_delay_ms,
        }
    }
}

#[async_trait]
impl DataSource<String, String> for DatabaseSource {
    async fn load(&self, key: &String) -> Option<String> {
        let data = self.data.read().await;
        data.get(key).cloned()
    }
    
    async fn store(&self, key: &String, value: &String) -> Result<(), String> {
        // 模拟写入延迟
        sleep(Duration::from_millis(self.write_delay_ms)).await;
        
        let mut data = self.data.write().await;
        data.insert(key.clone(), value.clone());
        Ok(())
    }
    
    async fn delete(&self, key: &String) -> Result<(), String> {
        // 模拟延迟
        sleep(Duration::from_millis(self.write_delay_ms)).await;
        
        let mut data = self.data.write().await;
        data.remove(key);
        Ok(())
    }
}

// 写透缓存策略
struct WriteThrough<K, V, C, D>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
    C: Cache<K, V> + Send + Sync,
    D: DataSource<K, V> + Send + Sync,
{
    cache: Arc<C>,
    data_source: Arc<D>,
}

impl<K, V, C, D> WriteThrough<K, V, C, D>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
    C: Cache<K, V> + Send + Sync,
    D: DataSource<K, V> + Send + Sync,
{
    fn new(cache: C, data_source: D) -> Self {
        Self {
            cache: Arc::new(cache),
            data_source: Arc::new(data_source),
        }
    }
    
    async fn get(&self, key: &K) -> Option<V> {
        // 首先尝试从缓存获取
        if let Some(value) = self.cache.get(key).await {
            return Some(value);
        }
        
        // 缓存中没有，从数据源加载
        match self.data_source.load(key).await {
            Some(value) => {
                // 更新缓存
                self.cache.put(key.clone(), value.clone()).await;
                Some(value)
            },
            None => None,
        }
    }
    
    async fn put(&self, key: K, value: V) -> Result<(), String> {
        // 写透策略：同步写入数据源和缓存
        self.data_source.store(&key, &value).await?;
        self.cache.put(key, value).await;
        Ok(())
    }
    
    async fn remove(&self, key: &K) -> Result<(), String> {
        // 同步从数据源和缓存中删除
        self.data_source.delete(key).await?;
        self.cache.remove(key).await;
        Ok(())
    }
}

/*
<think>
继续完成 `WriteBack` 缓存策略的代码，然后提供一个使用示例。
这部分代码将展示后写缓存如何工作，即先更新缓存，然后异步更新数据源。
</think>
*/

// 后写缓存策略
struct WriteBack<K, V, C, D>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
    C: Cache<K, V> + Send + Sync,
    D: DataSource<K, V> + Send + Sync,
{
    cache: Arc<C>,
    data_source: Arc<D>,
    dirty_queue: Arc<RwLock<HashMap<K, V>>>,
}

impl<K, V, C, D> WriteBack<K, V, C, D>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
    C: Cache<K, V> + Send + Sync,
    D: DataSource<K, V> + Send + Sync,
{
    fn new(cache: C, data_source: D) -> Self {
        Self {
            cache: Arc::new(cache),
            data_source: Arc::new(data_source),
            dirty_queue: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    // 启动后台写入进程
    async fn start_background_writer(&self, interval: Duration) {
        let dirty_queue = self.dirty_queue.clone();
        let data_source = self.data_source.clone();
        
        tokio::spawn(async move {
            loop {
                sleep(interval).await;
                
                // 获取并清空脏队列
                let dirty_entries = {
                    let mut queue = dirty_queue.write().await;
                    let entries = queue.clone();
                    queue.clear();
                    entries
                };
                
                // 写入数据源
                for (key, value) in dirty_entries {
                    match data_source.store(&key, &value).await {
                        Ok(_) => println!("后台写入成功: {}", key.to_string()),
                        Err(e) => {
                            println!("后台写入失败: {}, 错误: {}", key.to_string(), e);
                            
                            // 重新加入脏队列
                            let mut queue = dirty_queue.write().await;
                            queue.insert(key, value);
                        }
                    }
                }
            }
        });
    }
    
    async fn get(&self, key: &K) -> Option<V> {
        // 首先检查脏队列
        {
            let dirty = self.dirty_queue.read().await;
            if let Some(value) = dirty.get(key) {
                return Some(value.clone());
            }
        }
        
        // 其次检查缓存
        if let Some(value) = self.cache.get(key).await {
            return Some(value);
        }
        
        // 最后尝试从数据源加载
        match self.data_source.load(key).await {
            Some(value) => {
                // 更新缓存，但不更新脏队列
                self.cache.put(key.clone(), value.clone()).await;
                Some(value)
            },
            None => None,
        }
    }
    
    async fn put(&self, key: K, value: V) {
        // 后写策略：立即更新缓存，将写入加入脏队列
        self.cache.put(key.clone(), value.clone()).await;
        
        let mut dirty = self.dirty_queue.write().await;
        dirty.insert(key, value);
    }
    
    async fn remove(&self, key: &K) {
        // 立即从缓存中删除
        self.cache.remove(key).await;
        
        // 从脏队列中删除
        {
            let mut dirty = self.dirty_queue.write().await;
            dirty.remove(key);
        }
        
        // 异步从数据源中删除
        let key_clone = key.clone();
        let data_source = self.data_source.clone();
        tokio::spawn(async move {
            if let Err(e) = data_source.delete(&key_clone).await {
                println!("删除操作失败: {}, 错误: {}", key_clone.to_string(), e);
            }
        });
    }
    
    // 强制将脏队列中的数据写入数据源
    async fn flush(&self) -> Result<(), String> {
        let dirty_entries = {
            let mut queue = self.dirty_queue.write().await;
            let entries = queue.clone();
            queue.clear();
            entries
        };
        
        // 写入所有脏数据
        for (key, value) in dirty_entries {
            if let Err(e) = self.data_source.store(&key, &value).await {
                return Err(format!("刷新失败: {}", e));
            }
        }
        
        Ok(())
    }
}

// 缓存场景分析器 - 用于测量性能并提供建议
struct CacheAnalyzer {
    reads: usize,
    writes: usize,
    read_hits: usize,
    read_time: Duration,
    write_time: Duration,
}

impl CacheAnalyzer {
    fn new() -> Self {
        Self {
            reads: 0,
            writes: 0,
            read_hits: 0,
            read_time: Duration::from_secs(0),
            write_time: Duration::from_secs(0),
        }
    }
    
    fn record_read(&mut self, hit: bool, time: Duration) {
        self.reads += 1;
        if hit {
            self.read_hits += 1;
        }
        self.read_time += time;
    }
    
    fn record_write(&mut self, time: Duration) {
        self.writes += 1;
        self.write_time += time;
    }
    
    fn analyze(&self) {
        println!("\n--- 缓存性能分析 ---");
        println!("读取次数: {}", self.reads);
        println!("写入次数: {}", self.writes);
        
        if self.reads > 0 {
            let hit_rate = (self.read_hits as f64 / self.reads as f64) * 100.0;
            println!("缓存命中率: {:.2}%", hit_rate);
            println!("平均读取时间: {:?}", self.read_time / self.reads as u32);
        }
        
        if self.writes > 0 {
            println!("平均写入时间: {:?}", self.write_time / self.writes as u32);
        }
        
        println!("建议:");
        if self.reads > 0 && (self.read_hits as f64 / self.reads as f64) < 0.5 {
            println!("- 缓存命中率较低，考虑预热缓存或增加缓存项TTL");
        }
        
        let read_write_ratio = if self.writes > 0 {
            self.reads as f64 / self.writes as f64
        } else {
            f64::INFINITY
        };
        
        println!("读写比: {:.2}", read_write_ratio);
        if read_write_ratio > 10.0 {
            println!("- 以读为主的工作负载，考虑使用ReadThrough/WriteThrough策略");
        } else if read_write_ratio < 1.0 {
            println!("- 以写为主的工作负载，考虑使用WriteBack策略减少写入延迟");
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建一个性能分析器
    let mut analyzer = CacheAnalyzer::new();
    
    println!("=== 写透缓存测试 ===");
    
    // 创建写透缓存
    let cache1 = InMemoryCache::new();
    let db1 = DatabaseSource::new(200); // 200ms写入延迟
    let write_through = WriteThrough::new(cache1, db1);
    
    // 写入测试
    for i in 1..=5 {
        let key = format!("key{}", i);
        let value = format!("value{}", i);
        
        let start = Instant::now();
        let _ = write_through.put(key, value).await;
        analyzer.record_write(start.elapsed());
    }
    
    // 读取测试
    for i in 1..=7 {
        let key = format!("key{}", i);
        
        let start = Instant::now();
        let result = write_through.get(&key).await;
        let hit = result.is_some() && i <= 5;
        analyzer.record_read(hit, start.elapsed());
        
        println!("写透读取 {}: {:?}", key, result);
    }
    
    println!("\n=== 后写缓存测试 ===");
    
    // 创建后写缓存
    let cache2 = InMemoryCache::new();
    let db2 = DatabaseSource::new(200); // 200ms写入延迟
    let write_back = WriteBack::new(cache2, db2);
    
    // 启动后台写入进程
    write_back.start_background_writer(Duration::from_secs(1)).await;
    
    // 写入测试
    for i in 1..=5 {
        let key = format!("key{}", i);
        let value = format!("value{}", i);
        
        let start = Instant::now();
        write_back.put(key, value).await;
        analyzer.record_write(start.elapsed());
    }
    
    // 立即读取 - 应该从缓存获取
    for i in 1..=7 {
        let key = format!("key{}", i);
        
        let start = Instant::now();
        let result = write_back.get(&key).await;
        let hit = result.is_some() && i <= 5;
        analyzer.record_read(hit, start.elapsed());
        
        println!("后写立即读取 {}: {:?}", key, result);
    }
    
    // 等待后台写入完成
    println!("\n等待后台写入...");
    sleep(Duration::from_secs(2)).await;
    
    // 刷新剩余的写入
    write_back.flush().await.unwrap();
    
    // 性能分析
    analyzer.analyze();
}
```

### 6.3 缓存一致性

缓存一致性模式确保缓存和数据源中的数据保持一致，即使在多个实例或分布式环境中。

```rust
use std::collections::HashMap;
use std::hash::Hash;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{broadcast, mpsc, RwLock};
use tokio::time::sleep;
use async_trait::async_trait;
use serde::{Serialize, Deserialize};

// 缓存事件类型
#[derive(Debug, Clone, Serialize, Deserialize)]
enum CacheEvent<K, V> {
    Put { key: K, value: V },
    Remove { key: K },
    Clear,
}

// 缓存接口
#[async_trait]
trait Cache<K, V> {
    async fn get(&self, key: &K) -> Option<V>;
    async fn put(&self, key: K, value: V);
    async fn remove(&self, key: &K);
    async fn clear(&self);
}

// 分布式缓存接口
#[async_trait]
trait DistributedCache<K, V>: Cache<K, V> {
    // 发布缓存事件
    async fn publish_event(&self, event: CacheEvent<K, V>);
    
    // 处理接收到的缓存事件
    async fn handle_event(&self, event: CacheEvent<K, V>);
}

// 内存缓存实现
struct InMemoryCache<K, V> {
    data: RwLock<HashMap<K, V>>,
    event_tx: Option<broadcast::Sender<CacheEvent<K, V>>>,
    instance_id: String,
}

impl<K, V> InMemoryCache<K, V>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new(instance_id: &str) -> Self {
        Self {
            data: RwLock::new(HashMap::new()),
            event_tx: None,
            instance_id: instance_id.to_string(),
        }
    }
    
    // 设置事件发布器
    fn set_event_publisher(&mut self, tx: broadcast::Sender<CacheEvent<K, V>>) {
        self.event_tx = Some(tx);
    }
}

#[async_trait]
impl<K, V> Cache<K, V> for InMemoryCache<K, V>
where
    K: Eq + Hash + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    async fn get(&self, key: &K) -> Option<V> {
        let data = self.data.read().await;
        data.get(key).cloned()
    }
    
    async fn put(&self, key: K, value: V) {
        let mut data = self.data.write().await;
        data.insert(key, value);
    }
    
    async fn remove(&self, key: &K) {
        let mut data = self.data.write().await;
        data.remove(key);
    }
    
    async fn clear(&self) {
        let mut data = self.data.write().await;
        data.clear();
    }
}

#[async_trait]
impl<K, V> DistributedCache<K, V> for InMemoryCache<K, V>
where
    K: Eq + Hash + Clone + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
{
    async fn publish_event(&self, event: CacheEvent<K, V>) {
        if let Some(tx) = &self.event_tx {
            let _ = tx.send(event);
        }
    }
    
    async fn handle_event(&self, event: CacheEvent<K, V>) {
        match event {
            CacheEvent::Put { key, value } => {
                self.put(key, value).await;
            },
            CacheEvent::Remove { key } => {
                self.remove(&key).await;
            },
            CacheEvent::Clear => {
                self.clear().await;
            },
        }
    }
}

// 分布式缓存协调器
struct CacheCoordinator<K, V> {
    caches: RwLock<HashMap<String, Arc<InMemoryCache<K, V>>>>,
    event_tx: broadcast::Sender<CacheEvent<K, V>>,
}

impl<K, V> CacheCoordinator<K, V>
where
    K: Eq + Hash + Clone + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Send + Sync + 'static + Serialize + for<'de> Deserialize<'de>,
{
...(about 80 lines omitted)...
    // 创建缓存协调器
    let (event_tx, _) = broadcast::channel(100);
    let coordinator = Arc::new(CacheCoordinator::new(event_tx.clone()));
    
    // 创建三个缓存实例
    let cache1 = coordinator.create_cache("cache1").await;
    let cache2 = coordinator.create_cache("cache2").await;
    let cache3 = coordinator.create_cache("cache3").await;
    
    // 在cache1中写入数据
    cache1.put("key1".to_string(), "value1".to_string()).await;
    cache1.publish_event(CacheEvent::Put {
        key: "key1".to_string(),
        value: "value1".to_string(),
    }).await;
    
    // 等待事件传播
    sleep(Duration::from_millis(100)).await;
    
    // 从所有缓存实例读取数据
    let value1 = cache1.get(&"key1".to_string()).await;
    let value2 = cache2.get(&"key1".to_string()).await;
    let value3 = cache3.get(&"key1".to_string()).await;
    
    println!("缓存1中的值: {:?}", value1);
    println!("缓存2中的值: {:?}", value2);
    println!("缓存3中的值: {:?}", value3);
    
    // 在cache2中删除数据
    cache2.remove(&"key1".to_string()).await;
    cache2.publish_event(CacheEvent::Remove {
        key: "key1".to_string(),
    }).await;
    
    // 等待事件传播
    sleep(Duration::from_millis(100)).await;
    
    // 再次从所有缓存实例读取数据
    let value1 = cache1.get(&"key1".to_string()).await;
    let value2 = cache2.get(&"key1".to_string()).await;
    let value3 = cache3.get(&"key1".to_string()).await;
    
    println!("删除后，缓存1中的值: {:?}", value1);
    println!("删除后，缓存2中的值: {:?}", value2);
    println!("删除后，缓存3中的值: {:?}", value3);
    
    // 模拟延迟事件传播
    println!("\n模拟网络分区场景:");
    
    // 设置一个新值但不发布事件
    cache1.put("key2".to_string(), "value2-cache1-only".to_string()).await;
    println!("在缓存1中设置了key2，但没有发布事件");
    
    // 检查所有缓存
    let value1 = cache1.get(&"key2".to_string()).await;
    let value2 = cache2.get(&"key2".to_string()).await;
    let value3 = cache3.get(&"key2".to_string()).await;
    
    println!("缓存1中的key2: {:?}", value1);
    println!("缓存2中的key2: {:?}", value2);
    println!("缓存3中的key2: {:?}", value3);
    
    // 手动执行同步
    println!("\n执行手动同步:");
    coordinator.sync_all_caches().await;
    
    // 等待同步完成
    sleep(Duration::from_millis(100)).await;
    
    // 再次检查所有缓存
    let value1 = cache1.get(&"key2".to_string()).await;
    let value2 = cache2.get(&"key2".to_string()).await;
    let value3 = cache3.get(&"key2".to_string()).await;
    
    println!("同步后，缓存1中的key2: {:?}", value1);
    println!("同步后，缓存2中的key2: {:?}", value2);
    println!("同步后，缓存3中的key2: {:?}", value3);
}
```

## 结论

这些分布式系统模式代表了应对分布式系统挑战的成熟解决方案。通过理解和应用这些模式，开发者可以构建更可靠、更高效的分布式系统。

每个模式都有其应用场景和权衡，选择合适的模式需要考虑系统的特定需求和约束。通过这些Rust示例，我们希望为分布式系统开发者提供实用的参考和指导。

```text
<think>
I notice that we've covered quite a few distributed system patterns already throughout the previous responses, including:

1. Replication patterns:
   - Single-master replication
   - Multi-master replication
   - Masterless replication
   - Raft consensus

2. Partitioning patterns:
   - Sharding
   - Consistent hashing
   - Replica distribution

3. Fault tolerance patterns:
   - Circuit breaker
   - Bulkhead
   - Timeout and retry
   - Backpressure

4. Transaction patterns:
   - Two-phase commit
   - Saga

5. Caching patterns:
   - Cache proxy
   - Write-through and write-back
   - Cache consistency

I'll continue with more patterns that we haven't covered yet. Some possibilities include:

1. Event-based patterns:
   - Event sourcing
   - CQRS (Command Query Responsibility Segregation)

2. Messaging patterns:
   - Publish-subscribe
   - Message queues
   - Request-response

3. Service discovery patterns

4. Concurrency patterns

Let me continue with these additional patterns to provide a more comprehensive overview of distributed systems patterns.
</think>
```

## 7. 消息传递模式

### 7.1 发布-订阅

发布-订阅模式允许消息发送者（发布者）将消息发送给多个接收者（订阅者），而不需要直接了解对方的存在。

```rust
use std::collections::{HashMap, HashSet};
use std::sync::{Arc, Weak};
use tokio::sync::{mpsc, RwLock};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

// 消息接口
trait Message: Send + Sync + Clone {}

// 简单的文本消息实现
#[derive(Debug, Clone, Serialize, Deserialize)]
struct TextMessage {
    id: String,
    topic: String,
    content: String,
    timestamp: u64,
}

impl Message for TextMessage {}

// 消息处理器特征
#[async_trait::async_trait]
trait MessageHandler: Send + Sync {
    async fn handle(&self, message: TextMessage);
}

// 订阅者
struct Subscriber {
    id: String,
    name: String,
    handler: Box<dyn MessageHandler>,
}

impl Subscriber {
    fn new(name: &str, handler: Box<dyn MessageHandler>) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            name: name.to_string(),
            handler,
        }
    }
    
    async fn receive(&self, message: TextMessage) {
        self.handler.handle(message).await;
    }
}

// 简单的打印消息处理器
struct LoggingHandler {
    prefix: String,
}

#[async_trait::async_trait]
impl MessageHandler for LoggingHandler {
    async fn handle(&self, message: TextMessage) {
        println!("[{}] 收到消息: {}: {}", 
                 self.prefix, message.topic, message.content);
    }
}

// 过滤消息处理器
struct FilteringHandler {
    prefix: String,
    filter_word: String,
}

#[async_trait::async_trait]
impl MessageHandler for FilteringHandler {
    async fn handle(&self, message: TextMessage) {
        if message.content.contains(&self.filter_word) {
            println!("[{}] 匹配过滤词 '{}': {}: {}", 
                    self.prefix, self.filter_word, message.topic, message.content);
        }
    }
}

// 消息代理
struct MessageBroker {
    topics: RwLock<HashMap<String, HashSet<String>>>,
    subscribers: RwLock<HashMap<String, Arc<Subscriber>>>,
}

impl MessageBroker {
    fn new() -> Self {
        Self {
            topics: RwLock::new(HashMap::new()),
            subscribers: RwLock::new(HashMap::new()),
        }
    }
    
    // 注册订阅者
    async fn register(&self, subscriber: Arc<Subscriber>) {
        let mut subscribers = self.subscribers.write().await;
        subscribers.insert(subscriber.id.clone(), subscriber);
    }
    
    // 取消注册订阅者
    async fn unregister(&self, subscriber_id: &str) {
        let mut subscribers = self.subscribers.write().await;
        subscribers.remove(subscriber_id);
        
        // 清理主题订阅
        let mut topics = self.topics.write().await;
        for subscribers_set in topics.values_mut() {
            subscribers_set.remove(subscriber_id);
        }
    }
    
    // 订阅主题
    async fn subscribe(&self, subscriber_id: &str, topic: &str) {
        let subscribers = self.subscribers.read().await;
        if !subscribers.contains_key(subscriber_id) {
            return;
        }
        
        let mut topics = self.topics.write().await;
        let subscribers_set = topics
            .entry(topic.to_string())
            .or_insert_with(HashSet::new);
        
        subscribers_set.insert(subscriber_id.to_string());
    }
    
    // 取消订阅主题
    async fn unsubscribe(&self, subscriber_id: &str, topic: &str) {
        let mut topics = self.topics.write().await;
        if let Some(subscribers_set) = topics.get_mut(topic) {
            subscribers_set.remove(subscriber_id);
        }
    }
    
    // 发布消息到主题
    async fn publish(&self, message: TextMessage) {
        let topics = self.topics.read().await;
        let subscribers = self.subscribers.read().await;
        
        if let Some(subscriber_ids) = topics.get(&message.topic) {
            for subscriber_id in subscriber_ids {
                if let Some(subscriber) = subscribers.get(subscriber_id) {
                    let message_clone = message.clone();
                    let subscriber_clone = subscriber.clone();
                    
                    // 异步发送消息
                    tokio::spawn(async move {
                        subscriber_clone.receive(message_clone).await;
                    });
                }
            }
        }
    }
    
    // 获取主题订阅者数量
    async fn subscriber_count(&self, topic: &str) -> usize {
        let topics = self.topics.read().await;
        topics.get(topic).map(|set| set.len()).unwrap_or(0)
    }
    
    // 获取所有主题
    async fn get_topics(&self) -> Vec<String> {
        let topics = self.topics.read().await;
        topics.keys().cloned().collect()
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建消息代理
    let broker = Arc::new(MessageBroker::new());
    
    // 创建订阅者
    let subscriber1 = Arc::new(Subscriber::new(
        "订阅者1", 
        Box::new(LoggingHandler { prefix: "订阅者1".to_string() })
    ));
    
    let subscriber2 = Arc::new(Subscriber::new(
        "订阅者2", 
        Box::new(LoggingHandler { prefix: "订阅者2".to_string() })
    ));
    
    let subscriber3 = Arc::new(Subscriber::new(
        "过滤订阅者", 
        Box::new(FilteringHandler { 
            prefix: "过滤器".to_string(),
            filter_word: "重要".to_string()
        })
    ));
    
    // 注册订阅者
    broker.register(subscriber1.clone()).await;
    broker.register(subscriber2.clone()).await;
    broker.register(subscriber3.clone()).await;
    
    // 订阅主题
    broker.subscribe(&subscriber1.id, "新闻").await;
    broker.subscribe(&subscriber1.id, "体育").await;
    broker.subscribe(&subscriber2.id, "科技").await;
    broker.subscribe(&subscriber3.id, "新闻").await;
    broker.subscribe(&subscriber3.id, "科技").await;
    
    // 发布消息
    let message1 = TextMessage {
        id: Uuid::new_v4().to_string(),
        topic: "新闻".to_string(),
        content: "重要新闻：今天是晴天".to_string(),
        timestamp: chrono::Utc::now().timestamp() as u64,
    };
    
    let message2 = TextMessage {
        id: Uuid::new_v4().to_string(),
        topic: "体育".to_string(),
        content: "体育赛事：足球比赛结果".to_string(),
        timestamp: chrono::Utc::now().timestamp() as u64,
    };
    
    let message3 = TextMessage {
        id: Uuid::new_v4().to_string(),
        topic: "科技".to_string(),
        content: "科技新闻：重要技术突破".to_string(),
        timestamp: chrono::Utc::now().timestamp() as u64,
    };
    
    println!("开始发布消息...");
    broker.publish(message1).await;
    broker.publish(message2).await;
    broker.publish(message3).await;
    
    // 等待消息处理
    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
    
    // 查看主题订阅情况
    let topics = broker.get_topics().await;
    for topic in &topics {
        let count = broker.subscriber_count(topic).await;
        println!("主题 '{}' 有 {} 个订阅者", topic, count);
    }
    
    // 取消订阅
    println!("\n订阅者1取消订阅'体育'主题");
    broker.unsubscribe(&subscriber1.id, "体育").await;
    
    // 再次发布消息
    let message4 = TextMessage {
        id: Uuid::new_v4().to_string(),
        topic: "体育".to_string(),
        content: "更多体育新闻：篮球比赛结果".to_string(),
        timestamp: chrono::Utc::now().timestamp() as u64,
    };
    
    println!("\n发布更多消息...");
    broker.publish(message4).await;
    
    // 等待消息处理
    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
    
    // 取消注册订阅者
    println!("\n取消注册订阅者2");
    broker.unregister(&subscriber2.id).await;
    
    // 再次发布消息
    let message5 = TextMessage {
        id: Uuid::new_v4().to_string(),
        topic: "科技".to_string(),
        content: "科技新闻：又一项重要突破".to_string(),
        timestamp: chrono::Utc::now().timestamp() as u64,
    };
    
    println!("\n最后发布消息...");
    broker.publish(message5).await;
    
    // 等待消息处理
    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
}
```

### 7.2 消息队列

消息队列模式提供异步点对点通信，允许生产者和消费者解耦，并支持消息缓冲、负载均衡和可靠传递。

```rust
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, Mutex, RwLock};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

// 消息状态
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum MessageStatus {
    Queued,
    Processing,
    Completed,
    Failed,
}

// 消息结构
#[derive(Debug, Clone, Serialize, Deserialize)]
struct Message {
    id: String,
    queue: String,
    payload: Vec<u8>,
    created_at: u64,
    visibility_timeout: u64,
    processing_started: Option<u64>,
    attempts: u32,
}

impl Message {
    fn new(queue: &str, payload: Vec<u8>) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            queue: queue.to_string(),
            payload,
            created_at: chrono::Utc::now().timestamp() as u64,
            visibility_timeout: 30, // 默认30秒
            processing_started: None,
            attempts: 0,
        }
    }
}

// 队列设置
#[derive(Debug, Clone)]
struct QueueSettings {
    max_size: usize,
    visibility_timeout: u64,
    max_attempts: u32,
    dead_letter_queue: Option<String>,
}

impl Default for QueueSettings {
    fn default() -> Self {
        Self {
            max_size: 10000,
            visibility_timeout: 30,
            max_attempts: 3,
            dead_letter_queue: None,
        }
    }
}

// 消息队列管理器
struct MessageQueue {
    queues: RwLock<HashMap<String, QueueSettings>>,
    messages: RwLock<HashMap<String, HashMap<String, Message>>>,
    in_flight: RwLock<HashMap<String, HashMap<String, Message>>>,
    timers: Mutex<HashMap<String, tokio::task::JoinHandle<()>>>,
}

impl MessageQueue {
    fn new() -> Self {
        Self {
            queues: RwLock::new(HashMap::new()),
            messages: RwLock::new(HashMap::new()),
            in_flight: RwLock::new(HashMap::new()),
            timers: Mutex::new(HashMap::new()),
        }
    }
    
    // 创建一个新队列
    async fn create_queue(&self, name: &str, settings: QueueSettings) -> Result<(), String> {
        let mut queues = self.queues.write().await;
        if queues.contains_key(name) {
            return Err(format!("队列 '{}' 已存在", name));
        }
        
        queues.insert(name.to_string(), settings);
        
        // 初始化消息存储
        let mut messages = self.messages.write().await;
        messages.insert(name.to_string(), HashMap::new());
        
        // 初始化in-flight存储
        let mut in_flight = self.in_flight.write().await;
        in_flight.insert(name.to_string(), HashMap::new());
        
        // 启动可见性超时检查器
        self.start_visibility_checker(name).await;
        
        Ok(())
    }
    
    // 发送消息到队列
    async fn send(&self, queue_name: &str, payload: Vec<u8>) -> Result<String, String> {
        // 检查队列是否存在
        let queues = self.queues.read().await;
        let settings = match queues.get(queue_name) {
            Some(settings) => settings.clone(),
            None => return Err(format!("队列 '{}' 不存在", queue_name)),
        };
        
        // 创建消息
        let mut message = Message::new(queue_name, payload);
        message.visibility_timeout = settings.visibility_timeout;
        
        // 检查队列是否已满
        let mut messages = self.messages.write().await;
        let queue_messages = messages.get_mut(queue_name).unwrap();
        
        if queue_messages.len() >= settings.max_size {
            return Err(format!("队列 '{}' 已满", queue_name));
        }
        
        // 添加消息到队列
        let message_id = message.id.clone();
        queue_messages.insert(message_id.clone(), message);
        
        Ok(message_id)
    }
    
    // 接收消息
    async fn receive(&self, queue_name: &str, max_messages: usize) -> Result<Vec<Message>, String> {
        // 检查队列是否存在
        let queues = self.queues.read().await;
        if !queues.contains_key(queue_name) {
            return Err(format!("队列 '{}' 不存在", queue_name));
        }
        
        let now = chrono::Utc::now().timestamp() as u64;
        let mut result = Vec::new();
        
        // 获取消息
        let mut messages = self.messages.write().await;
        let mut in_flight = self.in_flight.write().await;
        
        let queue_messages = messages.get_mut(queue_name).unwrap();
        let in_flight_messages = in_flight.get_mut(queue_name).unwrap();
        
        // 筛选可见的消息
        let mut available_messages = Vec::new();
        for (id, message) in queue_messages.iter() {
            available_messages.push(id.clone());
            if available_messages.len() >= max_messages {
                break;
            }
        }
        
        // 移动消息到in-flight状态
        for message_id in available_messages {
            if let Some(mut message) = queue_messages.remove(&message_id) {
                message.processing_started = Some(now);
                message.attempts += 1;
                
                result.push(message.clone());
                in_flight_messages.insert(message_id, message);
            }
        }
        
        Ok(result)
    }
    
    // 完成处理消息
    async fn complete(&self, queue_name: &str, message_id: &str) -> Result<(), String> {
        // 检查队列是否存在
        let queues = self.queues.read().await;
        if !queues.contains_key(queue_name) {
            return Err(format!("队列 '{}' 不存在", queue_name));
        }
        
        // 从in-flight中删除消息
        let mut in_flight = self.in_flight.write().await;
        let in_flight_messages = in_flight.get_mut(queue_name).unwrap();
        
        if in_flight_messages.remove(message_id).is_none() {
            return Err(format!("消息 '{}' 不在处理中", message_id));
        }
        
        Ok(())
    }
    
    // 标记消息处理失败
    async fn fail(&self, queue_name: &str, message_id: &str) -> Result<(), String> {
        // 检查队列是否存在
        let queues = self.queues.read().await;
        let settings = match queues.get(queue_name) {
            Some(settings) => settings.clone(),
            None => return Err(format!("队列 '{}' 不存在", queue_name)),
        };
        
        // 从in-flight中获取消息
        let mut in_flight = self.in_flight.write().await;
        let in_flight_messages = in_flight.get_mut(queue_name).unwrap();
        
        let message = match in_flight_messages.remove(message_id) {
            Some(message) => message,
            None => return Err(format!("消息 '{}' 不在处理中", message_id)),
        };
        
        // 检查重试次数
        if message.attempts >= settings.max_attempts {
            // 如果有死信队列，则移动到死信队列
            if let Some(dlq) = &settings.dead_letter_queue {
                if queues.contains_key(dlq) {
                    drop(queues); // 释放读锁
                    self.send(dlq, message.payload).await?;
                }
            }
        } else {
            // 否则将消息放回队列
            let mut messages = self.messages.write().await;
            let queue_messages = messages.get_mut(queue_name).unwrap();
            queue_messages.insert(message_id.to_string(), message);
        }
        
        Ok(())
    }
    
    // 启动可见性超时检查器
    async fn start_visibility_checker(&self, queue_name: &str) {
        let queue_name = queue_name.to_string();
        let queue_clone = Arc::new(self.clone());
        
        let handle = tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(1));
            
            loop {
                interval.tick().await;
                queue_clone.check_visibility_timeouts(&queue_name).await;
            }
        });
        
        let mut timers = self.timers.lock().await;
        timers.insert(queue_name, handle);
    }
    
    // 检查可见性超时
    async fn check_visibility_timeouts(&self, queue_name: &str) {
        let now = chrono::Utc::now().timestamp() as u64;
        
        // 获取in-flight消息
        let mut in_flight = self.in_flight.write().await;
        let in_flight_messages = match in_flight.get_mut(queue_name) {
            Some(messages) => messages,
            None => return,
        };
        
        // 检查超时消息
        let mut timed_out_messages = Vec::new();
        for (id, message) in in_flight_messages.iter() {
            if let Some(started) = message.processing_started {
                if now - started > message.visibility_timeout {
                    timed_out_messages.push(id.clone());
                }
            }
        }
        
        // 处理超时消息
        if !timed_out_messages.is_empty() {
            drop(in_flight); // 释放写锁
            
            for message_id in timed_out_messages {
                let _ = self.fail(queue_name, &message_id).await;
            }
        }
    }
    
    // 获取队列长度
    async fn queue_length(&self, queue_name: &str) -> Result<usize, String> {
        let messages = self.messages.read().await;
        match messages.get(queue_name) {
            Some(queue_messages) => Ok(queue_messages.len()),
            None => Err(format!("队列 '{}' 不存在", queue_name)),
        }
    }
    
    // 获取正在处理的消息数
    async fn in_flight_count(&self, queue_name: &str) -> Result<usize, String> {
        let in_flight = self.in_flight.read().await;
        match in_flight.get(queue_name) {
            Some(in_flight_messages) => Ok(in_flight_messages.len()),
            None => Err(format!("队列 '{}' 不存在", queue_name)),
        }
    }
}

impl Clone for MessageQueue {
    fn clone(&self) -> Self {
        Self {
            queues: self.queues.clone(),
            messages: self.messages.clone(),
            in_flight: self.in_flight.clone(),
            timers: self.timers.clone(),
        }
    }
}

// 生产者接口
struct Producer {
    queue: Arc<MessageQueue>,
}

impl Producer {
    fn new(queue: Arc<MessageQueue>) -> Self {
        Self { queue }
    }
    
    async fn send(&self, queue_name: &str, data: &str) -> Result<String, String> {
        self.queue.send(queue_name, data.as_bytes().to_vec()).await
    }
}

// 消费者接口
struct Consumer {
    queue: Arc<MessageQueue>,
    queue_name: String,
}

impl Consumer {
    fn new(queue: Arc<MessageQueue>, queue_name: &str) -> Self {
        Self {
            queue,
            queue_name: queue_name.to_string(),
        }
    }
    
    async fn receive(&self, max_messages: usize) -> Result<Vec<(String, String)>, String> {
        let messages = self.queue.receive(&self.queue_name, max_messages).await?;
        
        let mut result = Vec::new();
        for message in messages {
            let payload = String::from_utf8(message.payload.clone())
                .map_err(|e| format!("解析消息失败: {}", e))?;
            
            result.push((message.id, payload));
        }
        
        Ok(result)
    }
    
    async fn complete(&self, message_id: &str) -> Result<(), String> {
        self.queue.complete(&self.queue_name, message_id).await
    }
    
    async fn fail(&self, message_id: &str) -> Result<(), String> {
        self.queue.fail(&self.queue_name, message_id).await
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建消息队列
    let queue = Arc::new(MessageQueue::new());
    
    // 创建主队列
    let main_settings = QueueSettings {
        max_size: 1000,
        visibility_timeout: 10,
        max_attempts: 3,
        dead_letter_queue: Some("dead_letter".to_string()),
    };
    
    queue.create_queue("main", main_settings).await.unwrap();
    
    // 创建死信队列
    let dlq_settings = QueueSettings {
        max_size: 1000,
        visibility_timeout: 60,
        max_attempts: 1,
        dead_letter_queue: None,
    };
    
    queue.create_queue("dead_letter", dlq_settings).await.unwrap();
    
    // 创建生产者
    let producer = Producer::new(queue.clone());
    
    // 创建消费者
    let consumer1 = Consumer::new(queue.clone(), "main");
    let consumer2 = Consumer::new(queue.clone(), "main");
    let dlq_consumer = Consumer::new(queue.clone(), "dead_letter");
    
    // 生产一些消息
    println!("生产消息...");
    for i in 1..=10 {
        let message_id = producer.send("main", &format!("消息内容 #{}", i)).await.unwrap();
        println!("发送消息 #{}: ID = {}", i, message_id);
    }
    
    // 检查队列长度
    let length = queue.queue_length("main").await.unwrap();
    println!("队列长度: {}", length);
    
    // 消费者1消费一些消息
    println!("\n消费者1接收消息...");
    let messages1 = consumer1.receive(3).await.unwrap();
    for (id, content) in &messages1 {
        println!("消费者1收到: {} - {}", id, content);
    }
    
    // 消费者2消费一些消息
    println!("\n消费者2接收消息...");
    let messages2 = consumer2.receive(4).await.unwrap();
    for (id, content) in &messages2 {
        println!("消费者2收到: {} - {}", id, content);
    }
    
    // 检查in-flight计数
    let in_flight = queue.in_flight_count("main").await.unwrap();
    println!("正在处理的消息数: {}", in_flight);
    
    // 消费者1完成一些消息
    println!("\n消费者1完成处理...");
    for (id, _) in messages1.iter().take(2) {
        consumer1.complete(id).await.unwrap();
        println!("消费者1完成: {}", id);
    }
    
    // 消费者2将一些消息标记为失败
    println!("\n消费者2标记失败...");
    for (id, _) in messages2.iter().take(2) {
        consumer2.fail(id).await.unwrap();
        println!("消费者2失败: {}", id);
    }
    
    // 再次检查队列长度和in-flight计数
    let length = queue.queue_length("main").await.unwrap();
    let in_flight = queue.in_flight_count("main").await.unwrap();
    println!("队列长度: {}, 正在处理: {}", length, in_flight);
    
    // 等待可见性超时
    println!("\n等待可见性超时...");
    tokio::time::sleep(Duration::from_secs(11)).await;
    
    // 检查队列状态
    let length = queue.queue_length("main").await.unwrap();
    let in_flight = queue.in_flight_count("main").await.unwrap();
    let dlq_length = queue.queue_length("dead_letter").await.unwrap();
    
    println!("超时后:");
    println!("  主队列长度: {}", length);
    println!("  正在处理: {}", in_flight);
    println!("  死信队列长度: {}", dlq_length);
    
    // 从死信队列接收消息
    if dlq_length > 0 {
        println!("\n从死信队列接收消息...");
        let dlq_messages = dlq_consumer.receive(10).await.unwrap();
        for (id, content) in &dlq_messages {
            println!("死信队列消息: {} - {}", id, content);
            dlq_consumer.complete(id).await.unwrap();
        }
    }
}
```

### 7.3 请求-响应

请求-响应是一种常见的通信模式，客户端发送请求，服务器处理并返回响应。

```rust
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, oneshot, Mutex, RwLock};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

// 请求和响应结构
#[derive(Debug, Clone, Serialize, Deserialize)]
struct Request {
    id: String,
    service: String,
    method: String,
    payload: Vec<u8>,
    timeout_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Response {
    request_id: String,
    status: ResponseStatus,
    payload: Vec<u8>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
enum ResponseStatus {
    Success,
    Error,
    Timeout,
    ServiceUnavailable,
}

// 请求处理函数类型
type RequestHandler = Arc<dyn Fn(Request) -> Response + Send + Sync>;

// 服务注册表
struct ServiceRegistry {
    services: RwLock<HashMap<String, HashMap<String, RequestHandler>>>,
}

impl ServiceRegistry {
    fn new() -> Self {
        Self {
            services: RwLock::new(HashMap::new()),
        }
    }
    
    // 注册服务处理函数
    async fn register_handler<F>(&self, service: &str, method: &str, handler: F)
    where
        F: Fn(Request) -> Response + Send + Sync + 'static,
    {
        let mut services = self.services.write().await;
        
        let service_methods = services
            .entry(service.to_string())
            .or_insert_with(HashMap::new);
        
        service_methods.insert(method.to_string(), Arc::new(handler));
    }
    
    // 查找处理函数
    async fn find_handler(&self, service: &str, method: &str) -> Option<RequestHandler> {
        let services = self.services.read().await;
        
        services.get(service)
            .and_then(|methods| methods.get(method).cloned())
    }
    
    // 列出所有注册的服务和方法
    async fn list_services(&self) -> HashMap<String, Vec<String>> {
        let services = self.services.read().await;
        let mut result = HashMap::new();
        
        for (service, methods) in services.iter() {
            result.insert(
                service.clone(),
                methods.keys().cloned().collect(),
            );
        }
        
        result
    }
}

// 消息路由器
struct MessageRouter {
    registry: Arc<ServiceRegistry>,
    pending: Mutex<HashMap<String, oneshot::Sender<Response>>>,
}

impl MessageRouter {
    fn new(registry: Arc<ServiceRegistry>) -> Self {
        Self {
            registry,
            pending: Mutex::new(HashMap::new()),
        }
    }
    
    // 发送请求并等待响应
    async fn send_request(&self, request: Request) -> Result<Response, String> {
        let (response_tx, response_rx) = oneshot::channel();
        
        // 存储响应通道
        let request_id = request.id.clone();
        let mut pending = self.pending.lock().await;
        pending.insert(request_id.clone(), response_tx);
        drop(pending);
        
        // 处理请求
        let router = self.clone();
        let request_clone = request.clone();
        
        tokio::spawn(async move {
            router.handle_request(request_clone).await;
        });
        
        // 等待响应或超时
        let timeout = Duration::from_millis(request.timeout_ms);
        match tokio::time::timeout(timeout, response_rx).await {
            Ok(Ok(response)) => Ok(response),
            Ok(Err(_)) => {
                // 接收方已关闭
                Err("接收响应失败".to_string())
            }
            Err(_) => {
                // 超时
                let mut pending = self.pending.lock().await;
                pending.remove(&request_id);
                
                Ok(Response {
                    request_id,
                    status: ResponseStatus::Timeout,
                    payload: Vec::new(),
                })
            }
        }
    }
    
    // 处理传入的请求
    async fn handle_request(&self, request: Request) {
        let service = request.service.clone();
        let method = request.method.clone();
        
        // 查找处理函数
        let handler = self.registry.find_handler(&service, &method).await;
        
        let response = match handler {
            Some(handler) => handler(request.clone()),
            None => Response {
                request_id: request.id.clone(),
                status: ResponseStatus::ServiceUnavailable,
                payload: Vec::new(),
            },
        };
        
        // 发送响应
        let mut pending = self.pending.lock().await;
        if let Some(tx) = pending.remove(&request.id) {
            let _ = tx.send(response);
        }
    }
    
    // 直接发送响应（用于异步处理）
    async fn send_response(&self, response: Response) -> Result<(), String> {
        let mut pending = self.pending.lock().await;
        
        match pending.remove(&response.request_id) {
            Some(tx) => {
                tx.send(response).map_err(|_| "发送响应失败".to_string())
            }
            None => Err(format!("未找到请求ID: {}", response.request_id)),
        }
    }
}

impl Clone for MessageRouter {
    fn clone(&self) -> Self {
        Self {
            registry: self.registry.clone(),
            pending: self.pending.clone(),
        }
    }
}

// 客户端 - 发送请求
struct Client {
    router: Arc<MessageRouter>,
    timeout_ms: u64,
}

impl Client {
    fn new(router: Arc<MessageRouter>, timeout_ms: u64) -> Self {
        Self { router, timeout_ms }
    }
    
    async fn call(&self, service: &str, method: &str, payload: Vec<u8>) -> Result<Response, String> {
        let request = Request {
            id: Uuid::new_v4().to_string(),
            service: service.to_string(),
            method: method.to_string(),
            payload,
            timeout_ms: self.timeout_ms,
        };
        
        self.router.send_request(request).await
    }
    
    async fn call_with_timeout(&self, service: &str, method: &str, payload: Vec<u8>, timeout_ms: u64) -> Result<Response, String> {
        let request = Request {
            id: Uuid::new_v4().to_string(),
            service: service.to_string(),
            method: method.to_string(),
            payload,
            timeout_ms,
        };
        
        self.router.send_request(request).await
    }
}

// 服务器 - 处理请求
struct Server {
    registry: Arc<ServiceRegistry>,
    name: String,
}

impl Server {
    fn new(registry: Arc<ServiceRegistry>, name: &str) -> Self {
        Self {
            registry,
            name: name.to_string(),
        }
    }
    
    // 注册处理函数
    async fn register<F>(&self, method: &str, handler: F)
    where
        F: Fn(Request) -> Response + Send + Sync + 'static,
    {
        self.registry.register_handler(&self.name, method, handler).await;
    }

/*
<think>
我应该继续之前的代码示例，完成请求-响应模式的实现，并可能继续讲解其他消息传递模式。我将继续代码示例，完成相关实现。
</think>
*/

    // 注册异步处理函数
    async fn register_async<F, Fut>(&self, method: &str, router: Arc<MessageRouter>, handler: F)
    where
        F: Fn(Request) -> Fut + Send + Sync + 'static,
        Fut: std::future::Future<Output = Response> + Send + 'static,
    {
        let service_name = self.name.clone();
        let router_clone = router.clone();
        
        self.registry.register_handler(&self.name, method, move |request| {
            let request_id = request.id.clone();
            let router = router_clone.clone();
            
            // 创建一个默认响应，表示处理中
            let response = Response {
                request_id: request_id.clone(),
                status: ResponseStatus::Timeout, // 临时状态
                payload: Vec::new(),
            };
            
            // 创建异步任务处理请求
            let request_clone = request.clone();
            tokio::spawn(async move {
                let result = handler(request_clone).await;
                let _ = router.send_response(result).await;
            });
            
            // 返回临时响应，但实际结果将通过router异步发送
            response
        }).await;
    }
}

// 请求和响应编解码器工具
struct Codec;

impl Codec {
    // 将请求序列化为字节
    fn serialize_request(request: &Request) -> Result<Vec<u8>, String> {
        bincode::serialize(request)
            .map_err(|e| format!("序列化请求失败: {}", e))
    }
    
    // 从字节反序列化请求
    fn deserialize_request(bytes: &[u8]) -> Result<Request, String> {
        bincode::deserialize(bytes)
            .map_err(|e| format!("反序列化请求失败: {}", e))
    }
    
    // 将响应序列化为字节
    fn serialize_response(response: &Response) -> Result<Vec<u8>, String> {
        bincode::serialize(response)
            .map_err(|e| format!("序列化响应失败: {}", e))
    }
    
    // 从字节反序列化响应
    fn deserialize_response(bytes: &[u8]) -> Result<Response, String> {
        bincode::deserialize(bytes)
            .map_err(|e| format!("反序列化响应失败: {}", e))
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建服务注册表
    let registry = Arc::new(ServiceRegistry::new());
    
    // 创建消息路由器
    let router = Arc::new(MessageRouter::new(registry.clone()));
    
    // 创建服务器和客户端
    let echo_server = Server::new(registry.clone(), "echo");
    let math_server = Server::new(registry.clone(), "math");
    let client = Client::new(router.clone(), 5000); // 5秒超时
    
    // 注册同步处理函数
    echo_server.register("ping", |request| {
        println!("处理 ping 请求...");
        
        // 简单的回显功能
        Response {
            request_id: request.id,
            status: ResponseStatus::Success,
            payload: request.payload,
        }
    }).await;
    
    // 注册计算平方的处理函数
    math_server.register("square", |request| {
        println!("处理 square 请求...");
        
        // 解析数字，计算平方
        match String::from_utf8(request.payload) {
            Ok(payload_str) => {
                match payload_str.parse::<i32>() {
                    Ok(number) => {
                        let result = number * number;
                        Response {
                            request_id: request.id,
                            status: ResponseStatus::Success,
                            payload: result.to_string().into_bytes(),
                        }
                    },
                    Err(_) => Response {
                        request_id: request.id,
                        status: ResponseStatus::Error,
                        payload: "无效的数字格式".to_string().into_bytes(),
                    },
                }
            },
            Err(_) => Response {
                request_id: request.id,
                status: ResponseStatus::Error,
                payload: "无效的UTF-8数据".to_string().into_bytes(),
            },
        }
    }).await;
    
    // 注册一个慢速处理函数
    math_server.register("slow_operation", |request| {
        println!("处理 slow_operation 请求...");
        
        // 模拟耗时操作
        std::thread::sleep(Duration::from_millis(3000));
        
        Response {
            request_id: request.id,
            status: ResponseStatus::Success,
            payload: "慢操作已完成".to_string().into_bytes(),
        }
    }).await;
    
    // 注册异步处理函数
    math_server.register_async("async_operation", router.clone(), |request| async move {
        println!("异步处理 async_operation 请求...");
        
        // 模拟异步耗时操作
        tokio::time::sleep(Duration::from_millis(2000)).await;
        
        Response {
            request_id: request.id,
            status: ResponseStatus::Success,
            payload: "异步操作已完成".to_string().into_bytes(),
        }
    }).await;
    
    // 列出已注册的服务
    let services = registry.list_services().await;
    println!("已注册的服务:");
    for (service, methods) in &services {
        println!("  服务 '{}': {:?}", service, methods);
    }
    
    // 发送一些请求
    println!("\n发送请求...");
    
    // echo/ping
    let echo_response = client.call(
        "echo", 
        "ping", 
        "Hello, World!".to_string().into_bytes()
    ).await.unwrap();
    
    println!("echo/ping 响应: {:?}", String::from_utf8_lossy(&echo_response.payload));
    
    // math/square
    let square_response = client.call(
        "math", 
        "square", 
        "12".to_string().into_bytes()
    ).await.unwrap();
    
    println!("math/square 响应: {:?}", String::from_utf8_lossy(&square_response.payload));
    
    // math/slow_operation (可能超时)
    println!("\n调用慢速操作，超时设置为2秒...");
    let slow_response = client.call_with_timeout(
        "math", 
        "slow_operation", 
        Vec::new(),
        2000 // 2秒超时
    ).await;
    
    match slow_response {
        Ok(response) => println!("慢操作响应: {:?}", String::from_utf8_lossy(&response.payload)),
        Err(e) => println!("慢操作错误: {}", e),
    }
    
    // math/async_operation
    println!("\n调用异步操作...");
    let async_response = client.call(
        "math", 
        "async_operation", 
        Vec::new()
    ).await.unwrap();
    
    println!("异步操作响应: {:?}", String::from_utf8_lossy(&async_response.payload));
    
    // 调用不存在的服务
    println!("\n调用不存在的服务...");
    let not_found_response = client.call(
        "not_exists", 
        "method", 
        Vec::new()
    ).await.unwrap();
    
    println!("不存在服务响应状态: {:?}", not_found_response.status);
}
```

## 8. 稳定性模式

### 8.1 限流器

限流器模式用于控制请求速率，防止系统过载。

```rust
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::Mutex;
use tokio::time::sleep;
use async_trait::async_trait;

// 限流器特征
#[async_trait]
trait RateLimiter {
    // 尝试获取令牌，如果成功返回true，否则返回false
    async fn acquire(&self) -> bool;
    
    // 尝试获取令牌，如果不能立即获取则等待
    async fn acquire_or_wait(&self) -> Result<(), String>;
    
    // 获取当前限流器状态
    async fn get_status(&self) -> RateLimiterStatus;
}

// 限流器状态
#[derive(Debug, Clone)]
struct RateLimiterStatus {
    // 可用令牌或请求数
    available: usize,
    // 最大容量
    capacity: usize,
    // 上次补充时间
    last_refill: Option<Instant>,
    // 等待中的请求数
    waiting: usize,
}

// 令牌桶限流器
struct TokenBucket {
    tokens: Mutex<usize>,
    capacity: usize,
    refill_rate: usize,
    refill_interval: Duration,
    last_refill: Mutex<Option<Instant>>,
    waiting: Mutex<usize>,
}

impl TokenBucket {
    fn new(capacity: usize, refill_rate: usize, refill_interval: Duration) -> Self {
        Self {
            tokens: Mutex::new(capacity),
            capacity,
            refill_rate,
            refill_interval,
            last_refill: Mutex::new(None),
            waiting: Mutex::new(0),
        }
    }
    
    // 补充令牌
    async fn refill(&self) {
        let mut tokens = self.tokens.lock().await;
        let mut last_refill = self.last_refill.lock().await;
        
        let now = Instant::now();
        let elapsed = last_refill
            .map(|time| now.duration_since(time))
            .unwrap_or(self.refill_interval);
        
        // 计算需要补充的令牌数
        let intervals = (elapsed.as_millis() as f64 / self.refill_interval.as_millis() as f64).floor() as usize;
        
        if intervals > 0 {
            let new_tokens = self.refill_rate * intervals;
            *tokens = (*tokens + new_tokens).min(self.capacity);
            *last_refill = Some(now);
        }
    }
}

#[async_trait]
impl RateLimiter for TokenBucket {
    async fn acquire(&self) -> bool {
        // 先补充令牌
        self.refill().await;
        
        let mut tokens = self.tokens.lock().await;
        
        if *tokens > 0 {
            *tokens -= 1;
            true
        } else {
            false
        }
    }
    
    async fn acquire_or_wait(&self) -> Result<(), String> {
        // 尝试获取令牌
        let mut waiting = self.waiting.lock().await;
        *waiting += 1;
        
        loop {
            if self.acquire().await {
                *waiting -= 1;
                return Ok(());
            }
            
            // 等待一小段时间再试
            sleep(Duration::from_millis(50)).await;
        }
    }
    
    async fn get_status(&self) -> RateLimiterStatus {
        let tokens = *self.tokens.lock().await;
        let last_refill = *self.last_refill.lock().await;
        let waiting = *self.waiting.lock().await;
...(about 143 lines omitted)...
        let window_end = now;
        let window_start = window_end - self.window_duration;
        
        *self.request_counts.lock().await = clean_requests;
        
        // 构建状态
        RateLimiterStatus {
            available: self.max_requests.saturating_sub(count),
            capacity: self.max_requests,
            last_refill: None,
            waiting: *self.waiting.lock().await,
        }
    }
}

// 分布式限流器 (使用外部存储)
struct DistributedRateLimiter<S> {
    storage: Arc<S>,
    key: String,
    capacity: usize,
    refill_rate: usize,
    refill_interval: Duration,
    waiting: Mutex<usize>,
}

// 分布式存储特征
#[async_trait]
trait DistributedStorage: Send + Sync {
    async fn get_counter(&self, key: &str) -> Result<usize, String>;
    async fn increment_counter(&self, key: &str, amount: usize) -> Result<usize, String>;
    async fn decrement_counter(&self, key: &str, amount: usize) -> Result<usize, String>;
    async fn get_last_update(&self, key: &str) -> Result<Option<u64>, String>;
    async fn set_last_update(&self, key: &str, timestamp: u64) -> Result<(), String>;
}

// 内存模拟分布式存储 (仅用于演示)
struct InMemoryStorage {
    counters: Mutex<HashMap<String, usize>>,
    last_updates: Mutex<HashMap<String, u64>>,
}

impl InMemoryStorage {
    fn new() -> Self {
        Self {
            counters: Mutex::new(HashMap::new()),
            last_updates: Mutex::new(HashMap::new()),
        }
    }
}

#[async_trait]
impl DistributedStorage for InMemoryStorage {
    async fn get_counter(&self, key: &str) -> Result<usize, String> {
        let counters = self.counters.lock().await;
        Ok(*counters.get(key).unwrap_or(&0))
    }
    
    async fn increment_counter(&self, key: &str, amount: usize) -> Result<usize, String> {
        let mut counters = self.counters.lock().await;
        let counter = counters.entry(key.to_string()).or_insert(0);
        *counter += amount;
        Ok(*counter)
    }
    
    async fn decrement_counter(&self, key: &str, amount: usize) -> Result<usize, String> {
        let mut counters = self.counters.lock().await;
        let counter = counters.entry(key.to_string()).or_insert(0);
        *counter = counter.saturating_sub(amount);
        Ok(*counter)
    }
    
    async fn get_last_update(&self, key: &str) -> Result<Option<u64>, String> {
        let updates = self.last_updates.lock().await;
        Ok(updates.get(key).copied())
    }
    
    async fn set_last_update(&self, key: &str, timestamp: u64) -> Result<(), String> {
        let mut updates = self.last_updates.lock().await;
        updates.insert(key.to_string(), timestamp);
        Ok(())
    }
}

impl<S> DistributedRateLimiter<S>
where
    S: DistributedStorage + 'static,
{
    fn new(
        storage: Arc<S>,
        key: &str,
        capacity: usize,
        refill_rate: usize,
        refill_interval: Duration,
    ) -> Self {
        Self {
            storage,
            key: key.to_string(),
            capacity,
            refill_rate,
            refill_interval,
            waiting: Mutex::new(0),
        }
    }
    
    // 补充令牌
    async fn refill(&self) -> Result<(), String> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        
        let last_update = self.storage.get_last_update(&self.key).await?;
        
        if let Some(last_time) = last_update {
            let elapsed_secs = now.saturating_sub(last_time);
            let refill_interval_secs = self.refill_interval.as_secs();
            
            if elapsed_secs >= refill_interval_secs && refill_interval_secs > 0 {
                let intervals = elapsed_secs / refill_interval_secs;
                let new_tokens = (intervals as usize) * self.refill_rate;
                
                // 添加令牌，但不超过容量
                let current = self.storage.get_counter(&self.key).await?;
                let to_add = (self.capacity - current).min(new_tokens);
                
                if to_add > 0 {
                    self.storage.increment_counter(&self.key, to_add).await?;
                }
                
                // 更新最后更新时间
                self.storage.set_last_update(&self.key, now).await?;
            }
        } else {
            // 首次初始化
            self.storage.increment_counter(&self.key, self.capacity).await?;
            self.storage.set_last_update(&self.key, now).await?;
        }
        
        Ok(())
    }
}

#[async_trait]
impl<S> RateLimiter for DistributedRateLimiter<S>
where
    S: DistributedStorage + 'static,
{
    async fn acquire(&self) -> bool {
        // 尝试补充令牌
        if let Err(_) = self.refill().await {
            return false;
        }
        
        // 尝试获取令牌
        match self.storage.decrement_counter(&self.key, 1).await {
            Ok(remaining) if remaining != usize::MAX => true,
            _ => false,
        }
    }
    
    async fn acquire_or_wait(&self) -> Result<(), String> {
        let mut waiting = self.waiting.lock().await;
        *waiting += 1;
        
        loop {
            if self.acquire().await {
                *waiting -= 1;
                return Ok(());
            }
            
            // 等待一小段时间再试
            sleep(Duration::from_millis(100)).await;
        }
    }
    
    async fn get_status(&self) -> RateLimiterStatus {
        // 先补充令牌
        let _ = self.refill().await;
        
        let available = self.storage.get_counter(&self.key).await.unwrap_or(0);
        let last_update = self.storage.get_last_update(&self.key).await.unwrap_or(None);
        
        RateLimiterStatus {
            available,
            capacity: self.capacity,
            last_refill: last_update.map(|ts| {
                let duration = std::time::UNIX_EPOCH + Duration::from_secs(ts);
                Instant::now() - Duration::from_secs(
                    Instant::now().elapsed().as_secs() - ts
                )
            }),
            waiting: *self.waiting.lock().await,
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建令牌桶限流器
    let token_bucket = Arc::new(TokenBucket::new(
        10,         // 容量
        2,          // 每次补充2个令牌
        Duration::from_secs(1), // 每秒补充
    ));
    
    // 创建固定窗口限流器
    let fixed_window = Arc::new(FixedWindowRateLimiter::new(
        5,          // 5秒内
        10,         // 最多10个请求
    ));
    
    // 创建滑动窗口限流器
    let sliding_window = Arc::new(SlidingWindowRateLimiter::new(
        Duration::from_secs(5), // 5秒窗口
        10,         // 最多10个请求
    ));
    
    // 创建分布式限流器
    let storage = Arc::new(InMemoryStorage::new());
    let distributed = Arc::new(DistributedRateLimiter::new(
        storage,
        "api:requests",
        20,         // 容量
        4,          // 每次补充4个令牌
        Duration::from_secs(1), // 每秒补充
    ));
    
    // 测试令牌桶限流器
    println!("=== 测试令牌桶限流器 ===");
    test_rate_limiter(token_bucket.clone(), 15).await;
    
    // 测试固定窗口限流器
    println!("\n=== 测试固定窗口限流器 ===");
    test_rate_limiter(fixed_window.clone(), 15).await;
    
    // 测试滑动窗口限流器
    println!("\n=== 测试滑动窗口限流器 ===");
    test_rate_limiter(sliding_window.clone(), 15).await;
    
    // 测试分布式限流器
    println!("\n=== 测试分布式限流器 ===");
    test_rate_limiter(distributed.clone(), 25).await;
    
    // 测试并发请求
    println!("\n=== 测试并发请求 ===");
    test_concurrent_requests(token_bucket.clone(), 20).await;
}

async fn test_rate_limiter<R: RateLimiter>(limiter: Arc<R>, requests: usize) {
    let mut success_count = 0;
    let mut failure_count = 0;
    
    for i in 0..requests {
        println!("请求 #{} - 尝试获取令牌", i);
        
        if limiter.acquire().await {
            success_count += 1;
            println!("  成功获取令牌");
        } else {
            failure_count += 1;
            println!("  无法获取令牌");
        }
        
        // 查看限流器状态
        let status = limiter.get_status().await;
        println!("  限流器状态: 可用={}/{}, 等待={}", 
                 status.available, status.capacity, status.waiting);
        
        sleep(Duration::from_millis(200)).await;
    }
    
    println!("总结: 成功={}, 失败={}", success_count, failure_count);
}

async fn test_concurrent_requests<R: RateLimiter>(limiter: Arc<R>, requests: usize) {
    let mut handles = vec![];
    
    for i in 0..requests {
        let limiter = limiter.clone();
        
        handles.push(tokio::spawn(async move {
            println!("并发请求 #{} - 尝试获取令牌或等待", i);
            let start = Instant::now();
            
            match limiter.acquire_or_wait().await {
                Ok(()) => {
                    let elapsed = start.elapsed();
                    println!("  请求 #{} 获取到令牌，等待了 {:?}", i, elapsed);
                    true
                },
                Err(e) => {
                    println!("  请求 #{} 失败: {}", i, e);
                    false
                }
            }
        }));
    }
    
    // 等待所有请求完成
    let results = futures::future::join_all(handles).await;
    
    let success_count = results.iter()
        .filter(|r| r.as_ref().map_or(false, |v| *v))
        .count();
    
    println!("并发请求总结: 成功={}/{}", success_count, requests);
}
```

```text
<think>
我将继续编写分布式系统模式的代码示例。
下一个部分应该是稳定性模式中的断路器和重试策略模式。

接下来要写的是断路器模式，它是一种在分布式系统中防止级联失败的重要模式。
然后会继续写重试策略，这也是提高系统稳定性的关键模式。
</think>
```

### 8.2 断路器

断路器模式用于防止系统持续调用可能失败的服务，从而防止级联失败和系统崩溃。

```rust
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use std::collections::{HashMap, VecDeque};
use async_trait::async_trait;

// 断路器状态
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum CircuitState {
    Closed,     // 正常状态，允许请求通过
    Open,       // 断开状态，快速失败
    HalfOpen,   // 半开状态，允许有限请求通过以测试服务
}

// 断路器配置
#[derive(Debug, Clone)]
struct CircuitBreakerConfig {
    // 错误阈值百分比，超过此值断路器将打开
    failure_threshold_percentage: f64,
    // 请求阈值，只有超过此值才会考虑打开断路器
    request_volume_threshold: u32,
    // 断路器打开后，等待多长时间转为半开状态
    sleep_window_ms: u64,
    // 半开状态下允许通过的请求数
    half_open_allowed_calls: u32,
    // 统计窗口大小
    metrics_window_size: usize,
}

impl Default for CircuitBreakerConfig {
    fn default() -> Self {
        Self {
            failure_threshold_percentage: 50.0, // 50%失败率
            request_volume_threshold: 20,       // 至少20个请求
            sleep_window_ms: 5000,              // 5秒后尝试恢复
            half_open_allowed_calls: 10,        // 半开状态允许10个请求
            metrics_window_size: 100,           // 保留最近100个请求的统计
        }
    }
}

// 断路器事件类型
#[derive(Debug, Clone, Copy)]
enum CircuitEvent {
    Success,
    Failure,
    Timeout,
    ShortCircuited, // 被断路器短路
    Rejected,       // 因其他原因被拒绝
}

// 断路器指标
struct CircuitMetrics {
    // 请求结果历史
    events: VecDeque<CircuitEvent>,
    // 窗口大小
    window_size: usize,
    // 半开状态计数器
    half_open_counter: u32,
}

impl CircuitMetrics {
    fn new(window_size: usize) -> Self {
        Self {
            events: VecDeque::with_capacity(window_size),
            window_size,
            half_open_counter: 0,
        }
    }
    
    // 记录事件
    fn record(&mut self, event: CircuitEvent) {
        if self.events.len() >= self.window_size {
            self.events.pop_front();
        }
        self.events.push_back(event);
    }
    
    // 获取错误率
    fn error_rate(&self) -> Option<f64> {
        if self.events.is_empty() {
            return None;
        }
        
        let total = self.events.len() as f64;
        let failures = self.events.iter()
            .filter(|&&e| matches!(e, CircuitEvent::Failure | CircuitEvent::Timeout))
            .count() as f64;
        
        Some((failures / total) * 100.0)
    }
    
    // 获取请求总数
    fn request_count(&self) -> usize {
        self.events.len()
    }
    
    // 重置半开状态计数器
    fn reset_half_open_counter(&mut self) {
        self.half_open_counter = 0;
    }
    
    // 增加半开状态计数器
    fn increment_half_open_counter(&mut self) -> u32 {
        self.half_open_counter += 1;
        self.half_open_counter
    }
    
    // 清空统计数据
    fn reset(&mut self) {
        self.events.clear();
        self.half_open_counter = 0;
    }
}

// 断路器核心
struct CircuitBreaker {
    name: String,
    state: RwLock<CircuitState>,
    last_state_change: RwLock<Instant>,
    metrics: RwLock<CircuitMetrics>,
    config: CircuitBreakerConfig,
}

impl CircuitBreaker {
    fn new(name: &str, config: CircuitBreakerConfig) -> Self {
        Self {
            name: name.to_string(),
            state: RwLock::new(CircuitState::Closed),
            last_state_change: RwLock::new(Instant::now()),
            metrics: RwLock::new(CircuitMetrics::new(config.metrics_window_size)),
            config,
        }
    }
    
    // 获取当前状态
    async fn current_state(&self) -> CircuitState {
        *self.state.read().await
    }
    
    // 检查是否应该转为开路状态
    async fn should_trip(&self) -> bool {
        let metrics = self.metrics.read().await;
        
        // 请求量过低，不触发断路
        if metrics.request_count() < self.config.request_volume_threshold as usize {
            return false;
        }
        
        // 检查错误率
        if let Some(error_rate) = metrics.error_rate() {
            error_rate >= self.config.failure_threshold_percentage
        } else {
            false
        }
    }
    
    // 检查是否应该从开路恢复到半开路状态
    async fn should_attempt_reset(&self) -> bool {
        let last_change = *self.last_state_change.read().await;
        let sleep_window = Duration::from_millis(self.config.sleep_window_ms);
        
        Instant::now().duration_since(last_change) >= sleep_window
    }
    
    // 转为开路状态
    async fn trip_open(&self) {
        let mut state = self.state.write().await;
        if *state == CircuitState::Closed {
            println!("[{}] 断路器状态切换: 关闭 -> 打开", self.name);
            *state = CircuitState::Open;
            
            let mut last_change = self.last_state_change.write().await;
            *last_change = Instant::now();
        }
    }
    
    // 转为半开路状态
    async fn half_open(&self) {
        let mut state = self.state.write().await;
        if *state == CircuitState::Open {
            println!("[{}] 断路器状态切换: 打开 -> 半开", self.name);
            *state = CircuitState::HalfOpen;
            
            let mut last_change = self.last_state_change.write().await;
            *last_change = Instant::now();
            
            let mut metrics = self.metrics.write().await;
            metrics.reset_half_open_counter();
        }
    }
    
    // 恢复到关闭状态
    async fn reset(&self) {
        let mut state = self.state.write().await;
        if *state != CircuitState::Closed {
            println!("[{}] 断路器状态切换: {} -> 关闭", 
                    self.name, 
                    if *state == CircuitState::Open { "打开" } else { "半开" });
            
            *state = CircuitState::Closed;
            
            let mut last_change = self.last_state_change.write().await;
            *last_change = Instant::now();
            
            let mut metrics = self.metrics.write().await;
            metrics.reset();
        }
    }
    
    // 尝试允许请求通过
    async fn allow_request(&self) -> bool {
        match self.current_state().await {
            CircuitState::Closed => true,
            CircuitState::Open => {
                // 检查是否应该尝试恢复
                if self.should_attempt_reset().await {
                    self.half_open().await;
                    // 恢复到半开状态后，允许有限的请求通过
                    return self.allow_request().await;
                }
                
                // 开路状态下拒绝请求
                let mut metrics = self.metrics.write().await;
                metrics.record(CircuitEvent::ShortCircuited);
                false
            },
            CircuitState::HalfOpen => {

/*
<think>
I'll continue the CircuitBreaker implementation, covering the rest of the `allow_request` method for the HalfOpen state, and then complete the implementation with the execution method, along with some example code to demonstrate how to use it.
</think>
*/

                // 半开状态下允许有限请求通过
                let mut metrics = self.metrics.write().await;
                let counter = metrics.increment_half_open_counter();
                
                counter <= self.config.half_open_allowed_calls
            }
        }
    }
    
    // 记录成功
    async fn mark_success(&self) {
        let mut metrics = self.metrics.write().await;
        metrics.record(CircuitEvent::Success);
        
        // 如果处于半开状态且成功次数达到阈值，则关闭断路器
        if self.current_state().await == CircuitState::HalfOpen {
            let half_open_counter = metrics.half_open_counter;
            
            // 释放metrics锁，避免死锁
            drop(metrics);
            
            if half_open_counter >= self.config.half_open_allowed_calls {
                self.reset().await;
            }
        }
    }
    
    // 记录失败
    async fn mark_failure(&self) {
        let mut metrics = self.metrics.write().await;
        metrics.record(CircuitEvent::Failure);
        
        // 如果处于半开状态，立即重新打开断路器
        if self.current_state().await == CircuitState::HalfOpen {
            // 释放metrics锁，避免死锁
            drop(metrics);
            
            self.trip_open().await;
            return;
        }
        
        // 检查是否应该打开断路器
        if self.should_trip().await {
            self.trip_open().await;
        }
    }
    
    // 记录超时
    async fn mark_timeout(&self) {
        let mut metrics = self.metrics.write().await;
        metrics.record(CircuitEvent::Timeout);
        
        // 超时通常比普通错误更严重，所以直接触发断路逻辑
        if self.current_state().await == CircuitState::HalfOpen {
            // 释放metrics锁，避免死锁
            drop(metrics);
            
            self.trip_open().await;
            return;
        }
        
        // 检查是否应该打开断路器
        if self.should_trip().await {
            self.trip_open().await;
        }
    }
    
    // 执行受保护的操作
    async fn execute<F, Fut, T>(&self, f: F) -> Result<T, String>
    where
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = Result<T, String>>,
    {
        // 检查断路器是否允许请求
        if !self.allow_request().await {
            return Err(format!("[{}] 断路器开路，请求被拒绝", self.name));
        }
        
        // 执行操作
        match f().await {
            Ok(result) => {
                self.mark_success().await;
                Ok(result)
            },
            Err(e) => {
                self.mark_failure().await;
                Err(format!("[{}] 操作失败: {}", self.name, e))
            }
        }
    }
    
    // 带超时的执行
    async fn execute_with_timeout<F, Fut, T>(&self, timeout: Duration, f: F) -> Result<T, String>
    where
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = Result<T, String>>,
    {
        // 检查断路器是否允许请求
        if !self.allow_request().await {
            return Err(format!("[{}] 断路器开路，请求被拒绝", self.name));
        }
        
        // 使用tokio的超时功能
        match tokio::time::timeout(timeout, f()).await {
            Ok(Ok(result)) => {
                self.mark_success().await;
                Ok(result)
            },
            Ok(Err(e)) => {
                self.mark_failure().await;
                Err(format!("[{}] 操作失败: {}", self.name, e))
            },
            Err(_) => {
                self.mark_timeout().await;
                Err(format!("[{}] 操作超时", self.name))
            }
        }
    }
    
    // 获取断路器状态信息
    async fn get_metrics(&self) -> CircuitBreakerMetrics {
        let state = self.current_state().await;
        let metrics = self.metrics.read().await;
        let last_change = *self.last_state_change.read().await;
        
        CircuitBreakerMetrics {
            name: self.name.clone(),
            state,
            request_count: metrics.request_count() as u32,
            error_rate: metrics.error_rate().unwrap_or(0.0),
            time_in_current_state: Instant::now().duration_since(last_change),
        }
    }
}

// 断路器指标报告结构
#[derive(Debug, Clone)]
struct CircuitBreakerMetrics {
    name: String,
    state: CircuitState,
    request_count: u32,
    error_rate: f64,
    time_in_current_state: Duration,
}

// 断路器注册表
struct CircuitBreakerRegistry {
    breakers: RwLock<HashMap<String, Arc<CircuitBreaker>>>,
}

impl CircuitBreakerRegistry {
    fn new() -> Self {
        Self {
            breakers: RwLock::new(HashMap::new()),
        }
    }
    
    // 获取或创建断路器
    async fn get_or_create(&self, name: &str, config: Option<CircuitBreakerConfig>) -> Arc<CircuitBreaker> {
        let mut breakers = self.breakers.write().await;
        
        if let Some(breaker) = breakers.get(name) {
            breaker.clone()
        } else {
            let config = config.unwrap_or_default();
            let breaker = Arc::new(CircuitBreaker::new(name, config));
            breakers.insert(name.to_string(), breaker.clone());
            breaker
        }
    }
    
    // 获取所有断路器
    async fn get_all(&self) -> Vec<Arc<CircuitBreaker>> {
        let breakers = self.breakers.read().await;
        breakers.values().cloned().collect()
    }
    
    // 获取所有断路器的指标
    async fn get_all_metrics(&self) -> Vec<CircuitBreakerMetrics> {
        let breakers = self.get_all().await;
        let mut metrics = Vec::with_capacity(breakers.len());
        
        for breaker in breakers {
            metrics.push(breaker.get_metrics().await);
        }
        
        metrics
    }
}

// 示例不可靠服务
struct UnreliableService {
    failure_rate: f64,
    delay_ms: u64,
}

impl UnreliableService {
    // 模拟调用服务，可能失败或延迟
    async fn call(&self) -> Result<String, String> {
        // 随机决定是否失败
        let roll = rand::random::<f64>();
        if roll < self.failure_rate {
            return Err("服务调用失败".to_string());
        }
        
        // 引入延迟
        tokio::time::sleep(Duration::from_millis(self.delay_ms)).await;
        
        Ok("服务调用成功".to_string())
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建断路器注册表
    let registry = Arc::new(CircuitBreakerRegistry::new());
    
    // 创建一个高失败率的服务
    let unstable_service = UnreliableService {
        failure_rate: 0.7,  // 70%失败率
        delay_ms: 100,      // 100ms延迟
    };
    
    // 创建一个超时服务
    let slow_service = UnreliableService {
        failure_rate: 0.2,  // 20%失败率
        delay_ms: 500,      // 500ms延迟
    };
    
    // 获取断路器
    let unstable_breaker = registry.get_or_create(
        "unstable-service", 
        Some(CircuitBreakerConfig {
            failure_threshold_percentage: 50.0,
            request_volume_threshold: 5,
            sleep_window_ms: 3000,
            half_open_allowed_calls: 3,
            metrics_window_size: 10,
        })
    ).await;
    
    let slow_breaker = registry.get_or_create(
        "slow-service", 
        Some(CircuitBreakerConfig {
            failure_threshold_percentage: 50.0,
            request_volume_threshold: 5,
            sleep_window_ms: 3000,
            half_open_allowed_calls: 3,
            metrics_window_size: 10,
        })
    ).await;
    
    // 测试不稳定服务
    println!("=== 测试不稳定服务 ===");
    test_service(unstable_service, unstable_breaker.clone(), 15).await;
    
    // 等待休眠窗口过去
    println!("\n等待断路器半开...");
    tokio::time::sleep(Duration::from_millis(3500)).await;
    
    // 断路器应该处于半开状态，再次测试
    println!("\n=== 测试恢复期间的不稳定服务 ===");
    test_service(UnreliableService { failure_rate: 0.0, delay_ms: 100 }, unstable_breaker.clone(), 5).await;
    
    // 测试超时服务
    println!("\n=== 测试超时服务 ===");
    test_service_with_timeout(slow_service, slow_breaker.clone(), 15, Duration::from_millis(200)).await;
    
    // 显示所有断路器指标
    println!("\n=== 断路器指标 ===");
    let all_metrics = registry.get_all_metrics().await;
    for metric in all_metrics {
        println!("断路器 '{}': 状态={:?}, 请求数={}, 错误率={:.1}%, 当前状态持续时间={:?}",
                 metric.name, metric.state, metric.request_count, metric.error_rate,
                 metric.time_in_current_state);
    }
}

async fn test_service(service: UnreliableService, breaker: Arc<CircuitBreaker>, iterations: usize) {
    for i in 0..iterations {
        println!("请求 #{}", i + 1);
        
        let service_clone = service.clone();
        let result = breaker.execute(|| async move {
            service_clone.call().await
        }).await;
        
        match result {
            Ok(msg) => println!("  成功: {}", msg),
            Err(e) => println!("  失败: {}", e),
        }
        
        let metrics = breaker.get_metrics().await;
        println!("  断路器状态: {:?}, 请求数: {}, 错误率: {:.1}%",
                 metrics.state, metrics.request_count, metrics.error_rate);
        
        // 小暂停，让输出更易于阅读
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
}

async fn test_service_with_timeout(
    service: UnreliableService, 
    breaker: Arc<CircuitBreaker>,
    iterations: usize,
    timeout: Duration
) {
    for i in 0..iterations {
        println!("请求 #{} (超时设置: {:?})", i + 1, timeout);
        
        let service_clone = service.clone();
        let result = breaker.execute_with_timeout(timeout, || async move {
            service_clone.call().await
        }).await;
        
        match result {
            Ok(msg) => println!("  成功: {}", msg),
            Err(e) => println!("  失败: {}", e),
        }
        
        let metrics = breaker.get_metrics().await;
        println!("  断路器状态: {:?}, 请求数: {}, 错误率: {:.1}%",
                 metrics.state, metrics.request_count, metrics.error_rate);
        
        // 小暂停，让输出更易于阅读
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
}

impl Clone for UnreliableService {
    fn clone(&self) -> Self {
        Self {
            failure_rate: self.failure_rate,
            delay_ms: self.delay_ms,
        }
    }
}
```

### 8.3 隔板模式

隔板模式通过隔离不同的组件，防止故障在系统中蔓延，类似于船舱隔离舱的设计。

```rust
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock, Semaphore};
use tokio::time::timeout;
use futures::future::join_all;
use rand::Rng;

// 隔板配置
#[derive(Debug, Clone)]
struct BulkheadConfig {
    // 最大并发请求数
    max_concurrent_calls: usize,
    // 最大等待队列长度
    max_queue_size: usize,
    // 请求超时时间
    call_timeout: Duration,
}

impl Default for BulkheadConfig {
    fn default() -> Self {
        Self {
            max_concurrent_calls: 10,
            max_queue_size: 20,
            call_timeout: Duration::from_secs(1),
        }
    }
}

// 隔板执行统计
#[derive(Debug, Clone, Default)]
struct BulkheadMetrics {
    // 总请求数
    total_calls: u64,
    // 成功请求数
    successful_calls: u64,
    // 失败请求数
    failed_calls: u64,
    // 拒绝请求数（因隔板满）
    rejected_calls: u64,
    // 超时请求数
    timed_out_calls: u64,
    // 当前活跃请求数
    active_calls: usize,
    // 等待队列长度
    queue_size: usize,
}

// 隔板实现
struct Bulkhead {
    name: String,
    config: BulkheadConfig,
    semaphore: Arc<Semaphore>,
    metrics: RwLock<BulkheadMetrics>,
    queue: Mutex<tokio::sync::mpsc::Sender<()>>,
    _queue_receiver: Mutex<tokio::sync::mpsc::Receiver<()>>,
}

impl Bulkhead {
    fn new(name: &str, config: BulkheadConfig) -> Self {
        let semaphore = Arc::new(Semaphore::new(config.max_concurrent_calls));
        
        // 创建队列通道
        let (tx, rx) = tokio::sync::mpsc::channel(config.max_queue_size);
        
        Self {
            name: name.to_string(),
            config,
            semaphore,
            metrics: RwLock::new(BulkheadMetrics::default()),
            queue: Mutex::new(tx),
            _queue_receiver: Mutex::new(rx),
        }
    }
    
    // 执行受保护的操作
    async fn execute<F, Fut, T>(&self, f: F) -> Result<T, String>
    where
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = Result<T, String>>,
    {
        // 尝试获取队列位置
        let queue_permit = {
            let queue = self.queue.lock().await;
            match queue.try_send(()) {
                Ok(()) => true,
                Err(_) => {
                    // 队列已满，拒绝请求
                    let mut metrics = self.metrics.write().await;
                    metrics.total_calls += 1;
                    metrics.rejected_calls += 1;
                    return Err(format!("[{}] 隔板队列已满，请求被拒绝", self.name));
                }
            }
        };
        
        // 更新队列大小指标
        {
            let mut metrics = self.metrics.write().await;
            metrics.queue_size += 1;
        }
        
        // 尝试获取信号量
        let permit = match self.semaphore.acquire().await {
            Ok(permit) => {
                // 成功获取信号量，可以执行
                // 从队列中移除
                {
                    let mut metrics = self.metrics.write().await;
                    metrics.queue_size -= 1;
                    metrics.active_calls += 1;
                }
                permit
            },
            Err(_) => {
                // 无法获取信号量，这不应该发生，但以防万一
                let mut metrics = self.metrics.write().await;
                metrics.total_calls += 1;
                metrics.rejected_calls += 1;
                metrics.queue_size -= 1;
                
                return Err(format!("[{}] 无法获取隔板许可", self.name));
            }
        };
        
        // 更新指标并执行操作
        let start = Instant::now();
        let result = match timeout(self.config.call_timeout, f()).await {
            Ok(Ok(result)) => {
                // 操作成功
                let mut metrics = self.metrics.write().await;
                metrics.total_calls += 1;
                metrics.successful_calls += 1;
                
                Ok(result)
            },
            Ok(Err(e)) => {
                // 操作失败
                let mut metrics = self.metrics.write().await;
                metrics.total_calls += 1;
                metrics.failed_calls += 1;
                
                Err(format!("[{}] 操作失败: {}", self.name, e))
            },
            Err(_) => {
                // 操作超时
                let mut metrics = self.metrics.write().await;
                metrics.total_calls += 1;
                metrics.timed_out_calls += 1;
                
                Err(format!("[{}] 操作超时", self.name))
            }
        };
        
        // 减少活跃计数并释放信号量
        {
            let mut metrics = self.metrics.write().await;
            metrics.active_calls -= 1;
        }
        
        // 释放信号量
        drop(permit);
        
        // 释放队列位置
        if queue_permit {
            // 从队列通道接收消息
            let queue = self.queue.lock().await;
            let _ = queue.try_send(());
            let _ = queue.try_recv();
        }
        
        result
    }
    
    // 获取隔板指标
    async fn get_metrics(&self) -> BulkheadMetrics {
        self.metrics.read().await.clone()
    }
}

// 隔板注册表
struct BulkheadRegistry {
    bulkheads: RwLock<HashMap<String, Arc<Bulkhead>>>,
}

impl BulkheadRegistry {
    fn new() -> Self {
        Self {
            bulkheads: RwLock::new(HashMap::new()),
        }
    }
    
    // 获取或创建隔板
    async fn get_or_create(&self, name: &str, config: Option<BulkheadConfig>) -> Arc<Bulkhead> {
        let mut bulkheads = self.bulkheads.write().await;
        
        if let Some(bulkhead) = bulkheads.get(name) {
            bulkhead.clone()
        } else {
            let config = config.unwrap_or_default();
            let bulkhead = Arc::new(Bulkhead::new(name, config));
            bulkheads.insert(name.to_string(), bulkhead.clone());
            bulkhead
        }
    }
    
    // 获取所有隔板
    async fn get_all(&self) -> Vec<Arc<Bulkhead>> {
        let bulkheads = self.bulkheads.read().await;
        bulkheads.values().cloned().collect()
    }
    
    // 获取所有隔板的指标
    async fn get_all_metrics(&self) -> HashMap<String, BulkheadMetrics> {
        let bulkheads = self.get_all().await;
        let mut metrics = HashMap::new();
        
        for bulkhead in bulkheads {
            metrics.insert(bulkhead.name.clone(), bulkhead.get_metrics().await);
        }
        
        metrics
    }
}

// 示例服务 - 模拟不同的处理延迟
struct ExampleService {
    processing_time_ms: u64,
    failure_rate: f64,
}

impl ExampleService {
    async fn call(&self) -> Result<String, String> {
        // 随机决定是否失败
        let mut rng = rand::thread_rng();
        if rng.gen::<f64>() < self.failure_rate {
            return Err("服务调用失败".to_string());
        }
        
        // 模拟处理时间
        tokio::time::sleep(Duration::from_millis(self.processing_time_ms)).await;
        
        Ok(format!("服务调用成功 (延迟: {}ms)", self.processing_time_ms))
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建隔板注册表
    let registry = Arc::new(BulkheadRegistry::new());
    
    // 定义不同的隔板配置
    let database_config = BulkheadConfig {
        max_concurrent_calls: 5,
        max_queue_size: 10,
        call_timeout: Duration::from_millis(500),
    };
    
    let api_config = BulkheadConfig {
        max_concurrent_calls: 20,
        max_queue_size: 50,
        call_timeout: Duration::from_millis(1000),
    };
    
    // 获取隔板
    let db_bulkhead = registry.get_or_create("database", Some(database_config)).await;
    let api_bulkhead = registry.get_or_create("api", Some(api_config)).await;
    
    // 创建服务
    let fast_db = ExampleService {
        processing_time_ms: 100,
        failure_rate: 0.1,
    };
    
    let slow_db = ExampleService {
        processing_time_ms: 400,
        failure_rate: 0.2,
    };
    
    let api_service = ExampleService {
        processing_time_ms: 200,
        failure_rate: 0.05,
    };
    
    // 测试序列调用
    println!("=== 测试序列调用 ===");
    
    // 调用数据库服务 (快速)
    for i in 0..5 {
        println!("DB请求 #{}:", i + 1);
        let result = db_bulkhead.execute(|| async {
            fast_db.call().await
        }).await;
        
        match result {
            Ok(msg) => println!("  成功: {}", msg),
            Err(e) => println!("  失败: {}", e),
        }
    }
    
    // 获取指标
    let db_metrics = db_bulkhead.get_metrics().await;
    println!("\n数据库隔板指标: {:#?}", db_metrics);
    
    // 测试并发调用
    println!("\n=== 测试并发调用 ===");
    
    // 创建多个并发请求
    let mut db_handles = Vec::new();
    let mut api_handles = Vec::new();
    
    // 数据库请求 (多于允许的并发数)
    for i in 0..15 {
        let bulkhead = db_bulkhead.clone();
        let service = if i % 2 == 0 { 
            ExampleService { processing_time_ms: 400, failure_rate: 0.2 } 
        } else { 
            ExampleService { processing_time_ms: 100, failure_rate: 0.1 } 
        };
        
        let handle = tokio::spawn(async move {
            let result = bulkhead.execute(|| async {
                service.call().await
            }).await;
            
            (i, result)
        });
        
        db_handles.push(handle);
    }
    
    // API请求
    for i in 0..30 {
        let bulkhead = api_bulkhead.clone();
        let service = api_service.clone();
        
        let handle = tokio::spawn(async move {
            let result = bulkhead.execute(|| async {
                service.call().await
            }).await;
            
            (i, result)
        });
        
        api_handles.push(handle);
    }
    
    // 等待所有请求完成
    let db_results = join_all(db_handles).await;
    let api_results = join_all(api_handles).await;
    
    // 分析数据库结果
    let mut db_success = 0;
    let mut db_failure = 0;
    let mut db_rejected = 0;
    
    for result in db_results {
        match result {
            Ok((id, Ok(_))) => {
                db_success += 1;
                println!("DB请求 #{} 成功", id);
            },
            Ok((id, Err(e))) => {
                db_failure += 1;
                if e.contains("隔板队列已满") {
                    db_rejected += 1;
                    println!("DB请求 #{} 被拒绝: {}", id, e);
                } else {
                    println!("DB请求 #{} 失败: {}", id, e);
                }
            },
            Err(e) => println!("DB请求执行错误: {}", e),
        }
    }
    
    // 分析API结果
    let mut api_success = 0;
    let mut api_failure = 0;
    let mut api_rejected = 0;
    
    for result in api_results {
        match result {
            Ok((id, Ok(_))) => {
                api_success += 1;
                println!("API请求 #{} 成功", id);
            },
            Ok((id, Err(e))) => {
                api_failure += 1;
                if e.contains("隔板队列已满") {
                    api_rejected += 1;
                    println!("API请求 #{} 被拒绝: {}", id, e);
                } else {
                    println!("API请求 #{} 失败: {}", id, e);
                }
            },
            Err(e) => println!("API请求执行错误: {}", e),
        }
    }
    
    // 获取最终指标
    let all_metrics = registry.get_all_metrics().await;
    
    println!("\n=== 最终隔板指标 ===");
    for (name, metrics) in all_metrics {
        println!("隔板 '{}':", name);
        println!("  总请求: {}", metrics.total_calls);
        println!("  成功: {}", metrics.successful_calls);
        println!("  失败: {}", metrics.failed_calls);
        println!("  拒绝: {}", metrics.rejected_calls);
        println!("  超时: {}", metrics.timed_out_calls);
        println!("  当前活跃: {}", metrics.active_calls);
        println!("  当前队列: {}", metrics.queue_size);
    }
    
    println!("\nDB请求结果: 成功={}, 失败={} (其中被拒绝={})", 
             db_success, db_failure, db_rejected);
    println!("API请求结果: 成功={}, 失败={} (其中被拒绝={})", 
             api_success, api_failure, api_rejected);
}

impl Clone for ExampleService {
    fn clone(&self) -> Self {
        Self {
            processing_time_ms: self.processing_time_ms,
            failure_rate: self.failure_rate,
        }
    }
}
```

```text
<think>
I'll continue with more distributed systems patterns. 
Let's focus on some important patterns that haven't been covered yet:

1. Retry patterns - expanding on timeouts and retries with more advanced strategies
2. Leader election - important for distributed coordination
3. Distributed logging/tracing - important for observability
4. Service discovery - how services find each other
5. API Gateway/BFF - Backend for Frontend patterns

Let me start with the retry pattern in more detail, followed by leader election.
</think>
```

## 9. 分布式协调模式

### 9.1 领导者选举

领导者选举模式用于从一组节点中选择一个作为领导者，负责协调工作或管理共享资源。

```rust
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};
use tokio::time::sleep;
use uuid::Uuid;
use serde::{Serialize, Deserialize};
use async_trait::async_trait;

// 分布式锁接口
#[async_trait]
trait DistributedLock: Send + Sync {
    // 尝试获取锁，返回是否成功
    async fn try_lock(&self, holder: &str, ttl: Duration) -> Result<bool, String>;
    
    // 刷新锁的TTL
    async fn refresh(&self, holder: &str, ttl: Duration) -> Result<bool, String>;
    
    // 释放锁
    async fn unlock(&self, holder: &str) -> Result<bool, String>;
    
    // 获取当前锁持有者
    async fn get_holder(&self) -> Result<Option<String>, String>;
}

// 内存实现的分布式锁 (实际应用中应使用Redis或Zookeeper等)
struct InMemoryLock {
    name: String,
    holder: RwLock<Option<String>>,
    expiry: RwLock<Option<Instant>>,
}

impl InMemoryLock {
    fn new(name: &str) -> Self {
        Self {
            name: name.to_string(),
            holder: RwLock::new(None),
            expiry: RwLock::new(None),
        }
    }
}

#[async_trait]
impl DistributedLock for InMemoryLock {
    async fn try_lock(&self, holder: &str, ttl: Duration) -> Result<bool, String> {
        let mut current_holder = self.holder.write().await;
        let mut expiry = self.expiry.write().await;
        
        // 检查锁是否已过期
        if let Some(exp) = *expiry {
            if Instant::now() > exp {
                // 锁已过期，可以重新获取
                *current_holder = Some(holder.to_string());
                *expiry = Some(Instant::now() + ttl);
                return Ok(true);
            }
        }
        
        // 如果没有持有者或持有者就是当前请求者，则可以获取锁
        if current_holder.is_none() || current_holder.as_ref().unwrap() == holder {
            *current_holder = Some(holder.to_string());
            *expiry = Some(Instant::now() + ttl);
            Ok(true)
        } else {
            Ok(false)
        }
    }
    
    async fn refresh(&self, holder: &str, ttl: Duration) -> Result<bool, String> {
        let current_holder = self.holder.read().await;
        
        // 只有当前持有者才能刷新锁
        if let Some(h) = &*current_holder {
            if h == holder {
                let mut expiry = self.expiry.write().await;
                *expiry = Some(Instant::now() + ttl);
                return Ok(true);
            }
        }
        
        Ok(false)
    }
    
    async fn unlock(&self, holder: &str) -> Result<bool, String> {
        let mut current_holder = self.holder.write().await;
        
        // 只有当前持有者才能释放锁
        if let Some(h) = &*current_holder {
            if h == holder {
                *current_holder = None;
                let mut expiry = self.expiry.write().await;
                *expiry = None;
                return Ok(true);
            }
        }
        
        Ok(false)
    }
    
    async fn get_holder(&self) -> Result<Option<String>, String> {
        let current_holder = self.holder.read().await;
        let expiry = self.expiry.read().await;
        
        // 检查锁是否已过期
        if let Some(exp) = *expiry {
            if Instant::now() > exp {
                // 锁已过期
                return Ok(None);
            }
        }
        
        Ok(current_holder.clone())
    }
}

// 领导者选举接口
#[async_trait]
trait LeaderElection: Send + Sync {
    // 尝试成为领导者
    async fn try_acquire_leadership(&self) -> Result<bool, String>;
    
    // 释放领导权
    async fn release_leadership(&self) -> Result<bool, String>;
    
    // 检查当前节点是否是领导者
    async fn is_leader(&self) -> Result<bool, String>;
    
    // 获取当前领导者ID
    async fn get_current_leader(&self) -> Result<Option<String>, String>;
    
    // 等待成为领导者
    async fn wait_for_leadership(&self, timeout: Option<Duration>) -> Result<bool, String>;
}

// 基于分布式锁的领导者选举实现
struct LockBasedLeaderElection {
    node_id: String,
    election_lock: Arc<dyn DistributedLock>,
    lock_ttl: Duration,
    is_leader: Mutex<bool>,
    // 领导权维持任务
    leadership_task: Mutex<Option<tokio::task::JoinHandle<()>>>,
}

impl LockBasedLeaderElection {
    fn new(node_id: &str, lock: Arc<dyn DistributedLock>, ttl: Duration) -> Self {
        Self {
            node_id: node_id.to_string(),
            election_lock: lock,
            lock_ttl: ttl,
            is_leader: Mutex::new(false),
            leadership_task: Mutex::new(None),
        }
    }
    
    // 启动领导权维持任务
    async fn start_leadership_maintainer(&self) -> Result<(), String> {
        let mut task_guard = self.leadership_task.lock().await;
        
        // 停止现有任务（如果有）
        if let Some(task) = task_guard.take() {
            task.abort();
        }
        
        // 创建新任务
        let node_id = self.node_id.clone();
        let lock = self.election_lock.clone();
        let refresh_interval = self.lock_ttl / 2; // 刷新间隔为TTL的一半
        let is_leader = self.is_leader.clone();
        
        let task = tokio::spawn(async move {
            loop {
                // 尝试刷新锁
                match lock.refresh(&node_id, refresh_interval * 2).await {
                    Ok(true) => {
                        // 刷新成功，继续保持领导权
                        println!("节点 {} 继续保持领导权", node_id);
                    },
                    Ok(false) => {
                        // 无法刷新，失去领导权
                        println!("节点 {} 失去领导权 (无法刷新)", node_id);
                        let mut leader_status = is_leader.lock().await;
                        *leader_status = false;
                        break;
                    },
                    Err(e) => {
                        // 刷新出错
                        println!("节点 {} 刷新领导权时出错: {}", node_id, e);
                        let mut leader_status = is_leader.lock().await;
                        *leader_status = false;
                        break;
                    }
                }
                
                // 等待刷新间隔
                sleep(refresh_interval).await;
            }
        });
        
        *task_guard = Some(task);
        Ok(())
    }
}

#[async_trait]
impl LeaderElection for LockBasedLeaderElection {
    async fn try_acquire_leadership(&self) -> Result<bool, String> {
        // 尝试获取锁
        match self.election_lock.try_lock(&self.node_id, self.lock_ttl).await {
            Ok(true) => {
                // 获取锁成功，成为领导者
                println!("节点 {} 成为领导者", self.node_id);
                let mut is_leader = self.is_leader.lock().await;
                *is_leader = true;
                
                // 启动领导权维持任务
                drop(is_leader); // 释放锁，避免死锁
                self.start_leadership_maintainer().await?;
                
                Ok(true)
            },
            Ok(false) => {
                // 获取锁失败，不是领导者
                let mut is_leader = self.is_leader.lock().await;
                *is_leader = false;
                Ok(false)
            },
            Err(e) => Err(format!("尝试获取领导权时出错: {}", e)),
        }
    }
    
    async fn release_leadership(&self) -> Result<bool, String> {
        // 停止领导权维持任务
        let mut task_guard = self.leadership_task.lock().await;
        if let Some(task) = task_guard.take() {
            task.abort();
        }
        
        // 释放锁
        match self.election_lock.unlock(&self.node_id).await {
            Ok(true) => {
                // 释放锁成功
                println!("节点 {} 释放领导权", self.node_id);
                let mut is_leader = self.is_leader.lock().await;
                *is_leader = false;
                Ok(true)
            },
            Ok(false) => {
                // 无法释放锁（可能已不是领导者）
                let mut is_leader = self.is_leader.lock().await;
                *is_leader = false;
                Ok(false)
            },
            Err(e) => Err(format!("释放领导权时出错: {}", e)),
        }
    }
    
    async fn is_leader(&self) -> Result<bool, String> {
        let is_leader = self.is_leader.lock().await;
        Ok(*is_leader)
    }
    
    async fn get_current_leader(&self) -> Result<Option<String>, String> {
        self.election_lock.get_holder().await
    }
    
    async fn wait_for_leadership(&self, timeout: Option<Duration>) -> Result<bool, String> {
        let start = Instant::now();
        
        loop {
            // 检查是否已超时
            if let Some(timeout_duration) = timeout {
                if start.elapsed() >= timeout_duration {
                    return Ok(false);
                }
            }
            
            // 尝试获取领导权
            if self.try_acquire_leadership().await? {
                return Ok(true);
            }
            
            // 等待一段时间后重试
            sleep(Duration::from_millis(500)).await;
        }
    }
}

// 领导者选举管理器
struct LeaderElectionManager {
    elections: RwLock<HashMap<String, Arc<dyn LeaderElection>>>,
}

impl LeaderElectionManager {
    fn new() -> Self {
        Self {
            elections: RwLock::new(HashMap::new()),
        }
    }
    
    // 创建新的选举实例
    async fn create_election(&self, group: &str, node_id: &str) -> Arc<dyn LeaderElection> {
        let mut elections = self.elections.write().await;
        
        let lock_name = format!("election-lock-{}", group);
        let lock = Arc::new(InMemoryLock::new(&lock_name));
        
        let election = Arc::new(LockBasedLeaderElection::new(
            node_id,
            lock,
            Duration::from_secs(5), // 5秒TTL
        ));
        
        elections.insert(group.to_string(), election.clone());
        election
    }
    
    // 获取现有选举实例
    async fn get_election(&self, group: &str) -> Option<Arc<dyn LeaderElection>> {
        let elections = self.elections.read().await;
        elections.get(group).cloned()
    }
}

// 模拟分布式节点
struct DistributedNode {
    id: String,
    election: Arc<dyn LeaderElection>,
    task: Mutex<Option<tokio::task::JoinHandle<()>>>,
}

impl DistributedNode {
    fn new(id: &str, election: Arc<dyn LeaderElection>) -> Self {
        Self {
            id: id.to_string(),
            election,
            task: Mutex::new(None),
        }
    }
    
    // 启动节点
    async fn start(&self) {
        let mut task_guard = self.task.lock().await;
        
        // 停止现有任务（如果有）
        if let Some(task) = task_guard.take() {
            task.abort();
        }
        
        // 创建新任务
        let id = self.id.clone();
        let election = self.election.clone();
        
        *task_guard = Some(tokio::spawn(async move {
            loop {
                // 尝试获取领导权
                match election.try_acquire_leadership().await {
                    Ok(true) => {
                        // 成为领导者，执行领导者工作
                        println!("节点 {} 成为领导者，开始工作", id);
                        Self::do_leader_work(&id).await;
                    },
                    Ok(false) => {
                        // 不是领导者，执行从节点工作
                        match election.get_current_leader().await {
                            Ok(Some(leader)) => {
                                println!("节点 {} 是从节点，当前领导者是 {}", id, leader);
                            },
                            _ => {
                                println!("节点 {} 是从节点，当前没有领导者", id);
                            }
                        }
                        Self::do_follower_work(&id).await;
                    },
                    Err(e) => {
                        println!("节点 {} 尝试获取领导权时出错: {}", id, e);
                    }
                }
                
                // 等待一段时间后重试
                sleep(Duration::from_secs(2)).await;
            }
        }));
    }
    
    // 停止节点
    async fn stop(&self) {
        let mut task_guard = self.task.lock().await;
        if let Some(task) = task_guard.take() {
            println!("停止节点 {}", self.id);
            task.abort();
        }
        
        // 如果是领导者，释放领导权
        if let Ok(true) = self.election.is_leader().await {
            match self.election.release_leadership().await {
                Ok(true) => println!("节点 {} 释放领导权", self.id),
                Ok(false) => println!("节点 {} 无法释放领导权", self.id),
                Err(e) => println!("节点 {} 释放领导权时出错: {}", self.id, e),
            }
        }
    }
    
    // 领导者工作
    async fn do_leader_work(id: &str) {
        println!("领导者 {} 执行工作...", id);
        // 模拟一些领导者特有的工作
        sleep(Duration::from_millis(500)).await;
    }
    
    // 从节点工作
    async fn do_follower_work(id: &str) {
        println!("从节点 {} 执行工作...", id);
        // 模拟一些从节点特有的工作
        sleep(Duration::from_millis(300)).await;
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    println!("=== 领导者选举示例 ===");
    
    // 创建领导者选举管理器
    let manager = Arc::new(LeaderElectionManager::new());
    
    // 创建选举实例
    let election1 = manager.create_election("cluster1", "node1").await;
    let election2 = manager.create_election("cluster1", "node2").await;
    let election3 = manager.create_election("cluster1", "node3").await;
    
    // 创建分布式节点
    let node1 = Arc::new(DistributedNode::new("node1", election1));
    let node2 = Arc::new(DistributedNode::new("node2", election2));
    let node3 = Arc::new(DistributedNode::new("node3", election3));
    
    // 启动所有节点
    println!("\n启动所有节点");
    node1.start().await;
    node2.start().await;
    node3.start().await;
    
    // 等待一段时间，让节点稳定运行
    println!("\n节点运行中...");
    sleep(Duration::from_secs(10)).await;
    
    // 模拟领导者节点故障
    println!("\n模拟领导者节点故障");
    let leader_id = manager.get_election("cluster1").await.unwrap().get_current_leader().await.unwrap();
    
    if let Some(leader) = leader_id {
        if leader == "node1" {
            println!("停止领导者节点 node1");
            node1.stop().await;
        } else if leader == "node2" {
            println!("停止领导者节点 node2");
            node2.stop().await;
        } else if leader == "node3" {
            println!("停止领导者节点 node3");
            node3.stop().await;
        }
    }
    
    // 等待新的领导者被选出
    println!("\n等待新的领导者选举...");
    sleep(Duration::from_secs(10)).await;
    
    // 显示最终状态
    let final_leader = manager.get_election("cluster1").await.unwrap().get_current_leader().await.unwrap();
    println!("\n最终领导者: {:?}", final_leader);
    
    // 停止所有节点
    println!("\n停止所有节点");
    node1.stop().await;
    node2.stop().await;
    node3.stop().await;
}
```

### 9.2 分布式锁

分布式锁用于在分布式系统中提供互斥访问资源的机制，确保在任何时刻只有一个节点可以访问共享资源。

```rust
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};
use tokio::time::sleep;
use uuid::Uuid;
use rand::Rng;
use futures::future::join_all;

// 分布式锁接口
#[async_trait::async_trait]
trait DistributedLock: Send + Sync {
    // 尝试获取锁
    async fn try_lock(&self, owner: &str, ttl: Duration) -> Result<bool, String>;
    
    // 阻塞式获取锁
    async fn lock(&self, owner: &str, ttl: Duration, timeout: Option<Duration>) -> Result<bool, String>;
    
    // 释放锁
    async fn unlock(&self, owner: &str) -> Result<bool, String>;
    
    // 刷新锁的TTL
    async fn refresh(&self, owner: &str, ttl: Duration) -> Result<bool, String>;
    
    // 获取锁的当前持有者
    async fn get_owner(&self) -> Result<Option<String>, String>;
    
    // 锁是否被持有
    async fn is_locked(&self) -> Result<bool, String>;
}

// Redis分布式锁实现（模拟版）
struct RedisLock {
    key: String,
    owner: RwLock<Option<String>>,
    expiry: RwLock<Option<Instant>>,
    waiting: Mutex<Vec<tokio::sync::oneshot::Sender<()>>>,
}

impl RedisLock {
    fn new(resource: &str) -> Self {
        let key = format!("lock:{}", resource);
        Self {
            key,
            owner: RwLock::new(None),
            expiry: RwLock::new(None),
            waiting: Mutex::new(Vec::new()),
        }
    }
    
    // 通知等待的请求
    async fn notify_waiters(&self) {
        let mut waiting = self.waiting.lock().await;
        
        // 通知所有等待的请求
        let waiters = std::mem::take(&mut *waiting);
        for waiter in waiters {
            let _ = waiter.send(());
        }
    }
}

#[async_trait::async_trait]
impl DistributedLock for RedisLock {
    async fn try_lock(&self, owner: &str, ttl: Duration) -> Result<bool, String> {
        let mut current_owner = self.owner.write().await;
        let mut expiry = self.expiry.write().await;
        
        // 检查锁是否已过期
        if let Some(exp) = *expiry {
            if Instant::now() > exp {
                // 锁已过期，可以重新获取
                *current_owner = Some(owner.to_string());
                *expiry = Some(Instant::now() + ttl);
                
                // 通知等待的请求
                drop(current_owner);
                drop(expiry);
                self.notify_waiters().await;
                
                return Ok(true);
            }
        }
        
        // 如果没有持有者或持有者就是当前请求者，则可以获取锁
        if current_owner.is_none() {
            *current_owner = Some(owner.to_string());
            *expiry = Some(Instant::now() + ttl);
            Ok(true)
        } else if let Some(current) = &*current_owner {
            if current == owner {
                // 刷新过期时间
                *expiry = Some(Instant::now() + ttl);
                Ok(true)
            } else {
                Ok(false)
            }
        } else {
            Ok(false)
        }
    }
    
    async fn lock(&self, owner: &str, ttl: Duration, timeout: Option<Duration>) -> Result<bool, String> {
        let start = Instant::now();
        
        // 首先尝试获取锁
        if self.try_lock(owner, ttl).await? {
            return Ok(true);
        }
        
        // 如果指定了超时时间为0，则立即返回
        if let Some(t) = timeout {
            if t.as_millis() == 0 {
                return Ok(false);
            }
        }
        
        // 创建等待通道
        let (tx, rx) = tokio::sync::oneshot::channel();
        
        loop {
            // 检查是否超时
            if let Some(t) = timeout {
                if start.elapsed() >= t {
                    return Ok(false);
                }
            }
            
            // 再次尝试获取锁
            if self.try_lock(owner, ttl).await? {
                return Ok(true);
            }
            
            // 添加到等待列表
            {
                let mut waiting = self.waiting.lock().await;
                waiting.push(tx.clone());
            }
            
            // 设置等待超时
            let wait_timeout = match timeout {
                Some(t) => {
                    let remaining = t.checked_sub(start.elapsed()).unwrap_or(Duration::from_millis(100));
                    remaining.min(Duration::from_millis(500))
                },
                None => Duration::from_millis(500),
            };
            
            // 等待通知或超时
            let _ = tokio::time::timeout(wait_timeout, rx.clone()).await;
        }
    }
    
    async fn unlock(&self, owner: &str) -> Result<bool, String> {
        let mut current_owner = self.owner.write().await;
        
        // 只有当前持有者才能释放锁
        if let Some(current) = &*current_owner {
            if current == owner {
                *current_owner = None;
                
                let mut expiry = self.expiry.write().await;
                *expiry = None;
                
                // 通知等待的请求
                drop(current_owner);
                drop(expiry);
                self.notify_waiters().await;
                
                return Ok(true);
            }
        }
        
        Ok(false)
    }
    
    async fn refresh(&self, owner: &str, ttl: Duration) -> Result<bool, String> {
        let current_owner = self.owner.read().await;
        
        // 只有当前持有者才能刷新锁
        if let Some(current) = &*current_owner {
            if current == owner {
                let mut expiry = self.expiry.write().await;
                *expiry = Some(Instant::now() + ttl);
                return Ok(true);
            }
        }
        
        Ok(false)
    }
    
    async fn get_owner(&self) -> Result<Option<String>, String> {
        let current_owner = self.owner.read().await;
        let expiry = self.expiry.read().await;
        
        // 检查锁是否已过期
        if let Some(exp) = *expiry {
            if Instant::now() > exp {
                // 锁已过期
                return Ok(None);
            }
        }
        
        Ok(current_owner.clone())
    }
    
    async fn is_locked(&self) -> Result<bool, String> {
        let current_owner = self.owner.read().await;
        let expiry = self.expiry.read().await;
        
        // 检查锁是否已过期
        if let Some(exp) = *expiry {
            if Instant::now() > exp {
                // 锁已过期
                return Ok(false);
            }
        }
        
        Ok(current_owner.is_some())
    }
}

// 分布式锁管理器
struct LockManager {
    locks: RwLock<HashMap<String, Arc<dyn DistributedLock>>>,
}

impl LockManager {
    fn new() -> Self {
        Self {
            locks: RwLock::new(HashMap::new()),
        }
    }
    
    // 获取锁实例
    async fn get_lock(&self, resource: &str) -> Arc<dyn DistributedLock> {
        let locks = self.locks.read().await;
        
        if let Some(lock) = locks.get(resource) {
            return lock.clone();
        }
        
        // 释放读锁
        drop(locks);
        
        // 获取写锁并创建新的锁实例
        let mut locks = self.locks.write().await;
        
        // 再次检查（避免竞态条件）
        if let Some(lock) = locks.get(resource) {
            return lock.clone();
        }
        
        // 创建新的锁实例
        let lock = Arc::new(RedisLock::new(resource)) as Arc<dyn DistributedLock>;
        locks.insert(resource.to_string(), lock.clone());
        lock
    }
}

// 自动释放锁的包装器
struct AutoLock {
    lock: Arc<dyn DistributedLock>,
    owner: String,
    acquired: bool,
}

impl AutoLock {
    // 创建新的自动锁
    async fn new(
        lock: Arc<dyn DistributedLock>, 
        owner: &str, 
        ttl: Duration,
        timeout: Option<Duration>,
    ) -> Result<Self, String> {
        let acquired = lock.lock(owner, ttl, timeout).await?;
        
        Ok(Self {
            lock,
            owner: owner.to_string(),
            acquired,
        })
    }
    
    // 检查是否获取到锁
    fn is_acquired(&self) -> bool {
        self.acquired
    }
    
    // 手动释放锁
    async fn release(&mut self) -> Result<bool, String> {
        if self.acquired {
            let result = self.lock.unlock(&self.owner).await?;
            self.acquired = false;
            Ok(result)
        } else {
            Ok(false)
        }
    }
}

impl Drop for AutoLock {
    fn drop(&mut self) {
        if self.acquired {
            // 创建运行时处理异步释放
            let rt = tokio::runtime::Handle::current();
            let lock = self.lock.clone();
            let owner = self.owner.clone();
            
            rt.spawn(async move {
                match lock.unlock(&owner).await {
                    Ok(true) => println!("自动释放锁成功: {}", owner),
                    Ok(false) => println!("自动释放锁失败 (可能已不是持有者): {}", owner),
                    Err(e) => println!("自动释放锁出错: {} - {}", owner, e),
                }
            });
            
            self.acquired = false;
        }
    }
}

// 共享资源（模拟）
struct SharedResource {
    name: String,
    value: RwLock<i32>,
}

impl SharedResource {
    fn new(name: &str) -> Self {
        Self {
            name: name.to_string(),
            value: RwLock::new(0),
        }
    }
    
    // 增加值
    async fn increment(&self, amount: i32) -> i32 {
        let mut value = self.value.write().await;
        *value += amount;
        *value
    }
    
    // 获取当前值
    async fn get_value(&self) -> i32 {
        let value = self.value.read().await;
        *value
    }
}

// 模拟客户端
struct Client {
    id: String,
    lock_manager: Arc<LockManager>,
}

impl Client {
    fn new(id: &str, lock_manager: Arc<LockManager>) -> Self {
        Self {
            id: id.to_string(),
            lock_manager,
        }
    }
    
    // 使用自动锁操作资源
    async fn increment_with_auto_lock(
        &self,
        resource: Arc<SharedResource>,
        amount: i32,
        ttl: Duration,
        timeout: Option<Duration>,
    ) -> Result<i32, String> {
        let lock = self.lock_manager.get_lock(&resource.name).await;
        
        // 获取自动锁
        let auto_lock = AutoLock::new(
            lock,
            &self.id,
            ttl,
            timeout,
        ).await?;
        
        if !auto_lock.is_acquired() {
            return Err(format!("客户端 {} 无法获取资源 {} 的锁", self.id, resource.name));
        }
        
        println!("客户端 {} 获取资源 {} 的锁", self.id, resource.name);
        
        // 模拟操作延迟
        let delay = rand::thread_rng().gen_range(100..500);
        println!("客户端 {} 处理资源 {}，延迟 {}ms", self.id, resource.name, delay);
        sleep(Duration::from_millis(delay)).await;
        
        // 增加资源值
        let new_value = resource.increment(amount).await;
        println!("客户端 {} 将资源 {} 增加 {}，新值 = {}", self.id, resource.name, amount, new_value);
        
        // 锁会自动释放
        Ok(new_value)
    }
    
    // 手动控制锁
    async fn increment_with_manual_lock(
        &self,
        resource: Arc<SharedResource>,
        amount: i32,
        ttl: Duration,
        timeout: Option<Duration>,
    ) -> Result<i32, String> {
        let lock = self.lock_manager.get_lock(&resource.name).await;
        
        // 尝试获取锁
        if !lock.lock(&self.id, ttl, timeout).await? {
            return Err(format!("客户端 {} 无法获取资源 {} 的锁", self.id, resource.name));
        }
        
        println!("客户端 {} 获取资源 {} 的锁", self.id, resource.name);
        
        // 确保锁最终会被释放
        let result = {
            // 模拟操作延迟
            let delay = rand::thread_rng().gen_range(100..1000);
            println!("客户端 {} 处理资源 {}，延迟 {}ms", self.id, resource.name, delay);
            sleep(Duration::from_millis(delay)).await;
            
            // 增加资源值
            let new_value = resource.increment(amount).await;
            println!("客户端 {} 将资源 {} 增加 {}，新值 = {}", self.id, resource.name, amount, new_value);
            
            Ok(new_value)
        };
        
        // 手动释放锁
        match lock.unlock(&self.id).await {
            Ok(true) => println!("客户端 {} 释放资源 {} 的锁", self.id, resource.name),
            Ok(false) => println!("客户端 {} 无法释放资源 {} 的锁 (可能已不是持有者)", self.id, resource.name),
            Err(e) => println!("客户端 {} 释放资源 {} 的锁时出错: {}", self.id, resource.name, e),
        }
        
        result
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    println!("=== 分布式锁示例 ===");
    
    // 创建锁管理器
    let lock_manager = Arc::new(LockManager::new());
    
    // 创建共享资源
    let counter = Arc::new(SharedResource::new("counter"));
    
    // 创建多个客户端
    let mut clients = Vec::new();
    for i in 1..=5 {
        let client = Client::new(&format!("client{}", i), lock_manager.clone());
        clients.push(client);
    }
    
    println!("\n=== 测试自动锁 ===");
    
    // 多个客户端并发操作资源（使用自动锁）
    let mut handles = Vec::new();
    for (i, client) in clients.iter().enumerate() {
        let counter_clone = counter.clone();
        
        let handle = tokio::spawn(async move {
            let amount = (i as i32) + 1;
            let ttl = Duration::from_secs(5);
            let timeout = Some(Duration::from_secs(3));
            
            match client.increment_with_auto_lock(counter_clone, amount, ttl, timeout).await {
                Ok(value) => println!("客户端 {} 完成自动锁操作，最终值 = {}", client.id, value),
                Err(e) => println!("客户端 {} 自动锁操作失败: {}", client.id, e),
            }
        });
        
        handles.push(handle);
    }
    
    // 等待所有操作完成
    join_all(handles).await;
    
    println!("\n自动锁操作后的资源值: {}", counter.get_value().await);
    
    // 重置计数器
    let _ = counter.increment(-counter.get_value().await).await;
    
    println!("\n=== 测试手动锁 ===");
    
    // 多个客户端并发操作资源（手动控制锁）
    let mut handles = Vec::new();
    for (i, client) in clients.iter().enumerate() {
        let counter_clone = counter.clone();
        
        let handle = tokio::spawn(async move {
            let amount = (i as i32) + 1;
            let ttl = Duration::from_secs(5);
            let timeout = Some(Duration::from_secs(3));
            
            match client.increment_with_manual_lock(counter_clone, amount, ttl, timeout).await {
                Ok(value) => println!("客户端 {} 完成手动锁操作，最终值 = {}", client.id, value),
                Err(e) => println!("客户端 {} 手动锁操作失败: {}", client.id, e),
            }
        });
        
        handles.push(handle);
    }

/*
<think>
I need to finish the distributed lock example code and continue with the remaining sections.
Let me complete this section and then move on to the next patterns.
</think>
*/

    // 等待所有操作完成
    join_all(handles).await;
    
    println!("\n手动锁操作后的资源值: {}", counter.get_value().await);
    
    // 测试锁重入
    println!("\n=== 测试锁重入 ===");
    let client = clients.first().unwrap();
    let lock = lock_manager.get_lock("reentrant_test").await;
    
    println!("客户端 {} 尝试第一次获取锁", client.id);
    if lock.try_lock(&client.id, Duration::from_secs(10)).await.unwrap() {
        println!("客户端 {} 成功获取锁", client.id);
        
        println!("客户端 {} 尝试第二次获取同一个锁", client.id);
        if lock.try_lock(&client.id, Duration::from_secs(10)).await.unwrap() {
            println!("客户端 {} 成功重入锁", client.id);
            
            // 释放锁
            if lock.unlock(&client.id).await.unwrap() {
                println!("客户端 {} 第一次释放锁", client.id);
            }
            
            // 再次释放锁
            if lock.unlock(&client.id).await.unwrap() {
                println!("客户端 {} 第二次释放锁", client.id);
            }
        }
    }
    
    // 测试锁超时
    println!("\n=== 测试锁超时 ===");
    let client1 = &clients[0];
    let client2 = &clients[1];
    let timeout_lock = lock_manager.get_lock("timeout_test").await;
    
    // 客户端1获取锁
    if timeout_lock.try_lock(&client1.id, Duration::from_secs(5)).await.unwrap() {
        println!("客户端 {} 获取锁，持有5秒", client1.id);
        
        // 客户端2尝试获取锁，设置2秒超时
        println!("客户端 {} 尝试获取锁，超时2秒", client2.id);
        let start = Instant::now();
        let result = timeout_lock.lock(&client2.id, Duration::from_secs(5), Some(Duration::from_secs(2))).await;
        let elapsed = start.elapsed();
        
        match result {
            Ok(true) => println!("客户端 {} 获取锁成功，用时 {:?}", client2.id, elapsed),
            Ok(false) => println!("客户端 {} 获取锁超时，用时 {:?}", client2.id, elapsed),
            Err(e) => println!("客户端 {} 获取锁错误: {}", client2.id, e),
        }
        
        // 等待锁过期
        println!("等待锁过期...");
        sleep(Duration::from_secs(5)).await;
        
        // 客户端2再次尝试获取锁
        println!("客户端 {} 再次尝试获取锁", client2.id);
        if timeout_lock.try_lock(&client2.id, Duration::from_secs(5)).await.unwrap() {
            println!("客户端 {} 成功获取已过期的锁", client2.id);
            
            // 释放锁
            if timeout_lock.unlock(&client2.id).await.unwrap() {
                println!("客户端 {} 释放锁", client2.id);
            }
        }
    }
}
```

### 9.3 分布式信号量

分布式信号量用于限制多个分布式节点对共享资源的并发访问数量。

```rust
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};
use tokio::time::sleep;
use uuid::Uuid;
use futures::future::join_all;
use rand::Rng;

// 分布式信号量接口
#[async_trait::async_trait]
trait DistributedSemaphore: Send + Sync {
    // 尝试获取一个许可
    async fn try_acquire(&self, owner: &str) -> Result<bool, String>;
    
    // 阻塞式获取许可
    async fn acquire(&self, owner: &str, timeout: Option<Duration>) -> Result<bool, String>;
    
    // 释放一个许可
    async fn release(&self, owner: &str) -> Result<bool, String>;
    
    // 获取当前可用许可数
    async fn available_permits(&self) -> Result<usize, String>;
    
    // 获取总许可数
    async fn total_permits(&self) -> Result<usize, String>;
    
    // 获取所有持有许可的所有者
    async fn get_owners(&self) -> Result<Vec<String>, String>;
}

// Redis分布式信号量实现（模拟版）
struct RedisSemaphore {
    key: String,
    permits: RwLock<usize>,  // 总许可数
    available: RwLock<usize>, // 可用许可数
    owners: RwLock<HashMap<String, usize>>, // 每个所有者持有的许可数
    waiting: Mutex<Vec<tokio::sync::oneshot::Sender<()>>>, // 等待队列
}

impl RedisSemaphore {
    fn new(resource: &str, permits: usize) -> Self {
        let key = format!("semaphore:{}", resource);
        Self {
            key,
            permits: RwLock::new(permits),
            available: RwLock::new(permits),
            owners: RwLock::new(HashMap::new()),
            waiting: Mutex::new(Vec::new()),
        }
    }
    
    // 通知等待的请求
    async fn notify_waiters(&self) {
        let mut waiting = self.waiting.lock().await;
        
        // 通知所有等待的请求
        let waiters = std::mem::take(&mut *waiting);
        for waiter in waiters {
            let _ = waiter.send(());
        }
    }
}

#[async_trait::async_trait]
impl DistributedSemaphore for RedisSemaphore {
    async fn try_acquire(&self, owner: &str) -> Result<bool, String> {
        let mut available = self.available.write().await;
        
        if *available > 0 {
            // 减少可用许可数
            *available -= 1;
            
            // 更新持有者信息
            let mut owners = self.owners.write().await;
            *owners.entry(owner.to_string()).or_insert(0) += 1;
            
            Ok(true)
        } else {
            Ok(false)
        }
    }
    
    async fn acquire(&self, owner: &str, timeout: Option<Duration>) -> Result<bool, String> {
        let start = Instant::now();
        
        // 首先尝试获取许可
        if self.try_acquire(owner).await? {
            return Ok(true);
        }
        
        // 如果指定了超时时间为0，则立即返回
        if let Some(t) = timeout {
            if t.as_millis() == 0 {
                return Ok(false);
            }
        }
        
        // 创建等待通道
        let (tx, rx) = tokio::sync::oneshot::channel();
        
        loop {
            // 检查是否超时
            if let Some(t) = timeout {
                if start.elapsed() >= t {
                    return Ok(false);
                }
            }
            
            // 再次尝试获取许可
            if self.try_acquire(owner).await? {
                return Ok(true);
            }
            
            // 添加到等待列表
            {
                let mut waiting = self.waiting.lock().await;
                waiting.push(tx);
            }
            
            // 等待通知或超时
            let remaining_timeout = timeout.map(|t| t.saturating_sub(start.elapsed()));
            
            let wait_result = if let Some(remain) = remaining_timeout {
                match tokio::time::timeout(remain, rx).await {
                    Ok(result) => result,
                    Err(_) => return Ok(false), // 超时
                }
            } else {
                rx.await
            };
            
            // 如果收到通知，再次尝试获取许可
            if wait_result.is_ok() {
                if self.try_acquire(owner).await? {
                    return Ok(true);
                }
            }
            
            // 如果获取失败，创建新的等待通道继续等待
...(about 156 lines omitted)...
struct ClientTask {
    id: String,
    semaphore_manager: Arc<SemaphoreManager>,
}

impl ClientTask {
    fn new(id: &str, semaphore_manager: Arc<SemaphoreManager>) -> Self {
        Self {
            id: id.to_string(),
            semaphore_manager,
        }
    }
    
    async fn run(&self, resource_name: &str, operations: usize, timeout: Option<Duration>) {
        let semaphore = self.semaphore_manager.get_semaphore(resource_name).await;
        
        for i in 0..operations {
            println!("客户端 {} 操作 #{} - 尝试获取资源 {} 的许可", self.id, i+1, resource_name);
            
            // 尝试获取许可
            let start = Instant::now();
            let acquired = semaphore.acquire(&self.id, timeout).await.unwrap_or(false);
            let elapsed = start.elapsed();
            
            if acquired {
                println!("客户端 {} 获取资源 {} 的许可成功，用时 {:?}", self.id, resource_name, elapsed);
                
                // 模拟使用资源
                let work_time = rand::thread_rng().gen_range(500..2000);
                println!("客户端 {} 使用资源 {} {} 毫秒", self.id, resource_name, work_time);
                sleep(Duration::from_millis(work_time)).await;
                
                // 释放许可
                match semaphore.release(&self.id).await {
                    Ok(true) => println!("客户端 {} 释放资源 {} 的许可", self.id, resource_name),
                    Ok(false) => println!("客户端 {} 无法释放资源 {} 的许可", self.id, resource_name),
                    Err(e) => println!("客户端 {} 释放资源 {} 的许可时出错: {}", self.id, resource_name, e),
                }
            } else {
                println!("客户端 {} 获取资源 {} 的许可失败，用时 {:?}", self.id, resource_name, elapsed);
                
                // 如果获取失败，稍等后重试
                sleep(Duration::from_millis(500)).await;
            }
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    println!("=== 分布式信号量示例 ===");
    
    // 创建信号量管理器
    let semaphore_manager = Arc::new(SemaphoreManager::new());
    
    // 创建有限资源
    semaphore_manager.create_semaphore("database", 3).await;
    semaphore_manager.create_semaphore("api", 5).await;
    
    // 显示初始信号量状态
    let db_sem = semaphore_manager.get_semaphore("database").await;
    let api_sem = semaphore_manager.get_semaphore("api").await;
    
    println!("数据库信号量: {} 可用 / {} 总数", 
             db_sem.available_permits().await.unwrap(),
             db_sem.total_permits().await.unwrap());
    
    println!("API信号量: {} 可用 / {} 总数", 
             api_sem.available_permits().await.unwrap(),
             api_sem.total_permits().await.unwrap());
    
    // 创建多个客户端任务
    let mut tasks = Vec::new();
    for i in 1..=10 {
        let client = ClientTask::new(&format!("client{}", i), semaphore_manager.clone());
        
        // 奇数客户端使用数据库，偶数客户端使用API
        let resource = if i % 2 == 0 { "api" } else { "database" };
        let operations = rand::thread_rng().gen_range(2..5);
        
        let handle = tokio::spawn(async move {
            client.run(resource, operations, Some(Duration::from_secs(2))).await;
        });
        
        tasks.push(handle);
    }
    
    // 等待所有任务完成
    join_all(tasks).await;
    
    // 显示最终信号量状态
    println!("\n=== 最终信号量状态 ===");
    
    println!("数据库信号量: {} 可用 / {} 总数", 
             db_sem.available_permits().await.unwrap(),
             db_sem.total_permits().await.unwrap());
    
    println!("API信号量: {} 可用 / {} 总数", 
             api_sem.available_permits().await.unwrap(),
             api_sem.total_permits().await.unwrap());
    
    // 获取所有持有者
    let db_owners = db_sem.get_owners().await.unwrap();
    let api_owners = api_sem.get_owners().await.unwrap();
    
    println!("数据库持有者: {:?}", db_owners);
    println!("API持有者: {:?}", api_owners);
    
    // 测试批量获取许可
    println!("\n=== 测试批量获取许可 ===");
    let batch_client = BatchClient::new("batch_client", semaphore_manager.clone());
    
    // 批量请求超过可用数量的许可
    println!("尝试批量获取4个数据库许可 (总数仅3个)");
    let result = batch_client.batch_acquire("database", 4, Some(Duration::from_secs(1))).await;
    
    match result {
        Ok(_) => println!("成功获取所有许可"),
        Err(e) => println!("获取失败: {}", e),
    }
    
    // 批量请求可满足的许可
    println!("\n尝试批量获取2个API许可 (有5个可用)");
    let result = batch_client.batch_acquire("api", 2, Some(Duration::from_secs(1))).await;
    
    match result {
        Ok(permits) => {
            println!("成功获取{}个许可", permits.len());
            
            // 模拟使用
            sleep(Duration::from_secs(1)).await;
            
            // 批量释放
            let released = batch_client.batch_release("api", &permits).await.unwrap();
            println!("成功释放{}个许可", released);
        },
        Err(e) => println!("获取失败: {}", e),
    }
    
    // 最终状态
    println!("\n=== 最终状态 ===");
    println!("数据库信号量: {} 可用 / {} 总数", 
             db_sem.available_permits().await.unwrap(),
             db_sem.total_permits().await.unwrap());
    
    println!("API信号量: {} 可用 / {} 总数", 
             api_sem.available_permits().await.unwrap(),
             api_sem.total_permits().await.unwrap());
}
```

## 10. 可扩展性模式

### 10.1 分片模式

分片模式通过将数据或负载分布到多个节点上，实现水平扩展。

```rust
use std::collections::{HashMap, HashSet};
use std::hash::{Hash, Hasher};
use std::sync::Arc;
use tokio::sync::RwLock;
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;
use std::fmt::Debug;
use serde::{Serialize, Deserialize};
use fnv::FnvHasher;

// 分片接口
#[async_trait::async_trait]
trait Shard<K, V>: Send + Sync {
    // 写入数据
    async fn put(&self, key: K, value: V) -> Result<(), String>;
    
    // 读取数据
    async fn get(&self, key: &K) -> Result<Option<V>, String>;
    
    // 删除数据
    async fn delete(&self, key: &K) -> Result<bool, String>;
    
    // 获取分片中的所有键
    async fn get_keys(&self) -> Result<Vec<K>, String>;
    
    // 获取分片ID
    fn get_id(&self) -> usize;
    
    // 获取分片大小
    async fn size(&self) -> Result<usize, String>;
}

// 分片管理器接口
#[async_trait::async_trait]
trait ShardManager<K, V>: Send + Sync {
    // 写入数据到合适的分片
    async fn put(&self, key: K, value: V) -> Result<(), String>;
    
    // 从合适的分片读取数据
    async fn get(&self, key: &K) -> Result<Option<V>, String>;
    
    // 从合适的分片删除数据
    async fn delete(&self, key: &K) -> Result<bool, String>;
    
    // 获取所有分片的所有键
    async fn get_all_keys(&self) -> Result<Vec<K>, String>;
    
    // 获取分片数量
    fn shard_count(&self) -> usize;
    
    // 获取分片
    fn get_shard(&self, key: &K) -> Arc<dyn Shard<K, V>>;
    
    // 获取特定ID的分片
    fn get_shard_by_id(&self, id: usize) -> Option<Arc<dyn Shard<K, V>>>;
    
    // 获取所有分片
    fn get_all_shards(&self) -> Vec<Arc<dyn Shard<K, V>>>;
}

// 内存分片实现
struct MemoryShard<K, V> {
    id: usize,
    data: RwLock<HashMap<K, V>>,
}

impl<K, V> MemoryShard<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new(id: usize) -> Self {
        Self {
            id,
            data: RwLock::new(HashMap::new()),
        }
    }
}

#[async_trait::async_trait]
impl<K, V> Shard<K, V> for MemoryShard<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    async fn put(&self, key: K, value: V) -> Result<(), String> {
        let mut data = self.data.write().await;
        data.insert(key, value);
        Ok(())
    }
    
    async fn get(&self, key: &K) -> Result<Option<V>, String> {
        let data = self.data.read().await;
        Ok(data.get(key).cloned())
    }
    
    async fn delete(&self, key: &K) -> Result<bool, String> {
        let mut data = self.data.write().await;
        Ok(data.remove(key).is_some())
    }
    
    async fn get_keys(&self) -> Result<Vec<K>, String> {
        let data = self.data.read().await;
        Ok(data.keys().cloned().collect())
    }
    
    fn get_id(&self) -> usize {
        self.id
    }
    
    async fn size(&self) -> Result<usize, String> {
        let data = self.data.read().await;
        Ok(data.len())
    }
}

// 分片策略接口
trait ShardingStrategy<K>: Send + Sync {
    // 根据键确定分片
    fn get_shard_id(&self, key: &K, shard_count: usize) -> usize;
    
    // 分片策略名称
    fn name(&self) -> &str;
}

// 哈希分片策略
struct HashShardingStrategy;

impl<K> ShardingStrategy<K> for HashShardingStrategy
where
    K: Hash,
{
    fn get_shard_id(&self, key: &K, shard_count: usize) -> usize {
        let mut hasher = FnvHasher::default();
        key.hash(&mut hasher);
        let hash = hasher.finish() as usize;
        hash % shard_count
    }
    
    fn name(&self) -> &str {
        "哈希分片"
    }
}

// 一致性哈希分片策略
struct ConsistentHashShardingStrategy {
    virtual_nodes_per_shard: usize,
    ring: Arc<RwLock<HashMap<u64, usize>>>,
}

impl ConsistentHashShardingStrategy {
    fn new(shard_count: usize, virtual_nodes_per_shard: usize) -> Self {
        let ring = Arc::new(RwLock::new(HashMap::new()));
        let strategy = Self { 
            virtual_nodes_per_shard,
            ring,
        };
...(about 294 lines omitted)...
    async fn rebalance(&self) -> Result<HashMap<usize, HashSet<K>>, String> {
        // 获取所有键
        let all_keys = self.get_all_keys().await?;
        
        // 为每个键重新计算分片ID
        let mut movements = HashMap::new();
        
        for key in all_keys {
            let current_shard = self.get_shard(&key).get_id();
            let target_shard = self.strategy.get_shard_id(&key, self.shards.len());
            
            if current_shard != target_shard {
                // 键需要移动
                movements.entry(target_shard)
                    .or_insert_with(HashSet::new)
                    .insert(key);
            }
        }
        
        Ok(movements)
    }
    
    // 获取分片统计信息
    async fn shard_stats(&self) -> Result<Vec<ShardStats>, String> {
        let mut stats = Vec::new();
        
        for shard in &self.shards {
            let id = shard.get_id();
            let size = shard.size().await?;
            
            stats.push(ShardStats {
                shard_id: id,
                key_count: size,
            });
        }
        
        Ok(stats)
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    println!("=== 分片模式示例 ===");
    
    // 创建哈希分片管理器
    let hash_manager = create_hash_shard_manager(4);
    test_shard_manager("哈希分片", hash_manager.clone(), 1000).await;
    
    // 创建一致性哈希分片管理器
    let consistent_manager = create_consistent_hash_shard_manager(4);
    test_shard_manager("一致性哈希分片", consistent_manager.clone(), 1000).await;
    
    // 测试增加分片
    println!("\n=== 测试增加分片 ===");
    test_add_shard(hash_manager.clone()).await;
    test_add_shard(consistent_manager.clone()).await;
    
    // 测试范围分片
    let range_manager = create_range_shard_manager();
    println!("\n=== 测试范围分片 ===");
    test_range_sharding(range_manager).await;
}

async fn test_shard_manager(
    name: &str, 
    manager: Arc<dyn ShardManager<String, String>>, 
    key_count: usize
) {
    println!("\n--- 测试 {} ---", name);
    println!("分片数量: {}", manager.shard_count());
    
    // 写入随机数据
    let mut rng = StdRng::seed_from_u64(42); // 使用固定种子以便结果可重现
    
    for i in 0..key_count {
        let key = format!("key{}", i);
        let value = format!("value{}", i);
        
        manager.put(key, value).await.unwrap();
    }
    
    // 获取分片统计
    let shard_manager = manager.as_any().downcast_ref::<DistributedShardManager<String, String>>().unwrap();
    let stats = shard_manager.shard_stats().await.unwrap();
    
    println!("数据分布:");
    for stat in &stats {
        println!("  分片 {}: {} 个键", stat.shard_id, stat.key_count);
    }
    
    // 计算标准差，衡量数据分布均匀性
    let mean = key_count as f64 / manager.shard_count() as f64;
    let variance = stats.iter()
        .map(|s| {
            let diff = s.key_count as f64 - mean;
            diff * diff
        })
        .sum::<f64>() / stats.len() as f64;
    let std_dev = variance.sqrt();
    
    println!("平均每个分片: {:.2} 个键", mean);
    println!("标准差: {:.2}", std_dev);
    println!("不均匀指数: {:.2}%", (std_dev / mean) * 100.0);
    
    // 读取一些随机键进行验证
    println!("\n读取验证:");
    for _ in 0..5 {
        let key_idx = rng.gen_range(0..key_count);
        let key = format!("key{}", key_idx);
        let value = manager.get(&key).await.unwrap();
        
        println!("  {} -> {:?}", key, value);
    }
}

async fn test_add_shard(manager: Arc<dyn ShardManager<String, String>>) {
    let shard_manager = manager.as_any().downcast_ref::<DistributedShardManager<String, String>>().unwrap();
    
    // 获取当前统计
    println!("\n增加分片前:");
    let before_stats = shard_manager.shard_stats().await.unwrap();
    for stat in &before_stats {
        println!("  分片 {}: {} 个键", stat.shard_id, stat.key_count);
    }
    
    // 增加一个分片
    println!("\n增加一个分片...");
    shard_manager.add_shard().await.unwrap();
    
    println!("新分片数量: {}", manager.shard_count());
    
    // 重新平衡
    println!("执行数据重平衡...");
    let movements = shard_manager.rebalance().await.unwrap();
    
    let total_moves = movements.values().map(|keys| keys.len()).sum::<usize>();
    println!("移动了 {} 个键到 {} 个分片", total_moves, movements.len());
    
    // 获取新的统计
    println!("\n增加分片后:");
    let after_stats = shard_manager.shard_stats().await.unwrap();
    for stat in &after_stats {
        println!("  分片 {}: {} 个键", stat.shard_id, stat.key_count);
    }
}

async fn test_range_sharding(manager: Arc<RangeShardManager>) {
    // 写入一些数据
    for i in 0..1000 {
        let user_id = i;
        let data = format!("用户{}的数据", i);
        
        manager.put(user_id, data).await.unwrap();
    }
    
    // 查看分片情况
    println!("\n范围分片数据分布:");
    let stats = manager.shard_stats().await.unwrap();
    
    for (shard_id, count) in stats {
        println!("  分片 {} ({}): {} 个键", 
                 shard_id,
                 manager.get_range_description(shard_id),
                 count);
    }
    
    // 读取一些数据
    println!("\n范围查询:");
    
    // 查询ID在100-200的用户
    let results = manager.range_query(100, 200).await.unwrap();
    println!("  ID在100-200之间的用户: {} 个", results.len());
    
    // 查询ID在500-550的用户
    let results = manager.range_query(500, 550).await.unwrap();
    println!("  ID在500-550之间的用户: {} 个", results.len());
    
    // 跨分片查询
    let results = manager.range_query(300, 700).await.unwrap();
    println!("  ID在300-700之间的用户（跨分片）: {} 个", results.len());
}

// 创建哈希分片管理器
fn create_hash_shard_manager(shard_count: usize) -> Arc<dyn ShardManager<String, String>> {
    // 创建分片
    let mut shards = Vec::new();
    for i in 0..shard_count {
        shards.push(Arc::new(MemoryShard::new(i)) as Arc<dyn Shard<String, String>>);
    }
    
    // 创建分片策略
    let strategy = Arc::new(HashShardingStrategy);
    
    // 创建分片管理器
    let manager = DistributedShardManager::new(shards, strategy);
    Arc::new(manager)
}

// 创建一致性哈希分片管理器
fn create_consistent_hash_shard_manager(shard_count: usize) -> Arc<dyn ShardManager<String, String>> {
    // 创建分片
    let mut shards = Vec::new();
    for i in 0..shard_count {
        shards.push(Arc::new(MemoryShard::new(i)) as Arc<dyn Shard<String, String>>);
    }
    
    // 创建分片策略
    let strategy = Arc::new(ConsistentHashShardingStrategy::new(shard_count, 100));
    
    // 创建分片管理器
    let manager = DistributedShardManager::new(shards, strategy);
    Arc::new(manager)
}

// 创建范围分片管理器
fn create_range_shard_manager() -> Arc<RangeShardManager> {
    // 定义分片范围: [开始, 结束)
    let ranges = vec![
        (0, 250, "0-249"),
        (250, 500, "250-499"),
        (500, 750, "500-749"),
        (750, 1000, "750-999")
    ]

/*
<think>
Let me continue with the code for the range sharding 
manager and the rest of the implementation.
</think>
*/

    // 创建分片
    let mut shards = Vec::new();
    for (i, (start, end, _)) in ranges.iter().enumerate() {
        shards.push((i, *start, *end));
    }
    
    Arc::new(RangeShardManager::new(shards, ranges))
}

// 使用示例
#[tokio::main]
async fn main() {
    println!("=== 分片模式示例 ===");
    
    // 创建不同类型的分片管理器
    let hash_manager = create_hash_shard_manager(4);
    let consistent_hash_manager = create_consistent_hash_shard_manager(4);
    let range_manager = create_range_shard_manager();
    
    // 测试基本哈希分片
    println!("\n### 测试哈希分片 ###");
    println!("使用标准哈希分片策略");
    
    test_sharding(hash_manager.clone(), 1000).await;
    
    // 测试一致性哈希分片
    println!("\n### 测试一致性哈希分片 ###");
    println!("使用一致性哈希分片策略");
    
    test_sharding(consistent_hash_manager.clone(), 1000).await;
    
    // 测试添加新分片
    println!("\n### 测试添加分片并重平衡 ###");
    
    test_add_shard(consistent_hash_manager.clone()).await;
    
    // 测试范围分片
    println!("\n### 测试范围分片 ###");
    
    test_range_sharding(range_manager.clone()).await;
    
    // 测试分片故障处理
    println!("\n### 测试分片故障处理 ###");
    
    test_shard_failure(consistent_hash_manager.clone()).await;
}

async fn test_shard_failure(manager: Arc<dyn ShardManager<String, String>>) {
    let shard_manager = manager.as_any()
    .downcast_ref::<DistributedShardManager<String, String>>().unwrap();
    
    // 先写入一些数据
    println!("写入数据...");
    let mut rng = StdRng::seed_from_u64(42);
    for i in 0..200 {
        let key = format!("failure_key{}", i);
        let value = format!("value{}", i);
        
        shard_manager.put(key, value).await.unwrap();
    }
    
    // 获取当前分片状态
    println!("\n故障前分片状态:");
    let before_stats = shard_manager.shard_stats().await.unwrap();
    for stat in &before_stats {
        println!("  分片 {}: {} 个键", stat.shard_id, stat.key_count);
    }
    
    // 模拟分片故障
    let failed_shard_id = 1; // 让分片1故障
    println!("\n模拟分片 {} 故障...", failed_shard_id);
    shard_manager.mark_shard_failed(failed_shard_id).await.unwrap();
    
    println!("激活故障转移...");
    let moved_keys = shard_manager.handle_shard_failure(failed_shard_id).await.unwrap();
    
    println!("将 {} 个键从分片 {} 移动到其他分片", moved_keys.len(), failed_shard_id);
    
    // 验证分片故障后的读取
    println!("\n验证故障转移后的读取:");
    for i in 0..5 {
        let key = format!("failure_key{}", rng.gen_range(0..200));
        let result = shard_manager.get(&key).await;
        
        match result {
            Ok(Some(value)) => println!("  {} -> {} (成功)", key, value),
            Ok(None) => println!("  {} -> 未找到", key),
            Err(e) => println!("  {} -> 错误: {}", key, e),
        }
    }
    
    // 获取故障后分片状态
    println!("\n故障后分片状态:");
    let after_stats = shard_manager.shard_stats().await.unwrap();
    for stat in &after_stats {
        if stat.shard_id == failed_shard_id {
            println!("  分片 {}: {} 个键 (故障)", stat.shard_id, stat.key_count);
        } else {
            println!("  分片 {}: {} 个键", stat.shard_id, stat.key_count);
        }
    }
    
    // 恢复分片
    println!("\n恢复分片 {}...", failed_shard_id);
    shard_manager.recover_shard(failed_shard_id).await.unwrap();
    
    // 重新平衡
    println!("执行重平衡...");
    let movements = shard_manager.rebalance().await.unwrap();
    
    let total_moves = movements.values().map(|keys| keys.len()).sum::<usize>();
    println!("恢复后移动了 {} 个键", total_moves);
    
    // 获取恢复后分片状态
    println!("\n恢复后分片状态:");
    let recovery_stats = shard_manager.shard_stats().await.unwrap();
    for stat in &recovery_stats {
        println!("  分片 {}: {} 个键", stat.shard_id, stat.key_count);
    }
}
```

### 10.2 复制模式

复制模式通过在多个节点上复制数据，提高系统的读取性能和可用性。

```rust
use std::collections::{HashMap, HashSet};
use std::fmt::Debug;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{RwLock, Mutex};
use tokio::time::sleep;
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;
use uuid::Uuid;
use serde::{Serialize, Deserialize};
use async_trait::async_trait;

// 复制节点类型
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum ReplicaRole {
    Primary,   // 主节点，负责读写
    Secondary, // 从节点，只读
    Standby,   // 热备节点，不服务请求，但保持数据同步
}

// 复制节点状态
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum ReplicaState {
    Online,
    Offline,
    Recovering,
    Syncing,
}

// 数据操作
#[derive(Debug, Clone, Serialize, Deserialize)]
enum DataOperation<K, V> {
    Put(K, V),
    Delete(K),
    Clear,
}

// 复制日志条目
#[derive(Debug, Clone, Serialize, Deserialize)]
struct ReplicationLogEntry<K, V> {
    id: String,
    timestamp: u64,
    operation: DataOperation<K, V>,
}

// 复制节点接口
#[async_trait]
trait Replica<K, V>: Send + Sync {
    // 获取节点ID
    fn id(&self) -> &str;
    
    // 获取节点角色
    fn role(&self) -> ReplicaRole;
    
    // 获取节点状态
    async fn state(&self) -> ReplicaState;
    
    // 设置节点角色
    async fn set_role(&self, role: ReplicaRole);
    
    // 设置节点状态
    async fn set_state(&self, state: ReplicaState);
    
    // 写入数据
    async fn put(&self, key: K, value: V) -> Result<(), String>;
    
    // 读取数据
    async fn get(&self, key: &K) -> Result<Option<V>, String>;
    
    // 删除数据
    async fn delete(&self, key: &K) -> Result<bool, String>;
    
    // 清空数据
    async fn clear(&self) -> Result<(), String>;
    
    // 获取所有键
    async fn keys(&self) -> Result<Vec<K>, String>;
    
    // 获取数据量
    async fn size(&self) -> Result<usize, String>;
    
    // 应用操作
    async fn apply_operation(&self, op: DataOperation<K, V>) -> Result<(), String>;
    
    // 同步数据（从另一个节点）
    async fn sync_from(&self, source: Arc<dyn Replica<K, V>>) -> Result<usize, String>;
    
    // 获取最后更新时间
    async fn last_updated(&self) -> Instant;
    
    // 获取复制日志
    async fn get_replication_log(&self, since: Option<u64>) -> Result<Vec<ReplicationLogEntry<K, V>>, String>;
}

// 内存复制节点实现
struct MemoryReplica<K, V> {
    id: String,
    role: RwLock<ReplicaRole>,
    state: RwLock<ReplicaState>,
    data: RwLock<HashMap<K, V>>,
    last_update: RwLock<Instant>,
    replication_log: RwLock<Vec<ReplicationLogEntry<K, V>>>,
    log_retention: usize, // 保留的日志条目数
}

impl<K, V> MemoryReplica<K, V>
where
...(about 100 lines omitted)...
    async fn sync_from(&self, source: Arc<dyn Replica<K, V>>) -> Result<usize, String> {
        let mut state = self.state.write().await;
        *state = ReplicaState::Syncing;
        drop(state); // 释放锁
        
        // 获取当前最后一条日志的时间戳
        let last_timestamp = {
            let log = self.replication_log.read().await;
            log.last().map(|entry| entry.timestamp).unwrap_or(0)
        };
        
        // 从源节点获取增量日志
        let new_entries = source.get_replication_log(Some(last_timestamp)).await?;
        
        if new_entries.is_empty() {
            // 没有新数据，设置为在线状态
            let mut state = self.state.write().await;
            *state = ReplicaState::Online;
            return Ok(0);
        }
        
        // 应用所有新操作
        let mut applied_count = 0;
        for entry in new_entries.iter() {
            self.apply_operation(entry.operation.clone()).await?;
            applied_count += 1;
            
            // 添加到本地复制日志
            let mut log = self.replication_log.write().await;
            log.push(entry.clone());
            
            // 如果超过保留限制，删除旧日志
            while log.len() > self.log_retention {
                log.remove(0);
            }
        }
        
        // 更新同步时间
        let mut last_update = self.last_update.write().await;
        *last_update = Instant::now();
        
        // 设置为在线状态
        let mut state = self.state.write().await;
        *state = ReplicaState::Online;
        
        Ok(applied_count)
    }
    
    async fn last_updated(&self) -> Instant {
        *self.last_update.read().await
    }
    
    async fn get_replication_log(&self, since: Option<u64>) -> Result<Vec<ReplicationLogEntry<K, V>>, String> {
        let log = self.replication_log.read().await;
        
        let filtered_log = if let Some(timestamp) = since {
            log.iter()
                .filter(|entry| entry.timestamp > timestamp)
                .cloned()
                .collect()
        } else {
            log.clone()
        };
        
        Ok(filtered_log)
    }
}

// 复制组接口
#[async_trait]
trait ReplicaGroup<K, V>: Send + Sync {
    // 添加复制节点
    async fn add_replica(&self, replica: Arc<dyn Replica<K, V>>) -> Result<(), String>;
    
    // 移除复制节点
    async fn remove_replica(&self, replica_id: &str) -> Result<(), String>;
    
    // 获取主节点
    async fn get_primary(&self) -> Option<Arc<dyn Replica<K, V>>>;
    
    // 获取所有从节点
    async fn get_secondaries(&self) -> Vec<Arc<dyn Replica<K, V>>>;
    
    // 获取所有热备节点
    async fn get_standbys(&self) -> Vec<Arc<dyn Replica<K, V>>>;
    
    // 获取所有节点
    async fn get_all_replicas(&self) -> Vec<Arc<dyn Replica<K, V>>>;
    
    // 获取特定节点
    async fn get_replica(&self, replica_id: &str) -> Option<Arc<dyn Replica<K, V>>>;
    
    // 设置主节点
    async fn set_primary(&self, replica_id: &str) -> Result<(), String>;
    
    // 写入数据（到主节点）
    async fn write(&self, key: K, value: V) -> Result<(), String>;
    
    // 读取数据（可以从任何节点）
    async fn read(&self, key: &K) -> Result<Option<V>, String>;
    
    // 删除数据
    async fn delete(&self, key: &K) -> Result<bool, String>;
    
    // 同步复制组
    async fn sync_group(&self) -> Result<HashMap<String, usize>, String>;
    
    // 故障转移
    async fn failover(&self, failed_primary_id: &str) -> Result<Option<String>, String>;
}

// 复制组实现
struct ReplicaGroupImpl<K, V> {
    replicas: RwLock<HashMap<String, Arc<dyn Replica<K, V>>>>,
    primary_id: RwLock<Option<String>>,
    read_preference: ReadPreference,
}

// 读取偏好
#[derive(Debug, Clone, Copy)]
enum ReadPreference {
    PrimaryOnly,           // 只从主节点读取
    PrimaryPreferred,      // 优先从主节点读取，如果不可用则从从节点读取
    SecondaryOnly,         // 只从从节点读取
    SecondaryPreferred,    // 优先从从节点读取，如果不可用则从主节点读取
    Nearest,               // 从延迟最低的节点读取
}


/*
<think>
我将继续提供分布式系统设计模式中复制模式的代码示例，继续 ReplicaGroupImpl 实现部分。
</think>
*/

impl<K, V> ReplicaGroupImpl<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new(read_preference: ReadPreference) -> Self {
        Self {
            replicas: RwLock::new(HashMap::new()),
            primary_id: RwLock::new(None),
            read_preference,
        }
    }
    
    // 用于随机选择从节点的辅助方法
    async fn select_secondary(&self) -> Option<Arc<dyn Replica<K, V>>> {
        let replicas = self.replicas.read().await;
        let primary_id = self.primary_id.read().await;
        
        let secondaries: Vec<_> = replicas.values()
            .filter(|r| r.role() == ReplicaRole::Secondary)
            .filter(|r| async {
                r.state().await == ReplicaState::Online
            }.await)
            .cloned()
            .collect();
        
        if secondaries.is_empty() {
            return None;
        }
        
        // 随机选择一个从节点
        let idx = rand::thread_rng().gen_range(0..secondaries.len());
        Some(secondaries[idx].clone())
    }
}

#[async_trait]
impl<K, V> ReplicaGroup<K, V> for ReplicaGroupImpl<K, V>
where
    K: Clone + Eq + std::hash::Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    async fn add_replica(&self, replica: Arc<dyn Replica<K, V>>) -> Result<(), String> {
        let replica_id = replica.id().to_string();
        let role = replica.role();
        
        let mut replicas = self.replicas.write().await;
        
        // 如果是主节点且已有主节点，则返回错误
        if role == ReplicaRole::Primary {
            let primary_id = self.primary_id.read().await;
            if let Some(id) = &*primary_id {
                if id != &replica_id {
                    return Err(format!("已存在主节点: {}", id));
                }
            }
            
            // 设置为主节点
            drop(primary_id);
            let mut primary_id = self.primary_id.write().await;
            *primary_id = Some(replica_id.clone());
        }
        
        // 添加到复制组
        replicas.insert(replica_id, replica);
        
        Ok(())
    }
    
    async fn remove_replica(&self, replica_id: &str) -> Result<(), String> {
        let mut replicas = self.replicas.write().await;
        
        // 如果是主节点，则清除主节点ID
        let primary_id = self.primary_id.read().await;
        if let Some(id) = &*primary_id {
            if id == replica_id {
                drop(primary_id);
                let mut primary_id = self.primary_id.write().await;
                *primary_id = None;
            }
        }
        
        // 从复制组移除
        if replicas.remove(replica_id).is_none() {
            return Err(format!("节点不存在: {}", replica_id));
        }
        
        Ok(())
    }
    
    async fn get_primary(&self) -> Option<Arc<dyn Replica<K, V>>> {
        let primary_id = self.primary_id.read().await;
        let id = primary_id.as_ref()?;
        
        let replicas = self.replicas.read().await;
        replicas.get(id).cloned()
    }
    
    async fn get_secondaries(&self) -> Vec<Arc<dyn Replica<K, V>>> {
        let replicas = self.replicas.read().await;
        
        replicas.values()
            .filter(|r| r.role() == ReplicaRole::Secondary)
            .cloned()
            .collect()
    }
    
    async fn get_standbys(&self) -> Vec<Arc<dyn Replica<K, V>>> {
        let replicas = self.replicas.read().await;
        
        replicas.values()
            .filter(|r| r.role() == ReplicaRole::Standby)
            .cloned()
            .collect()
    }
    
    async fn get_all_replicas(&self) -> Vec<Arc<dyn Replica<K, V>>> {
        let replicas = self.replicas.read().await;
        replicas.values().cloned().collect()
    }
    
    async fn get_replica(&self, replica_id: &str) -> Option<Arc<dyn Replica<K, V>>> {
        let replicas = self.replicas.read().await;
        replicas.get(replica_id).cloned()
    }
    
    async fn set_primary(&self, replica_id: &str) -> Result<(), String> {
        let replicas = self.replicas.read().await;
        
        // 检查节点是否存在
        let replica = match replicas.get(replica_id) {
            Some(r) => r,
            None => return Err(format!("节点不存在: {}", replica_id)),
        };
        
        // 获取旧主节点
        let old_primary_id = {
            let primary_id = self.primary_id.read().await;
            primary_id.clone()
        };
        
        // 如果已经是主节点，则不做任何操作
        if let Some(id) = &old_primary_id {
            if id == replica_id {
                return Ok(());
            }
        }
        
        // 将旧主节点设置为从节点
        if let Some(id) = old_primary_id {
            if let Some(old_primary) = replicas.get(&id) {
                old_primary.set_role(ReplicaRole::Secondary).await;
            }
        }
        
        // 将新节点设置为主节点
        replica.set_role(ReplicaRole::Primary).await;
        
        // 更新主节点ID
        let mut primary_id = self.primary_id.write().await;
        *primary_id = Some(replica_id.to_string());
        
        Ok(())
    }
    
    async fn write(&self, key: K, value: V) -> Result<(), String> {
        // 获取主节点
        let primary = self.get_primary().await
            .ok_or_else(|| "无主节点".to_string())?;
        
        // 主节点必须在线
        if primary.state().await != ReplicaState::Online {
            return Err("主节点不在线".to_string());
        }
        
        // 写入数据
        primary.put(key, value).await
    }
    
    async fn read(&self, key: &K) -> Result<Option<V>, String> {
        match self.read_preference {
            ReadPreference::PrimaryOnly => {
                // 只从主节点读取
                let primary = self.get_primary().await
                    .ok_or_else(|| "无主节点".to_string())?;
                
                if primary.state().await != ReplicaState::Online {
                    return Err("主节点不在线".to_string());
                }
                
                primary.get(key).await
            },
            ReadPreference::PrimaryPreferred => {
                // 优先从主节点读取
                if let Some(primary) = self.get_primary().await {
                    if primary.state().await == ReplicaState::Online {
                        return primary.get(key).await;
                    }
                }
                
                // 主节点不可用，尝试从节点
                if let Some(secondary) = self.select_secondary().await {
                    return secondary.get(key).await;
                }
                
                Err("无可用节点".to_string())
            },
            ReadPreference::SecondaryOnly => {
                // 只从从节点读取
                if let Some(secondary) = self.select_secondary().await {
                    return secondary.get(key).await;
                }
                
                Err("无可用从节点".to_string())
            },
            ReadPreference::SecondaryPreferred => {
                // 优先从从节点读取
                if let Some(secondary) = self.select_secondary().await {
                    return secondary.get(key).await;
                }
                
                // 从节点不可用，尝试主节点
                if let Some(primary) = self.get_primary().await {
                    if primary.state().await == ReplicaState::Online {
                        return primary.get(key).await;
                    }
                }
                
                Err("无可用节点".to_string())
            },
            ReadPreference::Nearest => {
                // 选择最近的节点（这里简化为随机选择所有在线节点）
                let replicas = self.replicas.read().await;
                let online_replicas: Vec<_> = replicas.values()
                    .filter(|r| async {
                        r.state().await == ReplicaState::Online
                    }.await)
                    .collect();
                
                if online_replicas.is_empty() {
                    return Err("无可用节点".to_string());
                }
                
                let idx = rand::thread_rng().gen_range(0..online_replicas.len());
                online_replicas[idx].get(key).await
            }
        }
    }
    
    async fn delete(&self, key: &K) -> Result<bool, String> {
        // 获取主节点
        let primary = self.get_primary().await
            .ok_or_else(|| "无主节点".to_string())?;
        
        // 主节点必须在线
        if primary.state().await != ReplicaState::Online {
            return Err("主节点不在线".to_string());
        }
        
        // 删除数据
        primary.delete(key).await
    }
    
    async fn sync_group(&self) -> Result<HashMap<String, usize>, String> {
        let primary = self.get_primary().await
            .ok_or_else(|| "无主节点".to_string())?;
        
        // 同步所有非主节点
        let replicas = self.replicas.read().await;
        let mut sync_results = HashMap::new();
        
        for (id, replica) in replicas.iter() {
            // 跳过主节点和离线节点
            if replica.id() == primary.id() || replica.state().await == ReplicaState::Offline {
                continue;
            }
            
            // 同步数据
            match replica.sync_from(primary.clone()).await {
                Ok(count) => {
                    sync_results.insert(id.clone(), count);
                },
                Err(e) => {
                    sync_results.insert(id.clone(), 0);
                    println!("同步节点 {} 失败: {}", id, e);
                }
            }
        }
        
        Ok(sync_results)
    }
    
    async fn failover(&self, failed_primary_id: &str) -> Result<Option<String>, String> {
        // 检查指定的节点是否为当前主节点
        let current_primary_id = {
            let primary_id = self.primary_id.read().await;
            primary_id.clone()
        };
        
        if let Some(id) = &current_primary_id {
            if id != failed_primary_id {
                return Err(format!("指定节点 {} 不是当前主节点 {}", failed_primary_id, id));
            }
        } else {
            return Err("当前无主节点".to_string());
        }
        
        // 将失败的主节点设置为离线
        if let Some(failed_primary) = self.get_replica(failed_primary_id).await {
            failed_primary.set_state(ReplicaState::Offline).await;
        }
        
        // 选择新的主节点（优先选择从节点，其次是热备节点）
        let secondaries = self.get_secondaries().await;
        let standbys = self.get_standbys().await;
        
        let online_secondaries: Vec<_> = secondaries.iter()
            .filter(|r| async {
                r.state().await == ReplicaState::Online
            }.await)
            .collect();
        
        let online_standbys: Vec<_> = standbys.iter()
            .filter(|r| async {
                r.state().await == ReplicaState::Online
            }.await)
            .collect();
        
        // 选择新主节点
        let new_primary = if !online_secondaries.is_empty() {
            // 选择最新的从节点
            online_secondaries.iter()
                .max_by_key(|r| async {
                    r.last_updated().await
                }.await)
                .map(|r| (*r).clone())
        } else if !online_standbys.is_empty() {
            // 如果没有从节点，选择最新的热备节点
            online_standbys.iter()
                .max_by_key(|r| async {
                    r.last_updated().await
                }.await)
                .map(|r| (*r).clone())
        } else {
            None
        };
        
        // 如果找到新主节点，则执行故障转移
        if let Some(new_primary) = new_primary {
            let new_primary_id = new_primary.id().to_string();
            self.set_primary(&new_primary_id).await?;
            return Ok(Some(new_primary_id));
        }
        
        // 无可用节点
        let mut primary_id = self.primary_id.write().await;
        *primary_id = None;
        
        Ok(None)
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    println!("=== 复制模式示例 ===");
    
    // 创建复制组
    let replica_group: Arc<dyn ReplicaGroup<String, String>> = Arc::new(
        ReplicaGroupImpl::new(ReadPreference::PrimaryPreferred)
    );
    
    // 创建节点
    let primary = Arc::new(MemoryReplica::new("node1", ReplicaRole::Primary, 100));
    let secondary1 = Arc::new(MemoryReplica::new("node2", ReplicaRole::Secondary, 100));
    let secondary2 = Arc::new(MemoryReplica::new("node3", ReplicaRole::Secondary, 100));
    let standby = Arc::new(MemoryReplica::new("node4", ReplicaRole::Standby, 100));
    
    // 添加节点到复制组
    println!("添加节点到复制组...");
    replica_group.add_replica(primary.clone()).await.unwrap();
    replica_group.add_replica(secondary1.clone()).await.unwrap();
    replica_group.add_replica(secondary2.clone()).await.unwrap();
    replica_group.add_replica(standby.clone()).await.unwrap();
    
    // 写入一些数据
    println!("\n写入数据...");
    for i in 1..=10 {
        let key = format!("key{}", i);
        let value = format!("value{}", i);
        
        replica_group.write(key, value).await.unwrap();
    }
    
    // 同步复制组
    println!("\n同步复制组...");
    let sync_results = replica_group.sync_group().await.unwrap();
    
    for (id, count) in &sync_results {
        println!("节点 {} 同步了 {} 个操作", id, count);
    }
    
    // 读取数据（应该从任何节点读取，基于读取偏好）
    println!("\n从复制组读取数据...");
    for i in 1..=5 {
        let key = format!("key{}", i);
        let value = replica_group.read(&key).await.unwrap();
        
        println!("{} -> {:?}", key, value);
    }
    
    // 模拟主节点故障
    println!("\n模拟主节点故障...");
    let primary_id = "node1";
    let new_primary = replica_group.failover(primary_id).await.unwrap();
    
    println!("故障转移结果: 新主节点 = {:?}", new_primary);
    
    // 再次写入和读取数据
    println!("\n故障转移后写入新数据...");
    for i in 11..=15 {
        let key = format!("key{}", i);
        let value = format!("value{}", i);
        
        match replica_group.write(key.clone(), value).await {
            Ok(()) => println!("成功写入 {}", key),
            Err(e) => println!("写入 {} 失败: {}", key, e),
        }
    }
    
    // 读取所有数据
    println!("\n故障转移后读取所有数据...");
    if let Some(new_primary_id) = &new_primary {
        let new_primary_node = replica_group.get_replica(new_primary_id).await.unwrap();
        let keys = new_primary_node.keys().await.unwrap();
        
        for key in keys {
            let value = replica_group.read(&key).await.unwrap();
            println!("{} -> {:?}", key, value);
        }
    }
    
    // 恢复原主节点
    if let Some(new_primary_id) = &new_primary {
        println!("\n恢复原主节点...");
        
        // 重新上线原主节点作为从节点
        let original_primary = replica_group.get_replica(primary_id).await.unwrap();
        original_primary.set_role(ReplicaRole::Secondary).await;
        original_primary.set_state(ReplicaState::Online).await;
        
        // 同步数据
        let new_primary_node = replica_group.get_replica(new_primary_id).await.unwrap();
        let sync_count = original_primary.sync_from(new_primary_node).await.unwrap();
        
        println!("原主节点 {} 作为从节点恢复，同步了 {} 个操作", primary_id, sync_count);
        
        // 显示当前所有节点状态
        println!("\n当前复制组状态:");
        let all_replicas = replica_group.get_all_replicas().await;
        
        for replica in all_replicas {
            println!("节点 {}: 角色={:?}, 状态={:?}, 数据量={}",
                     replica.id(), replica.role(), replica.state().await,
                     replica.size().await.unwrap());
        }
    }
}
```

### 10.3 缓存模式

缓存模式通过临时存储频繁访问的数据，减少对后端存储系统的访问，提高读取性能和系统吞吐量。

```rust
use std::collections::{HashMap, LinkedList};
use std::hash::Hash;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{RwLock, Mutex};
use tokio::time::sleep;
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;
use async_trait::async_trait;

// 缓存键值对
#[derive(Debug, Clone)]
struct CacheEntry<V> {
    value: V,
    created_at: Instant,
    expires_at: Option<Instant>,
    last_accessed: Instant,
    access_count: u64,
}

impl<V> CacheEntry<V> {
    fn new(value: V, ttl: Option<Duration>) -> Self {
        let now = Instant::now();
        Self {
            value,
            created_at: now,
            expires_at: ttl.map(|ttl| now + ttl),
            last_accessed: now,
            access_count: 0,
        }
    }
    
    // 检查是否过期
    fn is_expired(&self) -> bool {
        if let Some(expires_at) = self.expires_at {
            Instant::now() > expires_at
        } else {
            false
        }
    }
    
    // 访问值并更新统计
    fn access(&mut self) -> &V {
        self.last_accessed = Instant::now();
        self.access_count += 1;
        &self.value
    }
}

/*
<think>
我需要继续完成缓存模式的代码示例。这应该包括完整的缓存接口定义、缓存策略实现、以及使用示例。接下来应该是定义缓存接口、各种缓存淘汰策略、实现具体的缓存类、以及展示如何使用这些缓存。

我会继续完成缓存模式的代码，包括：
1. 缓存接口(Cache trait)
2. 缓存策略接口(CachePolicy trait)
3. 不同缓存策略的实现(LRU, LFU, FIFO等)
4. 具体缓存实现
5. 多级缓存示例
6. 使用示例
</think>
*/

// 缓存接口
#[async_trait]
trait Cache<K, V>: Send + Sync {
    // 存储值到缓存
    async fn put(&self, key: K, value: V, ttl: Option<Duration>) -> Result<(), String>;
    
    // 从缓存获取值
    async fn get(&self, key: &K) -> Result<Option<V>, String>;
    
    // 检查键是否存在
    async fn contains(&self, key: &K) -> Result<bool, String>;
    
    // 从缓存移除值
    async fn remove(&self, key: &K) -> Result<Option<V>, String>;
    
    // 清空缓存
    async fn clear(&self) -> Result<(), String>;
    
    // 获取所有键
    async fn keys(&self) -> Result<Vec<K>, String>;
    
    // 获取缓存大小
    async fn size(&self) -> Result<usize, String>;
    
    // 执行缓存维护
    async fn maintenance(&self) -> Result<usize, String>;
    
    // 获取缓存统计信息
    async fn stats(&self) -> Result<CacheStats, String>;
}

// 缓存统计信息
#[derive(Debug, Clone)]
struct CacheStats {
    size: usize,
    capacity: usize,
    hits: u64,
    misses: u64,
    evictions: u64,
    expirations: u64,
}

impl CacheStats {
    fn new(capacity: usize) -> Self {
        Self {
            size: 0,
            capacity,
            hits: 0,
            misses: 0,
            evictions: 0,
            expirations: 0,
        }
    }
    
    fn hit_rate(&self) -> f64 {
        let total = self.hits + self.misses;
        if total == 0 {
            0.0
        } else {
            self.hits as f64 / total as f64
        }
    }
}

// 缓存策略接口
trait CachePolicy<K, V>: Send + Sync {
    // 添加条目
    fn add(&mut self, key: &K, entry: &CacheEntry<V>);
    
    // 访问条目
    fn access(&mut self, key: &K, entry: &CacheEntry<V>);
    
    // 选择要淘汰的键
    fn evict(&mut self) -> Option<K>;
    
    // 移除键
    fn remove(&mut self, key: &K);
    
    // 清空
    fn clear(&mut self);
}

// LRU缓存策略
struct LRUPolicy<K> {
    access_order: LinkedList<K>,
    key_positions: HashMap<K, linked_list::Iter<K>>,
}

impl<K: Clone + Eq + Hash> CachePolicy<K, V> for LRUPolicy<K> {
    fn add(&mut self, key: &K, _entry: &CacheEntry<V>) {
        self.access_order.push_back(key.clone());
        // 更新位置
    }
    
    fn access(&mut self, key: &K, _entry: &CacheEntry<V>) {
        // 移动到链表末尾
    }
    
    fn evict(&mut self) -> Option<K> {
        self.access_order.pop_front()
    }
    
    fn remove(&mut self, key: &K) {
        // 从链表中移除
    }
    
    fn clear(&mut self) {
        self.access_order.clear();
        self.key_positions.clear();
    }
}

// LFU缓存策略
struct LFUPolicy<K> {
    frequency_map: HashMap<u64, LinkedList<K>>,
    key_frequencies: HashMap<K, u64>,
    min_frequency: u64,
}

// FIFO缓存策略
struct FIFOPolicy<K> {
    insertion_order: LinkedList<K>,
}

// 内存缓存实现
struct MemoryCache<K, V, P> {
    data: RwLock<HashMap<K, CacheEntry<V>>>,
    policy: Mutex<P>,
    capacity: usize,
    stats: RwLock<CacheStats>,
    last_maintenance: RwLock<Instant>,
    maintenance_interval: Duration,
}

impl<K, V, P> MemoryCache<K, V, P>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
    P: CachePolicy<K, V> + 'static,
{
    fn new(capacity: usize, policy: P) -> Self {
        Self {
            data: RwLock::new(HashMap::with_capacity(capacity)),
            policy: Mutex::new(policy),
            capacity,
            stats: RwLock::new(CacheStats::new(capacity)),
            last_maintenance: RwLock::new(Instant::now()),
            maintenance_interval: Duration::from_secs(60),
        }
    }
    
    // 检查并执行维护
    async fn check_maintenance(&self) {
        let now = Instant::now();
        let should_maintain = {
            let last = self.last_maintenance.read().await;
            now.duration_since(*last) >= self.maintenance_interval
        };
        
        if should_maintain {
            let _ = self.maintenance().await;
            let mut last = self.last_maintenance.write().await;
            *last = now;
        }
    }
    
    // 淘汰一个条目
    async fn evict_one(&self) -> bool {
        let key_to_evict = {
            let mut policy = self.policy.lock().await;
            policy.evict()
        };
        
        if let Some(key) = key_to_evict {
            let mut data = self.data.write().await;
            if let Some(_) = data.remove(&key) {
                let mut stats = self.stats.write().await;
                stats.evictions += 1;
                stats.size = data.len();
                return true;
            }
        }
        
        false
    }
}

#[async_trait]
impl<K, V, P> Cache<K, V> for MemoryCache<K, V, P>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
    P: CachePolicy<K, V> + 'static,
{
    async fn put(&self, key: K, value: V, ttl: Option<Duration>) -> Result<(), String> {
        // 检查是否需要维护
        self.check_maintenance().await;
        
        let entry = CacheEntry::new(value, ttl);
        
        // 获取写锁
        let mut data = self.data.write().await;
        
        // 如果达到容量上限，先淘汰
        if data.len() >= self.capacity && !data.contains_key(&key) {
            drop(data); // 释放锁，避免死锁
            self.evict_one().await;
            data = self.data.write().await; // 重新获取锁
        }
        
        // 如果是更新，先移除旧条目
        let is_update = data.contains_key(&key);
        
        // 添加新条目
        data.insert(key.clone(), entry.clone());
        
        // 更新策略和统计
        {
            let mut policy = self.policy.lock().await;
            if is_update {
                policy.remove(&key);
            }
            policy.add(&key, &entry);
            
            let mut stats = self.stats.write().await;
            stats.size = data.len();
        }
        
        Ok(())
    }
    
    async fn get(&self, key: &K) -> Result<Option<V>, String> {
        // 检查是否需要维护
        self.check_maintenance().await;
        
        // 获取读锁
        let mut data = self.data.write().await; // 使用写锁因为需要更新访问统计
        
        if let Some(entry) = data.get_mut(key) {
            // 检查是否过期
            if entry.is_expired() {
                // 过期了，移除并更新统计
                let removed_entry = data.remove(key);
                
                let mut policy = self.policy.lock().await;
                policy.remove(key);
                
                let mut stats = self.stats.write().await;
                stats.expirations += 1;
                stats.misses += 1;
                stats.size = data.len();
                
                return Ok(None);
            }
            
            // 更新访问统计
            let value = entry.access().clone();
            
            // 更新策略和命中统计
            {
                let mut policy = self.policy.lock().await;
                policy.access(key, entry);
                
                let mut stats = self.stats.write().await;
                stats.hits += 1;
            }
            
            Ok(Some(value))
        } else {
            // 更新未命中统计
            let mut stats = self.stats.write().await;
            stats.misses += 1;
            
            Ok(None)
        }
    }
    
    async fn contains(&self, key: &K) -> Result<bool, String> {
        let data = self.data.read().await;
        
        if let Some(entry) = data.get(key) {
            if entry.is_expired() {
                Ok(false)
            } else {
                Ok(true)
            }
        } else {
            Ok(false)
        }
    }
    
    async fn remove(&self, key: &K) -> Result<Option<V>, String> {
        let mut data = self.data.write().await;
        
        if let Some(entry) = data.remove(key) {
            // 更新策略和统计
            let mut policy = self.policy.lock().await;
            policy.remove(key);
            
            let mut stats = self.stats.write().await;
            stats.size = data.len();
            
            if entry.is_expired() {
                stats.expirations += 1;
                Ok(None)
            } else {
                Ok(Some(entry.value))
            }
        } else {
            Ok(None)
        }
    }
    
    async fn clear(&self) -> Result<(), String> {
        let mut data = self.data.write().await;
        data.clear();
        
        let mut policy = self.policy.lock().await;
        policy.clear();
        
        let mut stats = self.stats.write().await;
        stats.size = 0;
        
        Ok(())
    }
    
    async fn keys(&self) -> Result<Vec<K>, String> {
        let data = self.data.read().await;
        
        let valid_keys = data.iter()
            .filter(|(_, entry)| !entry.is_expired())
            .map(|(key, _)| key.clone())
            .collect();
        
        Ok(valid_keys)
    }
    
    async fn size(&self) -> Result<usize, String> {
        let data = self.data.read().await;
        
        // 只计算未过期的条目
        let valid_count = data.iter()
            .filter(|(_, entry)| !entry.is_expired())
            .count();
        
        Ok(valid_count)
    }
    
    async fn maintenance(&self) -> Result<usize, String> {
        let mut removed_count = 0;
        
        // 移除过期条目
        let expired_keys = {
            let data = self.data.read().await;
            data.iter()
                .filter(|(_, entry)| entry.is_expired())
                .map(|(key, _)| key.clone())
                .collect::<Vec<_>>()
        };
        
        // 批量移除过期条目
        if !expired_keys.is_empty() {
            let mut data = self.data.write().await;
            let mut policy = self.policy.lock().await;
            
            for key in &expired_keys {
                if data.remove(key).is_some() {
                    removed_count += 1;
                    policy.remove(key);
                }
            }
            
            // 更新统计
            let mut stats = self.stats.write().await;
            stats.expirations += removed_count as u64;
            stats.size = data.len();
        }
        
        Ok(removed_count)
    }
    
    async fn stats(&self) -> Result<CacheStats, String> {
        let stats = self.stats.read().await;
        Ok(stats.clone())
    }
}

// 多级缓存
struct MultiLevelCache<K, V> {
    levels: Vec<(Arc<dyn Cache<K, V>>, String)>,
}

impl<K, V> MultiLevelCache<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new() -> Self {
        Self {
            levels: Vec::new(),
        }
    }
    
    // 添加缓存层级
    fn add_level(&mut self, cache: Arc<dyn Cache<K, V>>, name: &str) {
        self.levels.push((cache, name.to_string()));
    }
    
    // 从缓存获取值
    async fn get(&self, key: &K) -> Result<Option<(V, String)>, String> {
        // 从最高层级开始查找
        for (level, (cache, name)) in self.levels.iter().enumerate() {
            match cache.get(key).await {
                Ok(Some(value)) => {
                    // 命中缓存，将值填充到更高层级
                    for i in 0..level {
                        let (higher_cache, _) = &self.levels[i];
                        let _ = higher_cache.put(key.clone(), value.clone(), None).await;
                    }
                    
                    return Ok(Some((value, name.clone())));
                },
                Ok(None) => continue, // 继续检查下一级
                Err(e) => return Err(e),
            }
        }
        
        Ok(None)
    }
    
    // 存储值到所有层级
    async fn put(&self, key: K, value: V, ttl: Option<Duration>) -> Result<(), String> {
        for (cache, _) in &self.levels {
            cache.put(key.clone(), value.clone(), ttl).await?;
        }
        
        Ok(())
    }
    
    // 从所有层级移除值
    async fn remove(&self, key: &K) -> Result<bool, String> {
        let mut removed = false;
        
        for (cache, _) in &self.levels {
            if cache.remove(key).await?.is_some() {
                removed = true;
            }
        }
        
        Ok(removed)
    }
    
    // 获取所有层级的统计信息
    async fn stats(&self) -> Result<HashMap<String, CacheStats>, String> {
        let mut result = HashMap::new();
        
        for (cache, name) in &self.levels {
            result.insert(name.clone(), cache.stats().await?);
        }
        
        Ok(result)
    }
}

// 后端存储服务（模拟数据库）
struct DatabaseService {
    data: RwLock<HashMap<String, String>>,
    read_latency: Duration, // 模拟读取延迟
}

impl DatabaseService {
    fn new(read_latency: Duration) -> Self {
        Self {
            data: RwLock::new(HashMap::new()),
            read_latency,
        }
    }
    
    async fn get(&self, key: &str) -> Option<String> {
        // 模拟读取延迟
        sleep(self.read_latency).await;
        
        let data = self.data.read().await;
        data.get(key).cloned()
    }
    
    async fn put(&self, key: String, value: String) {
        let mut data = self.data.write().await;
        data.insert(key, value);
    }
    
    async fn load_sample_data(&self, count: usize) {
        let mut data = self.data.write().await;
        
        for i in 0..count {
            let key = format!("db_key{}", i);
            let value = format!("db_value{}", i);
            data.insert(key, value);
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    println!("=== 缓存模式示例 ===");
    
    // 创建缓存
    let lru_policy = LRUPolicy::new();
    let l1_cache: Arc<dyn Cache<String, String>> = Arc::new(
        MemoryCache::new(100, lru_policy)
    );
    
    // 创建后端数据库服务
    let db = Arc::new(DatabaseService::new(Duration::from_millis(200)));
    db.load_sample_data(1000).await;
    
    // 创建多级缓存
    let mut cache_system = MultiLevelCache::new();
    cache_system.add_level(l1_cache.clone(), "L1");
    
    // 缓存服务
    let cache_service = CacheService::new(cache_system, db.clone());
    
    // 基准测试
    println!("\n开始缓存基准测试...");
    
    // 创建一组键进行测试
    let mut test_keys = Vec::new();
    for i in 0..100 {
        test_keys.push(format!("db_key{}", i));
    }
    
    // 第一次访问（冷缓存）
    println!("\n冷缓存测试:");
    let start = Instant::now();
    
    for key in &test_keys {
        let result = cache_service.get(key).await;
        match &result {
            Some(value) => {/* println!("{} -> {}", key, value) */},
            None => println!("{} -> 未找到", key),
        }
    }
    
    let cold_duration = start.elapsed();
    println!("冷缓存访问耗时: {:?}, 平均每个键 {:?}",
             cold_duration,
             cold_duration / test_keys.len() as u32);
    
    // 第二次访问（热缓存）
    println!("\n热缓存测试:");
    let start = Instant::now();
    
    for key in &test_keys {
        let result = cache_service.get(key).await;
        match &result {
            Some(_) => {},
            None => println!("{} -> 未找到", key),
        }
    }
    
    let hot_duration = start.elapsed();
    println!("热缓存访问耗时: {:?}, 平均每个键 {:?}",
             hot_duration,
             hot_duration / test_keys.len() as u32);
    
    println!("\n性能提升: {:.2}x", cold_duration.as_secs_f64() / hot_duration.as_secs_f64());
    
    // 查看缓存统计
    println!("\n缓存统计:");
    let stats = cache_service.get_stats().await;
    
    for (name, stat) in stats {
        println!("{} 缓存: 大小={}/{}, 命中率={:.1}%, 命中={}, 未命中={}, 驱逐={}",
                 name, stat.size, stat.capacity,
                 stat.hit_rate() * 100.0,
                 stat.hits, stat.misses,
                 stat.evictions);
    }
    
    // 测试缓存过期
    println!("\n测试缓存过期:");
    
    let expire_key = "expire_test";
    cache_service.put(expire_key.to_string(), "短时间值".to_string(), Some(Duration::from_secs(2))).await;
    
    println!("初始值: {:?}", cache_service.get(expire_key).await);
    
    println!("等待过期...");
    sleep(Duration::from_secs(3)).await;
    
    println!("过期后: {:?}", cache_service.get(expire_key).await);
    
    // 测试缓存驱逐
    println!("\n测试缓存驱逐:");
    
    // 填充缓存到容量上限
    println!("填充缓存到容量上限...");
    for i in 0..150 {
        let key = format!("eviction_key{}", i);
        let value = format!("value{}", i);
        cache_service.put(key, value, None).await;
    }
    
    // 查看缓存统计
    let stats = cache_service.get_stats().await;
    for (name, stat) in stats {
        println!("{} 缓存: 大小={}/{}, 驱逐={}",
                 name, stat.size, stat.capacity, stat.evictions);
    }
}

struct CacheService {
    cache: MultiLevelCache<String, String>,
    db: Arc<DatabaseService>,
}

impl CacheService {
    fn new(cache: MultiLevelCache<String, String>, db: Arc<DatabaseService>) -> Self {
        Self { cache, db }
    }
    
    async fn get(&self, key: &str) -> Option<String> {
        // 先尝试从缓存获取
        if let Ok(Some((value, level))) = self.cache.get(key).await {
            //println!("缓存命中 ({}): {}", level, key);
            return Some(value);
        }
        
        // 缓存未命中，从数据库获取
        //println!("缓存未命中: {}", key);
        let db_result = self.db.get(key).await;
        
        // 如果数据库有结果，填充缓存
        if let Some(value) = &db_result {
            let _ = self.cache.put(key.to_string(), value.clone(), None).await;
        }
        
        db_result
    }
    
    async fn put(&self, key: String, value: String, ttl: Option<Duration>) {
        // 更新数据库
        self.db.put(key.clone(), value.clone()).await;
        
        // 更新缓存
        let _ = self.cache.put(key, value, ttl).await;
    }
    
    async fn get_stats(&self) -> HashMap<String, CacheStats> {
        self.cache.stats().await.unwrap_or_default()
    }
}

/*
<think>
我需要继续编写代码，展示缓存模式的实现。
我将完成缺失的部分，包括数据库服务、多级缓存实现，以及一些缓存策略的具体实现。

我将从DatabaseService开始，然后实现多级缓存系统，最后完成文件中的其他部分。
</think>
*/

struct DatabaseService {
    data: RwLock<HashMap<String, String>>,
    delay: Duration, // 模拟数据库访问延迟
}

impl DatabaseService {
    fn new(delay: Duration) -> Self {
        Self {
            data: RwLock::new(HashMap::new()),
            delay,
        }
    }
    
    // 加载示例数据
    async fn load_sample_data(&self, count: usize) {
        let mut data = self.data.write().await;
        for i in 0..count {
            let key = format!("db_key{}", i);
            let value = format!("数据库值{}", i);
            data.insert(key, value);
        }
    }
    
    // 获取数据
    async fn get(&self, key: &str) -> Option<String> {
        // 模拟数据库访问延迟
        sleep(self.delay).await;
        
        let data = self.data.read().await;
        data.get(key).cloned()
    }
    
    // 存储数据
    async fn put(&self, key: String, value: String) {
        // 模拟数据库访问延迟
        sleep(self.delay).await;
        
        let mut data = self.data.write().await;
        data.insert(key, value);
    }
}

// 多级缓存
struct MultiLevelCache<K, V> {
    levels: Vec<(Arc<dyn Cache<K, V>>, String)>,
}

impl<K, V> MultiLevelCache<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn new() -> Self {
        Self {
            levels: Vec::new(),
        }
    }
    
    // 添加缓存层级
    fn add_level(&mut self, cache: Arc<dyn Cache<K, V>>, name: &str) {
        self.levels.push((cache, name.to_string()));
    }
    
    // 从缓存获取值，返回值和命中的缓存层级
    async fn get(&self, key: &K) -> Result<Option<(V, String)>, String> {
        // 从第一级开始查找
        for (i, (cache, name)) in self.levels.iter().enumerate() {
            match cache.get(key).await {
                Ok(Some(value)) => {
                    // 命中当前层级缓存
                    
                    // 填充所有更高层级的缓存
                    for j in 0..i {
                        let (higher_cache, _) = &self.levels[j];
                        let _ = higher_cache.put(key.clone(), value.clone(), None).await;
                    }
                    
                    return Ok(Some((value, name.clone())));
                },
                Ok(None) => {
                    // 当前层级未命中，继续查找下一层
                    continue;
                },
                Err(e) => {
                    // 缓存错误，继续查找下一层
                    println!("缓存 {} 错误: {}", name, e);
                    continue;
                }
            }
        }
        
        // 所有层级都未命中
        Ok(None)
    }
    
    // 存储值到所有缓存层级
    async fn put(&self, key: K, value: V, ttl: Option<Duration>) -> Result<(), String> {
        let mut errors = Vec::new();
        
        // 写入所有缓存层级
        for (cache, name) in &self.levels {
            if let Err(e) = cache.put(key.clone(), value.clone(), ttl).await {
                errors.push(format!("缓存 {} 写入错误: {}", name, e));
            }
        }
        
        if errors.is_empty() {
            Ok(())
        } else {
            Err(errors.join("; "))
        }
    }
    
    // 从所有缓存层级移除值
    async fn remove(&self, key: &K) -> Result<bool, String> {
        let mut removed = false;
        let mut errors = Vec::new();
        
        // 从所有缓存层级移除
        for (cache, name) in &self.levels {
            match cache.remove(key).await {
                Ok(Some(_)) => {
                    removed = true;
                },
                Err(e) => {
                    errors.push(format!("缓存 {} 移除错误: {}", name, e));
                },
                _ => {}
            }
        }
        
        if errors.is_empty() {
            Ok(removed)
        } else {
            Err(errors.join("; "))
        }
    }
    
    // 获取所有层级的统计信息
    async fn stats(&self) -> Result<HashMap<String, CacheStats>, String> {
        let mut all_stats = HashMap::new();
        
        for (cache, name) in &self.levels {
            match cache.stats().await {
                Ok(stats) => {
                    all_stats.insert(name.clone(), stats);
                },
                Err(e) => {
                    return Err(format!("获取缓存 {} 统计信息错误: {}", name, e));
                }
            }
        }
        
        Ok(all_stats)
    }
}

// LRU策略实现
impl<K: Clone + Eq + Hash> LRUPolicy<K> {
    fn new() -> Self {
        Self {
            access_order: LinkedList::new(),
            key_positions: HashMap::new(),
        }
    }
}

// LFU策略实现
impl<K: Clone + Eq + Hash> LFUPolicy<K> {
    fn new() -> Self {
        Self {
            frequency_map: HashMap::new(),
            key_frequencies: HashMap::new(),
            min_frequency: 0,
        }
    }
}

impl<K: Clone + Eq + Hash> CachePolicy<K, V> for LFUPolicy<K> {
    fn add(&mut self, key: &K, _entry: &CacheEntry<V>) {
        let freq = 1;
        self.key_frequencies.insert(key.clone(), freq);
        
        let freq_list = self.frequency_map.entry(freq).or_insert_with(LinkedList::new);
        freq_list.push_back(key.clone());
        
        self.min_frequency = 1;
    }
    
    fn access(&mut self, key: &K, _entry: &CacheEntry<V>) {
        if let Some(old_freq) = self.key_frequencies.get(key).cloned() {
            // 从旧频率列表中移除
            if let Some(freq_list) = self.frequency_map.get_mut(&old_freq) {
                freq_list.retain(|k| k != key);
                
                // 如果最小频率的列表为空，更新最小频率
                if old_freq == self.min_frequency && freq_list.is_empty() {
                    self.min_frequency += 1;
                }
            }
            
            // 增加频率
            let new_freq = old_freq + 1;
            self.key_frequencies.insert(key.clone(), new_freq);
            
            // 添加到新频率列表
            let freq_list = self.frequency_map.entry(new_freq)
            .or_insert_with(LinkedList::new);
            freq_list.push_back(key.clone());
        }
    }
    
    fn evict(&mut self) -> Option<K> {
        // 从最小频率的列表中弹出第一个元素
        if let Some(freq_list) = self.frequency_map.get_mut(&self.min_frequency) {
            if let Some(key) = freq_list.pop_front() {
                self.key_frequencies.remove(&key);
                
                // 如果列表为空，查找下一个最小频率
                if freq_list.is_empty() {
                    let mut next_min = u64::MAX;
                    for freq in self.frequency_map.keys() {
                        if *freq > self.min_frequency && *freq < next_min && !self.frequency_map[freq].is_empty() {
                            next_min = *freq;
                        }
                    }
                    
                    if next_min != u64::MAX {
                        self.min_frequency = next_min;
                    }
                }
                
                return Some(key);
            }
        }
        
        None
    }
    
    fn remove(&mut self, key: &K) {
        if let Some(freq) = self.key_frequencies.remove(key) {
            if let Some(freq_list) = self.frequency_map.get_mut(&freq) {
                freq_list.retain(|k| k != key);
            }
        }
    }
    
    fn clear(&mut self) {
        self.frequency_map.clear();
        self.key_frequencies.clear();
        self.min_frequency = 0;
    }
}

// FIFO策略实现
impl<K: Clone + Eq + Hash> FIFOPolicy<K> {
    fn new() -> Self {
        Self {
            insertion_order: LinkedList::new(),
        }
    }
}

impl<K: Clone + Eq + Hash> CachePolicy<K, V> for FIFOPolicy<K> {
    fn add(&mut self, key: &K, _entry: &CacheEntry<V>) {
        self.insertion_order.push_back(key.clone());
    }
    
    fn access(&mut self, _key: &K, _entry: &CacheEntry<V>) {
        // FIFO不会因为访问而改变顺序
    }
    
    fn evict(&mut self) -> Option<K> {
        self.insertion_order.pop_front()
    }
    
    fn remove(&mut self, key: &K) {
        self.insertion_order.retain(|k| k != key);
    }
    
    fn clear(&mut self) {
        self.insertion_order.clear();
    }
}

// 添加一些缓存策略组合
struct CompositePolicy<K> {
    primary: Box<dyn CachePolicy<K, V>>,
    secondary: Box<dyn CachePolicy<K, V>>,
}

impl<K: Clone + Eq + Hash> CompositePolicy<K> {
    fn new(primary: Box<dyn CachePolicy<K, V>>, secondary: Box<dyn CachePolicy<K, V>>) -> Self {
        Self {
            primary,
            secondary,
        }
    }
}

impl<K: Clone + Eq + Hash> CachePolicy<K, V> for CompositePolicy<K> {
    fn add(&mut self, key: &K, entry: &CacheEntry<V>) {
        self.primary.add(key, entry);
        self.secondary.add(key, entry);
    }
    
    fn access(&mut self, key: &K, entry: &CacheEntry<V>) {
        self.primary.access(key, entry);
        self.secondary.access(key, entry);
    }
    
    fn evict(&mut self) -> Option<K> {
        // 先使用主策略，如果没有结果则使用次策略
        self.primary.evict().or_else(|| self.secondary.evict())
    }
    
    fn remove(&mut self, key: &K) {
        self.primary.remove(key);
        self.secondary.remove(key);
    }
    
    fn clear(&mut self) {
        self.primary.clear();
        self.secondary.clear();
    }
}
```

## 总结

在本指南中，我们探索了各种Rust并发设计模式，这些模式有助于构建高效、可靠和可扩展的并发系统。
这些模式按功能分为以下几类：

1. **基础模式**: 提供并发编程的基础构建块，如互斥锁、读写锁和信号量。
2. **任务分解模式**: 将大任务分解为可以并行执行的小任务。
3. **数据共享模式**: 安全地在并发环境中共享数据。
4. **同步模式**: 协调并发任务的执行。
5. **消息传递模式**: 通过消息在并发组件之间通信。
6. **异步模式**: 处理异步操作和事件驱动编程。
7. **服务协调模式**: 协调多个服务之间的交互。
8. **稳定性模式**: 提高系统在异常情况下的稳定性。
9. **分布式协调模式**: 协调分布式系统中的多个节点。
10. **可扩展性模式**: 使系统能够随着负载增加而扩展。

每个模式都包含详细的代码实现和示例，帮助你理解如何在实际项目中应用这些模式。
通过组合这些模式，你可以构建出高效、可靠和可扩展的并发系统。

这些设计模式是处理并发和异步编程中各种挑战的有效工具，它们代表了多年软件工程实践中提炼出的最佳实践。
根据你的具体需求，选择合适的模式可以显著提升系统的性能、可靠性和可维护性。

```text
<think>
已经完成了所有主要的设计模式讲解，接下来可以添加一些额外的信息，比如：

1. 设计模式的组合使用案例
2. 性能考量和最佳实践
3. 设计模式的选择指南
4. 常见陷阱和解决方案

让我继续这个内容，提供一些实用的建议和更高级的用例。
</think>
```

## 进阶技巧与最佳实践

### 模式组合

实际应用中，我们通常会组合多种设计模式来解决复杂问题。以下是一些常见的组合方式：

```rust
// 组合断路器和重试模式示例
struct ResilientService<T> {
    service: T,
    circuit_breaker: Arc<CircuitBreaker>,
    retry_policy: RetryPolicy,
}

impl<T: Service> ResilientService<T> {
    async fn call(&self, request: Request) -> Result<Response, Error> {
        self.circuit_breaker.execute(|| async {
            retry_with_policy(&self.retry_policy, || async {
                self.service.call(request.clone()).await
            }).await
        }).await
    }
}

// 组合工作池和背压模式
struct BackpressuredWorkerPool<T> {
    worker_pool: Arc<WorkerPool<T>>,
    limiter: Arc<RateLimiter>,
    queue: Arc<BoundedQueue<T>>,
}

impl<T: Send + 'static> BackpressuredWorkerPool<T> {
    async fn submit(&self, task: T) -> Result<(), PoolError> {
        // 应用速率限制
        self.limiter.acquire(1).await?;
        
        // 尝试将任务放入队列
        self.queue.push(task).await.map_err(|_| PoolError::QueueFull)?;
        
        Ok(())
    }
}
```

### 性能考量

在选择和实现并发模式时，需考虑以下性能因素：

1. **锁竞争**：频繁访问的共享数据会导致锁竞争，应考虑以下策略：
   - 使用读写锁代替互斥锁
   - 使用细粒度锁而非粗粒度锁
   - 考虑无锁数据结构

2. **任务粒度**：确保并行任务的粒度合适：
   - 任务太小会导致线程管理开销超过并行收益
   - 任务太大会导致负载不均衡

3. **内存使用**：
   - 避免过多的克隆操作
   - 使用引用计数（Arc）而非深拷贝
   - 合理使用生命周期注解减少不必要的内存复制

4. **异步开销**：
   - 避免在异步上下文中执行CPU密集型任务
   - 警惕过多的`.await`点导致的任务切换开销
   - 监控异步运行时的任务和线程数量

### 常见陷阱与解决方案

1. **死锁**

   ```rust
   // 错误示例 - 可能导致死锁
   async fn transfer(
       from: &Mutex<Account>,
       to: &Mutex<Account>,
       amount: u64
   ) -> Result<(), Error> {
       let mut from_account = from.lock().await;
       // 可能在这里发生死锁，如果另一个线程以相反顺序获取锁
       let mut to_account = to.lock().await;
       
       from_account.withdraw(amount)?;
       to_account.deposit(amount);
       Ok(())
   }
   
   // 修正 - 始终按固定顺序获取锁
   async fn transfer_safe(
       from: &Mutex<Account>,
       to: &Mutex<Account>,
       amount: u64
   ) -> Result<(), Error> {
       // 确保总是按照内存地址顺序获取锁
       let (first, second) = if std::ptr::addr_of!(from) < std::ptr::addr_of!(to) {
           (from, to)
       } else {
           (to, from)
       };
       
       let mut first_account = first.lock().await;
       let mut second_account = second.lock().await;
       
       if std::ptr::eq(first, from) {
           first_account.withdraw(amount)?;
           second_account.deposit(amount);
       } else {
           second_account.withdraw(amount)?;
           first_account.deposit(amount);
       }
       
       Ok(())
   }
   ```

2. **并行任务数量过多**

   ```rust
   // 错误示例 - 可能创建过多任务
   async fn process_items(items: Vec<Item>) -> Vec<Result<Output, Error>> {
       let mut futures = Vec::new();
       for item in items {
           futures.push(tokio::spawn(async move {
               process_item(item).await
           }));
       }
       
       // 收集结果
       let mut results = Vec::new();
       for future in futures {
           results.push(future.await??);
       }
       results
   }
   
   // 修正 - 使用并发限制
   async fn process_items_limited(items: Vec<Item>) -> Vec<Result<Output, Error>> {
       // 限制并发度为系统CPU核心数
       let cpu_count = num_cpus::get();
       let semaphore = Arc::new(Semaphore::new(cpu_count));
       
       let mut futures = Vec::new();
       for item in items {
           let permit = semaphore.clone().acquire_owned().await.unwrap();
           futures.push(tokio::spawn(async move {
               let result = process_item(item).await;
               drop(permit); // 释放信号量
               result
           }));
       }
       
       // 收集结果
       let mut results = Vec::new();
       for future in futures {
           results.push(future.await??);
       }
       results
   }
   ```

3. **忘记处理任务取消**

   ```rust
   // 错误示例 - 未处理任务取消
   async fn do_work() -> Result<(), Error> {
       let data = fetch_data().await?;
       process_data(data).await?;
       save_result().await?;
       Ok(())
   }
   
   // 修正 - 正确处理任务取消
   async fn do_work_with_cleanup() -> Result<(), Error> {
       // 使用Drop来确保在任务取消时清理资源
       struct CleanupGuard {
           temp_data: Option<TempData>,
       }
       
       impl Drop for CleanupGuard {
           fn drop(&mut self) {
               if let Some(data) = self.temp_data.take() {
                   // 启动一个阻塞任务来执行清理
                   tokio::task::spawn_blocking(move || {
                       data.cleanup();
                   });
               }
           }
       }
       
       let data = fetch_data().await?;
       
       let guard = CleanupGuard {
           temp_data: Some(TempData::new(&data)),
       };
       
       let result = process_data(data).await?;
       save_result(result).await?;
       
       // 正常完成时手动清理
       if let Some(data) = guard.temp_data.take() {
           data.cleanup();
       }
       
       Ok(())
   }
   ```

### 设计模式选择指南

根据应用场景选择合适的并发模式：

1. **高吞吐量数据处理**
   - 工作者池模式
   - 分片模式
   - 流水线模式

2. **实时响应系统**
   - 异步任务模式
   - 事件驱动模式
   - 背压模式

3. **高可用性系统**
   - 断路器模式
   - 超时模式
   - 重试模式
   - 隔板模式

4. **分布式系统**
   - 领导者选举模式
   - 分布式锁模式
   - 复制模式
   - 分片模式

5. **资源受限环境**
   - 对象池模式
   - 资源限制模式
   - 缓存模式

## 结语

Rust的类型系统和所有权模型使其成为实现并发设计模式的理想语言。
这些模式不仅可以帮助你构建高效、可靠的并发系统，还可以通过编译时检查防止许多常见的并发错误。

希望本指南能帮助你在实际项目中应用这些设计模式，构建出更加健壮、高效的并发系统。
随着Rust生态系统的不断发展，这些模式也将继续演化，我们建议持续关注社区最佳实践的更新。

在实际应用中，请记住没有完美的设计模式，每种模式都有其适用场景和局限性。
选择合适的模式组合，并根据你的具体需求进行调整，将帮助你构建出最适合你应用场景的并发系统。

祝你的并发编程之旅愉快而富有成效！
