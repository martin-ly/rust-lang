use anyhow::Result;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{RwLock, Semaphore, mpsc};
use tokio::time::sleep;
use tracing::{info, error};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
//use std::sync::atomic::{AtomicUsize, AtomicU64, Ordering};

/// 2025å¹´å¼‚æ­¥æœºå™¨å­¦ä¹ æ¨¡å¼æ¼”ç¤º
/// å±•ç¤ºæœ€æ–°çš„å¼‚æ­¥æœºå™¨å­¦ä¹ ç¼–ç¨‹æ¨¡å¼å’Œæœ€ä½³å®è·µ

/// 1. å¼‚æ­¥æ¨¡å‹è®­ç»ƒç®¡ç†å™¨
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingConfig {
    pub model_name: String,
    pub learning_rate: f64,
    pub batch_size: usize,
    pub epochs: usize,
    pub validation_split: f64,
    pub early_stopping_patience: usize,
}

impl Default for TrainingConfig {
    fn default() -> Self {
        Self {
            model_name: "default_model".to_string(),
            learning_rate: 0.001,
            batch_size: 32,
            epochs: 100,
            validation_split: 0.2,
            early_stopping_patience: 10,
        }
    }
}

#[derive(Debug, Clone)]
pub struct AsyncModelTrainer {
    config: TrainingConfig,
    training_data: Arc<RwLock<Vec<TrainingSample>>>,
    model_state: Arc<RwLock<ModelState>>,
    training_stats: Arc<RwLock<TrainingStats>>,
    progress_sender: mpsc::UnboundedSender<TrainingProgress>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingSample {
    pub id: String,
    pub features: Vec<f64>,
    pub label: f64,
    pub weight: Option<f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelState {
    pub weights: Vec<f64>,
    pub bias: f64,
    pub version: u64,
    pub accuracy: f64,
    pub loss: f64,
    pub last_updated: u64,
}

#[derive(Debug, Clone, Default)]
pub struct TrainingStats {
    pub total_samples: usize,
    pub processed_samples: usize,
    pub training_epochs: usize,
    pub validation_epochs: usize,
    pub best_accuracy: f64,
    pub best_loss: f64,
    pub training_time: Duration,
}

#[derive(Debug, Clone)]
pub struct TrainingProgress {
    pub epoch: usize,
    pub accuracy: f64,
    pub loss: f64,
    pub validation_accuracy: f64,
    pub validation_loss: f64,
    pub timestamp: u64,
}

impl AsyncModelTrainer {
    pub fn new(config: TrainingConfig) -> (Self, mpsc::UnboundedReceiver<TrainingProgress>) {
        let (progress_sender, progress_receiver) = mpsc::unbounded_channel();
        
        let trainer = Self {
            config,
            training_data: Arc::new(RwLock::new(Vec::new())),
            model_state: Arc::new(RwLock::new(ModelState {
                weights: vec![0.0; 10], // å‡è®¾10ä¸ªç‰¹å¾
                bias: 0.0,
                version: 0,
                accuracy: 0.0,
                loss: f64::INFINITY,
                last_updated: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs(),
            })),
            training_stats: Arc::new(RwLock::new(TrainingStats::default())),
            progress_sender,
        };
        
        (trainer, progress_receiver)
    }

    pub async fn add_training_data(&self, samples: Vec<TrainingSample>) -> Result<()> {
        let mut data = self.training_data.write().await;
        data.extend(samples);
        
        let mut stats = self.training_stats.write().await;
        stats.total_samples = data.len();
        
        info!("æ·»åŠ è®­ç»ƒæ•°æ®ï¼Œæ€»æ ·æœ¬æ•°: {}", stats.total_samples);
        Ok(())
    }

    pub async fn start_training(&self) -> Result<()> {
        info!("å¼€å§‹æ¨¡å‹è®­ç»ƒ: {}", self.config.model_name);
        let start_time = Instant::now();
        
        let data = self.training_data.read().await;
        let total_samples = data.len();
        
        if total_samples == 0 {
            return Err(anyhow::anyhow!("æ²¡æœ‰è®­ç»ƒæ•°æ®"));
        }
        
        let validation_size = (total_samples as f64 * self.config.validation_split) as usize;
        let training_size = total_samples - validation_size;
        
        info!("è®­ç»ƒé›†å¤§å°: {}, éªŒè¯é›†å¤§å°: {}", training_size, validation_size);
        
        let mut best_accuracy = 0.0;
        let mut patience_counter = 0;
        
        for epoch in 0..self.config.epochs {
            // æ¨¡æ‹Ÿè®­ç»ƒä¸€ä¸ªepoch
            let (train_acc, train_loss) = self.train_epoch(epoch, &data[..training_size]).await?;
            
            // æ¨¡æ‹ŸéªŒè¯
            let (val_acc, val_loss) = self.validate_epoch(&data[training_size..]).await?;
            
            // æ›´æ–°æ¨¡å‹çŠ¶æ€
            let mut model_state = self.model_state.write().await;
            model_state.accuracy = train_acc;
            model_state.loss = train_loss;
            model_state.version += 1;
            model_state.last_updated = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs();
            
            // å‘é€è¿›åº¦æ›´æ–°
            let progress = TrainingProgress {
                epoch,
                accuracy: train_acc,
                loss: train_loss,
                validation_accuracy: val_acc,
                validation_loss: val_loss,
                timestamp: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs(),
            };
            
            let _ = self.progress_sender.send(progress);
            
            // æ—©åœæ£€æŸ¥
            if val_acc > best_accuracy {
                best_accuracy = val_acc;
                patience_counter = 0;
            } else {
                patience_counter += 1;
                if patience_counter >= self.config.early_stopping_patience {
                    info!("æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {} epoch åœæ­¢è®­ç»ƒ", epoch);
                    break;
                }
            }
            
            info!("Epoch {}: è®­ç»ƒå‡†ç¡®ç‡: {:.4}, éªŒè¯å‡†ç¡®ç‡: {:.4}", epoch, train_acc, val_acc);
        }
        
        let training_time = start_time.elapsed();
        let mut stats = self.training_stats.write().await;
        stats.training_epochs = self.config.epochs;
        stats.best_accuracy = best_accuracy;
        stats.training_time = training_time;
        
        info!("è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {:?}, æœ€ä½³å‡†ç¡®ç‡: {:.4}", training_time, best_accuracy);
        Ok(())
    }

    #[allow(unused_variables)]
    async fn train_epoch(&self, epoch: usize, data: &[TrainingSample]) -> Result<(f64, f64)> {
        // æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        sleep(Duration::from_millis(100)).await;
        
        // æ¨¡æ‹Ÿæ¢¯åº¦ä¸‹é™æ›´æ–°
        let mut model_state = self.model_state.write().await;
        for (i, weight) in model_state.weights.iter_mut().enumerate() {
            *weight += self.config.learning_rate * (0.1 - *weight);
        }
        model_state.bias += self.config.learning_rate * (0.05 - model_state.bias);
        
        // æ¨¡æ‹Ÿè®¡ç®—å‡†ç¡®ç‡å’ŒæŸå¤±
        let mut correct = 0;
        let mut total_loss = 0.0;
        
        for sample in data.iter().take(self.config.batch_size) {
            let prediction = self.predict(&sample.features, &model_state).await;
            let error = (prediction - sample.label).abs();
            total_loss += error;
            
            if error < 0.5 {
                correct += 1;
            }
        }
        
        let accuracy = correct as f64 / data.len().min(self.config.batch_size) as f64;
        let loss = total_loss / data.len().min(self.config.batch_size) as f64;
        
        Ok((accuracy, loss))
    }

    #[allow(unused_variables)]
    async fn validate_epoch(&self, data: &[TrainingSample]) -> Result<(f64, f64)> {
        // æ¨¡æ‹ŸéªŒè¯è¿‡ç¨‹
        sleep(Duration::from_millis(50)).await;
        
        let model_state = self.model_state.read().await;
        let mut correct = 0;
        let mut total_loss = 0.0;
        
        for sample in data.iter().take(self.config.batch_size / 2) {
            let prediction = self.predict(&sample.features, &model_state).await;
            let error = (prediction - sample.label).abs();
            total_loss += error;
            
            if error < 0.5 {
                correct += 1;
            }
        }
        
        let accuracy = correct as f64 / data.len().min(self.config.batch_size / 2) as f64;
        let loss = total_loss / data.len().min(self.config.batch_size / 2) as f64;
        
        Ok((accuracy, loss))
    }

    async fn predict(&self, features: &[f64], model_state: &ModelState) -> f64 {
        let mut result = model_state.bias;
        for (feature, weight) in features.iter().zip(model_state.weights.iter()) {
            result += feature * weight;
        }
        // ç®€åŒ–çš„æ¿€æ´»å‡½æ•°
        result.tanh()
    }

    pub async fn get_model_state(&self) -> ModelState {
        self.model_state.read().await.clone()
    }

    pub async fn get_training_stats(&self) -> TrainingStats {
        self.training_stats.read().await.clone()
    }
}

/// 2. å¼‚æ­¥æ¨¡å‹æ¨ç†æœåŠ¡
#[allow(dead_code)]
#[derive(Debug, Clone)]
pub struct AsyncInferenceService {
    models: Arc<RwLock<HashMap<String, ModelState>>>,
    inference_queue: Arc<RwLock<Vec<InferenceRequest>>>,
    inference_stats: Arc<RwLock<InferenceStats>>,
    max_concurrent_requests: usize,
    semaphore: Arc<Semaphore>,
}

#[allow(dead_code)]
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceRequest {
    pub id: String,
    pub model_name: String,
    pub features: Vec<f64>,
    pub priority: InferencePriority,
    pub created_at: u64,
    pub timeout: Duration,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub enum InferencePriority {
    Low,
    Normal,
    High,
    Critical,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceResult {
    pub request_id: String,
    pub prediction: f64,
    pub confidence: f64,
    pub processing_time: Duration,
    pub model_version: u64,
}

#[derive(Debug, Clone, Default)]
pub struct InferenceStats {
    pub total_requests: usize,
    pub successful_requests: usize,
    pub failed_requests: usize,
    pub average_processing_time: Duration,
    pub queue_size: usize,
}

impl AsyncInferenceService {
    pub fn new(max_concurrent_requests: usize) -> Self {
        Self {
            models: Arc::new(RwLock::new(HashMap::new())),
            inference_queue: Arc::new(RwLock::new(Vec::new())),
            inference_stats: Arc::new(RwLock::new(InferenceStats::default())),
            max_concurrent_requests,
            semaphore: Arc::new(Semaphore::new(max_concurrent_requests)),
        }
    }

    pub async fn register_model(&self, model_name: String, model_state: ModelState) -> Result<()> {
        let mut models = self.models.write().await;
        models.insert(model_name.clone(), model_state);
        info!("æ³¨å†Œæ¨¡å‹: {}", model_name);
        Ok(())
    }

    pub async fn submit_inference_request(&self, request: InferenceRequest) -> Result<String> {
        let mut queue = self.inference_queue.write().await;
        queue.push(request.clone());
        
        let mut stats = self.inference_stats.write().await;
        stats.total_requests += 1;
        stats.queue_size = queue.len();
        
        info!("æäº¤æ¨ç†è¯·æ±‚: {} (æ¨¡å‹: {})", request.id, request.model_name);
        
        // å¯åŠ¨å¼‚æ­¥å¤„ç†
        let service_clone = self.clone();
        let request_id = request.id.clone();
        tokio::spawn(async move {
            if let Err(e) = service_clone.process_inference_request(request).await {
                error!("æ¨ç†è¯·æ±‚å¤„ç†å¤±è´¥: {}", e);
            }
        });
        
        Ok(request_id)
    }

    async fn process_inference_request(&self, request: InferenceRequest) -> Result<InferenceResult> {
        let _permit = self.semaphore.acquire().await.unwrap();
        let start_time = Instant::now();
        
        // è·å–æ¨¡å‹
        let models = self.models.read().await;
        let model_state = models.get(&request.model_name)
            .ok_or_else(|| anyhow::anyhow!("æ¨¡å‹ {} æœªæ‰¾åˆ°", request.model_name))?;
        
        // æ‰§è¡Œæ¨ç†
        let prediction = self.run_inference(&request.features, model_state).await;
        
        // è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰
        let confidence = 1.0 - (prediction.abs() * 0.1);
        
        let processing_time = start_time.elapsed();
        let result = InferenceResult {
            request_id: request.id.clone(),
            prediction,
            confidence,
            processing_time,
            model_version: model_state.version,
        };
        
        // æ›´æ–°ç»Ÿè®¡
        let mut stats = self.inference_stats.write().await;
        stats.successful_requests += 1;
        stats.average_processing_time = Duration::from_millis(
            ((stats.average_processing_time.as_millis() + processing_time.as_millis()) / 2) as u64
        );
        
        info!("æ¨ç†å®Œæˆ: {} -> {:.4} (ç½®ä¿¡åº¦: {:.4})", request.id, prediction, confidence);
        Ok(result)
    }

    async fn run_inference(&self, features: &[f64], model_state: &ModelState) -> f64 {
        // æ¨¡æ‹Ÿæ¨ç†è®¡ç®—
        sleep(Duration::from_millis(10)).await;
        
        let mut result = model_state.bias;
        for (feature, weight) in features.iter().zip(model_state.weights.iter()) {
            result += feature * weight;
        }
        
        result.tanh()
    }

    pub async fn get_inference_stats(&self) -> InferenceStats {
        let mut stats = self.inference_stats.read().await.clone();
        stats.queue_size = self.inference_queue.read().await.len();
        stats
    }
}

/// 3. å¼‚æ­¥æ•°æ®å¤„ç†ç®¡é“
#[derive(Debug, Clone)]
pub struct AsyncDataPipeline {
    stages: Arc<RwLock<Vec<PipelineStage>>>,
    data_queue: Arc<RwLock<Vec<DataBatch>>>,
    pipeline_stats: Arc<RwLock<PipelineStats>>,
}

#[derive(Debug, Clone)]
pub struct PipelineStage {
    pub id: String,
    pub name: String,
    pub stage_type: StageType,
    pub config: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub enum StageType {
    Loader,
    Transformer,
    Filter,
    Aggregator,
    Sink,
}

#[derive(Debug, Clone)]
pub struct DataBatch {
    pub id: String,
    pub data: Vec<HashMap<String, f64>>,
    pub metadata: HashMap<String, String>,
    pub created_at: u64,
}

#[derive(Debug, Clone, Default)]
pub struct PipelineStats {
    pub total_batches: usize,
    pub processed_batches: usize,
    pub failed_batches: usize,
    pub average_processing_time: Duration,
}

impl AsyncDataPipeline {
    pub fn new() -> Self {
        Self {
            stages: Arc::new(RwLock::new(Vec::new())),
            data_queue: Arc::new(RwLock::new(Vec::new())),
            pipeline_stats: Arc::new(RwLock::new(PipelineStats::default())),
        }
    }

    pub async fn add_stage(&self, stage: PipelineStage) -> Result<()> {
        let mut stages = self.stages.write().await;
        stages.push(stage.clone());
        info!("æ·»åŠ ç®¡é“é˜¶æ®µ: {} ({})", stage.name, stage.id);
        Ok(())
    }

    pub async fn process_data(&self, batch: DataBatch) -> Result<()> {
        let start_time = Instant::now();
        
        // å°†æ•°æ®æ·»åŠ åˆ°é˜Ÿåˆ—
        {
            let mut queue = self.data_queue.write().await;
            queue.push(batch.clone());
        }
        
        // è·å–æ‰€æœ‰é˜¶æ®µ
        let stages = self.stages.read().await;
        let mut current_data = batch.data;
        
        // æŒ‰é¡ºåºå¤„ç†æ¯ä¸ªé˜¶æ®µ
        for stage in stages.iter() {
            current_data = self.execute_stage(&stage, current_data).await?;
            info!("é˜¶æ®µ {} å¤„ç†å®Œæˆï¼Œè¾“å‡º {} æ¡è®°å½•", stage.name, current_data.len());
        }
        
        let processing_time = start_time.elapsed();
        
        // æ›´æ–°ç»Ÿè®¡
        let mut stats = self.pipeline_stats.write().await;
        stats.processed_batches += 1;
        stats.average_processing_time = Duration::from_millis(
            ((stats.average_processing_time.as_millis() + processing_time.as_millis()) / 2) as u64
        );
        
        info!("æ•°æ®å¤„ç†å®Œæˆï¼Œè€—æ—¶: {:?}", processing_time);
        Ok(())
    }

    async fn execute_stage(&self, stage: &PipelineStage, data: Vec<HashMap<String, f64>>) -> Result<Vec<HashMap<String, f64>>> {
        match stage.stage_type {
            StageType::Loader => self.execute_loader_stage(data).await,
            StageType::Transformer => self.execute_transformer_stage(data, &stage.config).await,
            StageType::Filter => self.execute_filter_stage(data, &stage.config).await,
            StageType::Aggregator => self.execute_aggregator_stage(data, &stage.config).await,
            StageType::Sink => self.execute_sink_stage(data).await,
        }
    }

    async fn execute_loader_stage(&self, data: Vec<HashMap<String, f64>>) -> Result<Vec<HashMap<String, f64>>> {
        // æ¨¡æ‹Ÿæ•°æ®åŠ è½½
        sleep(Duration::from_millis(20)).await;
        info!("åŠ è½½äº† {} æ¡è®°å½•", data.len());
        Ok(data)
    }

    async fn execute_transformer_stage(&self, data: Vec<HashMap<String, f64>>, config: &HashMap<String, String>) -> Result<Vec<HashMap<String, f64>>> {
        // æ¨¡æ‹Ÿæ•°æ®è½¬æ¢
        sleep(Duration::from_millis(30)).await;
        
        let mut transformed_data = Vec::new();
        for record in data {
            let mut transformed_record = record.clone();
            
            // æ·»åŠ æ–°ç‰¹å¾
            if let Some(feature_name) = config.get("new_feature") {
                let value = transformed_record.values().sum::<f64>() / transformed_record.len() as f64;
                transformed_record.insert(feature_name.clone(), value);
            }
            
            transformed_data.push(transformed_record);
        }
        
        info!("è½¬æ¢äº† {} æ¡è®°å½•", transformed_data.len());
        Ok(transformed_data)
    }

    #[allow(unused_variables)]
    async fn execute_filter_stage(&self, data: Vec<HashMap<String, f64>>, config: &HashMap<String, String>) -> Result<Vec<HashMap<String, f64>>> {
        // æ¨¡æ‹Ÿæ•°æ®è¿‡æ»¤
        sleep(Duration::from_millis(15)).await;
        
        let filtered_data: Vec<HashMap<String, f64>> = data
            .into_iter()
            .filter(|record| {
                // ç®€å•çš„è¿‡æ»¤é€»è¾‘
                record.values().any(|&v| v > 0.0)
            })
            .collect();
        
        info!("è¿‡æ»¤åå‰©ä½™ {} æ¡è®°å½•", filtered_data.len());
        Ok(filtered_data)
    }

    #[allow(unused_variables)]
    async fn execute_aggregator_stage(&self, data: Vec<HashMap<String, f64>>, config: &HashMap<String, String>) -> Result<Vec<HashMap<String, f64>>> {
        // æ¨¡æ‹Ÿæ•°æ®èšåˆ
        sleep(Duration::from_millis(25)).await;
        
        let mut aggregated_data = Vec::new();
        if !data.is_empty() {
            let mut aggregated_record = HashMap::new();
            
            // è®¡ç®—å¹³å‡å€¼
            for key in data[0].keys() {
                let sum: f64 = data.iter().map(|record| record.get(key).unwrap_or(&0.0)).sum();
                let avg = sum / data.len() as f64;
                aggregated_record.insert(key.clone(), avg);
            }
            
            aggregated_data.push(aggregated_record);
        }
        
        info!("èšåˆåç”Ÿæˆ {} æ¡è®°å½•", aggregated_data.len());
        Ok(aggregated_data)
    }

    #[allow(unused_variables)]
    async fn execute_sink_stage(&self, data: Vec<HashMap<String, f64>>) -> Result<Vec<HashMap<String, f64>>> {
        // æ¨¡æ‹Ÿæ•°æ®è¾“å‡º
        sleep(Duration::from_millis(10)).await;
        info!("è¾“å‡º {} æ¡è®°å½•", data.len());
        Ok(data)
    }

    pub async fn get_pipeline_stats(&self) -> PipelineStats {
        self.pipeline_stats.read().await.clone()
    }
}

#[tokio::main]
#[allow(unused_variables)]
async fn main() -> Result<()> {
    tracing_subscriber::fmt::init();
    
    info!("ğŸš€ å¼€å§‹ 2025 å¹´å¼‚æ­¥æœºå™¨å­¦ä¹ æ¨¡å¼æ¼”ç¤º");

    // 1. æ¼”ç¤ºå¼‚æ­¥æ¨¡å‹è®­ç»ƒ
    info!("ğŸ§  æ¼”ç¤ºå¼‚æ­¥æ¨¡å‹è®­ç»ƒç®¡ç†å™¨");
    let config = TrainingConfig {
        model_name: "user_behavior_model".to_string(),
        learning_rate: 0.001,
        batch_size: 16,
        epochs: 20,
        validation_split: 0.2,
        early_stopping_patience: 5,
    };
    
    let (trainer, mut progress_receiver) = AsyncModelTrainer::new(config);
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®
    let mut training_samples = Vec::new();
    for i in 0..100 {
        let features = (0..10).map(|_| rand::random::<f64>()).collect();
        let label = if i % 3 == 0 { 1.0 } else { 0.0 };
        
        training_samples.push(TrainingSample {
            id: format!("sample_{}", i),
            features,
            label,
            weight: Some(1.0),
        });
    }
    
    trainer.add_training_data(training_samples).await?;
    
    // å¯åŠ¨è®­ç»ƒè¿›åº¦ç›‘æ§
    let trainer_clone = trainer.clone();
    let progress_task = tokio::spawn(async move {
        while let Some(progress) = progress_receiver.recv().await {
            info!("è®­ç»ƒè¿›åº¦ - Epoch {}: å‡†ç¡®ç‡ {:.4}, æŸå¤± {:.4}", 
                  progress.epoch, progress.accuracy, progress.loss);
        }
    });
    
    // å¼€å§‹è®­ç»ƒ
    trainer.start_training().await?;
    drop(progress_task);
    
    let training_stats = trainer.get_training_stats().await;
    info!("è®­ç»ƒç»Ÿè®¡: æ€»æ ·æœ¬ {}, è®­ç»ƒæ—¶é—´ {:?}, æœ€ä½³å‡†ç¡®ç‡ {:.4}", 
          training_stats.total_samples, training_stats.training_time, training_stats.best_accuracy);

    // 2. æ¼”ç¤ºå¼‚æ­¥æ¨¡å‹æ¨ç†æœåŠ¡
    info!("ğŸ”® æ¼”ç¤ºå¼‚æ­¥æ¨¡å‹æ¨ç†æœåŠ¡");
    let inference_service = AsyncInferenceService::new(5);
    
    // æ³¨å†Œè®­ç»ƒå¥½çš„æ¨¡å‹
    let model_state = trainer.get_model_state().await;
    inference_service.register_model("user_behavior_model".to_string(), model_state).await?;
    
    // æäº¤æ¨ç†è¯·æ±‚
    let request_handles: Vec<String> = Vec::new();
    for i in 0..20 {
        let features = (0..10).map(|_| rand::random::<f64>()).collect();
        let request = InferenceRequest {
            id: format!("request_{}", i),
            model_name: "user_behavior_model".to_string(),
            features,
            priority: if i % 5 == 0 { InferencePriority::High } else { InferencePriority::Normal },
            created_at: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs(),
            timeout: Duration::from_secs(30),
        };
        
        inference_service.submit_inference_request(request).await?;
    }
    
    // ç­‰å¾…å¤„ç†å®Œæˆ
    sleep(Duration::from_millis(500)).await;
    
    let inference_stats = inference_service.get_inference_stats().await;
    info!("æ¨ç†ç»Ÿè®¡: æ€»è¯·æ±‚ {}, æˆåŠŸ {}, å¤±è´¥ {}, å¹³å‡å¤„ç†æ—¶é—´ {:?}", 
          inference_stats.total_requests, inference_stats.successful_requests, 
          inference_stats.failed_requests, inference_stats.average_processing_time);

    // 3. æ¼”ç¤ºå¼‚æ­¥æ•°æ®å¤„ç†ç®¡é“
    info!("ğŸ”„ æ¼”ç¤ºå¼‚æ­¥æ•°æ®å¤„ç†ç®¡é“");
    let pipeline = AsyncDataPipeline::new();
    
    // æ·»åŠ ç®¡é“é˜¶æ®µ
    pipeline.add_stage(PipelineStage {
        id: "loader".to_string(),
        name: "æ•°æ®åŠ è½½å™¨".to_string(),
        stage_type: StageType::Loader,
        config: HashMap::new(),
    }).await?;
    
    pipeline.add_stage(PipelineStage {
        id: "transformer".to_string(),
        name: "æ•°æ®è½¬æ¢å™¨".to_string(),
        stage_type: StageType::Transformer,
        config: [("new_feature".to_string(), "computed_feature".to_string())].iter().cloned().collect(),
    }).await?;
    
    pipeline.add_stage(PipelineStage {
        id: "filter".to_string(),
        name: "æ•°æ®è¿‡æ»¤å™¨".to_string(),
        stage_type: StageType::Filter,
        config: HashMap::new(),
    }).await?;
    
    pipeline.add_stage(PipelineStage {
        id: "aggregator".to_string(),
        name: "æ•°æ®èšåˆå™¨".to_string(),
        stage_type: StageType::Aggregator,
        config: HashMap::new(),
    }).await?;
    
    pipeline.add_stage(PipelineStage {
        id: "sink".to_string(),
        name: "æ•°æ®è¾“å‡ºå™¨".to_string(),
        stage_type: StageType::Sink,
        config: HashMap::new(),
    }).await?;
    
    // å¤„ç†æ•°æ®æ‰¹æ¬¡
    for batch_id in 0..5 {
        let mut data = Vec::new();
        for i in 0..10 {
            let mut record = HashMap::new();
            record.insert("feature_1".to_string(), rand::random::<f64>());
            record.insert("feature_2".to_string(), rand::random::<f64>());
            record.insert("feature_3".to_string(), rand::random::<f64>());
            data.push(record);
        }
        
        let batch = DataBatch {
            id: format!("batch_{}", batch_id),
            data,
            metadata: HashMap::new(),
            created_at: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs(),
        };
        
        pipeline.process_data(batch).await?;
    }
    
    let pipeline_stats = pipeline.get_pipeline_stats().await;
    info!("ç®¡é“ç»Ÿè®¡: å¤„ç†æ‰¹æ¬¡ {}, å¹³å‡å¤„ç†æ—¶é—´ {:?}", 
          pipeline_stats.processed_batches, pipeline_stats.average_processing_time);

    info!("âœ… 2025 å¹´å¼‚æ­¥æœºå™¨å­¦ä¹ æ¨¡å¼æ¼”ç¤ºå®Œæˆ!");
    
    Ok(())
}
