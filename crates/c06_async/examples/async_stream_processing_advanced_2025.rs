use anyhow::Result;
use futures::{Stream, StreamExt, stream};
use std::pin::Pin;
//use std::task::{Context, Poll};
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, RwLock, Semaphore};
use tokio::time::{sleep};
use tracing::{info, warn, debug};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::collections::HashMap;

/// 2025å¹´é«˜çº§å¼‚æ­¥æµå¤„ç†æ¼”ç¤º
/// å±•ç¤ºæœ€æ–°çš„å¼‚æ­¥æµå¤„ç†æŠ€æœ¯å’Œæ¨¡å¼

/// 1. å¼‚æ­¥æµå¤„ç†å™¨åŸºç¡€ç»“æ„
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StreamEvent {
    pub id: u64,
    pub timestamp: u64,
    pub data: String,
    pub event_type: EventType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum EventType {
    Data,
    Control,
    Error,
    Heartbeat,
}

/// å¼‚æ­¥æµå¤„ç†å™¨
pub struct AsyncStreamProcessor {
    input_stream: Pin<Box<dyn Stream<Item = StreamEvent> + Send>>,
    processors: Vec<StreamTransform>,
    buffer_size: usize,
    metrics: Arc<RwLock<StreamMetrics>>,
}

#[derive(Debug, Default, Clone)]
pub struct StreamMetrics {
    pub processed_count: u64,
    pub error_count: u64,
    pub throughput_per_second: f64,
    pub average_latency: Duration,
    pub last_processed_time: Option<Instant>,
}

/// æµè½¬æ¢å™¨æšä¸¾
#[derive(Clone)]
pub enum StreamTransform {
    DataTransformer(AsyncDataTransformer),
    ErrorFilter(AsyncErrorFilter),
    EventEnricher(AsyncEventEnricher),
}

impl StreamTransform {
    pub async fn transform(&mut self, input: StreamEvent) -> Result<Option<StreamEvent>> {
        match self {
            StreamTransform::DataTransformer(transformer) => transformer.transform(input).await,
            StreamTransform::ErrorFilter(filter) => filter.transform(input).await,
            StreamTransform::EventEnricher(enricher) => enricher.transform(input).await,
        }
    }

    pub fn name(&self) -> &str {
        match self {
            StreamTransform::DataTransformer(transformer) => transformer.name(),
            StreamTransform::ErrorFilter(filter) => filter.name(),
            StreamTransform::EventEnricher(enricher) => enricher.name(),
        }
    }
}

/// æµè½¬æ¢å™¨trait (ä¿ç•™ç”¨äºå…·ä½“å®ç°)
pub trait StreamTransformImpl: Send + Sync + Clone {
    fn transform(&mut self, input: StreamEvent) -> impl std::future::Future<Output = Result<Option<StreamEvent>>> + Send;
    fn name(&self) -> &str;
}

impl AsyncStreamProcessor {
    pub fn new(input_stream: impl Stream<Item = StreamEvent> + Send + 'static) -> Self {
        Self {
            input_stream: Box::pin(input_stream),
            processors: Vec::new(),
            buffer_size: 1000,
            metrics: Arc::new(RwLock::new(StreamMetrics::default())),
        }
    }

    pub fn add_processor(mut self, processor: StreamTransform) -> Self {
        self.processors.push(processor);
        self
    }

    pub fn with_buffer_size(mut self, size: usize) -> Self {
        self.buffer_size = size;
        self
    }

    pub async fn process_stream<F>(mut self, mut output_handler: F) -> Result<()>
    where
        F: FnMut(StreamEvent) -> Result<()>,
    {
        let mut stream = self.input_stream;
        let start_time = Instant::now();
        let metrics = self.metrics.clone();

        while let Some(mut item) = stream.next().await {
            let item_start = Instant::now();

            // åº”ç”¨æ‰€æœ‰å¤„ç†å™¨
            let mut should_continue = false;
            for processor in &mut self.processors {
                match processor.transform(item.clone()).await? {
                    Some(new_item) => item = new_item,
                    None => {
                        // å¤„ç†å™¨å†³å®šä¸¢å¼ƒæ­¤é¡¹ç›®
                        debug!("é¡¹ç›®è¢«å¤„ç†å™¨ {} ä¸¢å¼ƒ", processor.name());
                        should_continue = true;
                        break;
                    }
                }
            }

            if should_continue {
                continue;
            }

            // å¤„ç†è¾“å‡º
            output_handler(item)?;

            // æ›´æ–°æŒ‡æ ‡
            Self::update_metrics_static(metrics.clone(), item_start).await;
        }

        let total_time = start_time.elapsed();
        let metrics_data = metrics.read().await;
        info!("æµå¤„ç†å®Œæˆ: å¤„ç†äº† {} ä¸ªé¡¹ç›®ï¼Œè€—æ—¶ {:?}", metrics_data.processed_count, total_time);

        Ok(())
    }


    async fn update_metrics_static(metrics: Arc<RwLock<StreamMetrics>>, item_start: Instant) {
        let mut metrics_data = metrics.write().await;
        metrics_data.processed_count += 1;
        metrics_data.last_processed_time = Some(Instant::now());
        
        let latency = item_start.elapsed();
        metrics_data.average_latency = Duration::from_nanos(
            ((metrics_data.average_latency.as_nanos() * (metrics_data.processed_count - 1) as u128 + latency.as_nanos()) / metrics_data.processed_count as u128) as u64
        );
    }

    pub async fn get_metrics(&self) -> StreamMetrics {
        self.metrics.read().await.clone()
    }
}

/// 2. å¼‚æ­¥æµèšåˆå™¨
pub struct AsyncStreamAggregator<T, Acc> {
    window_size: Duration,
    max_items: usize,
    aggregator: fn(Acc, T) -> Acc,
    initial_value: Acc,
    current_window: Vec<T>,
    current_accumulator: Acc,
    last_window_time: Instant,
}

impl<T: Clone, Acc: Clone> AsyncStreamAggregator<T, Acc> {
    pub fn new(
        window_size: Duration,
        max_items: usize,
        aggregator: fn(Acc, T) -> Acc,
        initial_value: Acc,
    ) -> Self {
        Self {
            window_size,
            max_items,
            aggregator,
            initial_value: initial_value.clone(),
            current_window: Vec::new(),
            current_accumulator: initial_value,
            last_window_time: Instant::now(),
        }
    }

    pub async fn process_item(&mut self, item: T) -> Option<Acc> {
        self.current_window.push(item);
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°çª—å£
        let should_flush = self.current_window.len() >= self.max_items ||
                          self.last_window_time.elapsed() >= self.window_size;

        if should_flush {
            self.flush_window().await
        } else {
            None
        }
    }

    async fn flush_window(&mut self) -> Option<Acc> {
        if self.current_window.is_empty() {
            return None;
        }

        // èšåˆå½“å‰çª—å£çš„æ•°æ®
        let mut accumulator = self.initial_value.clone();
        for item in &self.current_window {
            accumulator = (self.aggregator)(accumulator, item.clone());
        }

        // é‡ç½®çª—å£
        self.current_window.clear();
        self.current_accumulator = self.initial_value.clone();
        self.last_window_time = Instant::now();

        Some(accumulator)
    }

    pub async fn force_flush(&mut self) -> Option<Acc> {
        self.flush_window().await
    }
}

/// 3. å¼‚æ­¥æµåˆ†ç‰‡å™¨
pub struct AsyncStreamSharder<T> {
    shard_count: usize,
    shard_function: fn(&T) -> usize,
    shards: Vec<mpsc::UnboundedSender<T>>,
    receivers: Vec<mpsc::UnboundedReceiver<T>>,
}

impl<T: Send + 'static> AsyncStreamSharder<T> {
    pub fn new(shard_count: usize, shard_function: fn(&T) -> usize) -> Self {
        let mut shards = Vec::new();
        let mut receivers = Vec::new();

        for _ in 0..shard_count {
            let (tx, rx) = mpsc::unbounded_channel();
            shards.push(tx);
            receivers.push(rx);
        }

        Self {
            shard_count,
            shard_function,
            shards,
            receivers,
        }
    }

    pub async fn shard_item(&self, item: T) -> Result<()> {
        let shard_index = (self.shard_function)(&item) % self.shard_count;
        self.shards[shard_index].send(item)
            .map_err(|_| anyhow::anyhow!("Failed to send to shard {}", shard_index))?;
        Ok(())
    }

    pub fn get_shard_stream(&mut self, shard_index: usize) -> Option<mpsc::UnboundedReceiver<T>> {
        self.receivers.get_mut(shard_index).map(|rx| std::mem::replace(rx, mpsc::unbounded_channel().1))
    }

    pub async fn shard_stream(
        self,
        input_stream: impl Stream<Item = T> + Send + 'static,
    ) -> Vec<mpsc::UnboundedReceiver<T>> {
        let shards = self.shards.clone();
        let shard_function = self.shard_function;

        tokio::spawn(async move {
            let mut stream = Box::pin(input_stream);
            while let Some(item) = stream.next().await {
                let shard_index = shard_function(&item) % shards.len();
                if let Err(_) = shards[shard_index].send(item) {
                    warn!("Failed to send item to shard {}", shard_index);
                }
            }
        });

        self.receivers
    }
}

/// 4. å¼‚æ­¥æµåˆå¹¶å™¨
pub struct AsyncStreamMerger<T> {
    input_streams: Vec<Pin<Box<dyn Stream<Item = T> + Send>>>,
    merge_strategy: MergeStrategy,
}

#[derive(Debug, Clone)]
pub enum MergeStrategy {
    RoundRobin,
    Priority(Vec<usize>),
    Weighted(Vec<f64>),
}

impl<T: Send + 'static> AsyncStreamMerger<T> {
    pub fn new() -> Self {
        Self {
            input_streams: Vec::new(),
            merge_strategy: MergeStrategy::RoundRobin,
        }
    }

    pub fn add_stream(mut self, stream: impl Stream<Item = T> + Send + 'static) -> Self {
        self.input_streams.push(Box::pin(stream));
        self
    }

    pub fn with_strategy(mut self, strategy: MergeStrategy) -> Self {
        self.merge_strategy = strategy;
        self
    }

    pub async fn merge_streams(self) -> Pin<Box<dyn Stream<Item = T> + Send>> {
        match self.merge_strategy {
            MergeStrategy::RoundRobin => Box::pin(self.round_robin_merge().await),
            MergeStrategy::Priority(priorities) => {
                let merger = AsyncStreamMerger {
                    input_streams: self.input_streams,
                    merge_strategy: MergeStrategy::RoundRobin,
                };
                Box::pin(merger.priority_merge(priorities).await)
            }
            MergeStrategy::Weighted(weights) => {
                let merger = AsyncStreamMerger {
                    input_streams: self.input_streams,
                    merge_strategy: MergeStrategy::RoundRobin,
                };
                Box::pin(merger.weighted_merge(weights).await)
            }
        }
    }

    async fn round_robin_merge(self) -> impl Stream<Item = T> {
        let streams = self.input_streams;
        let current_index = 0;

        stream::unfold((streams, current_index), |(mut streams, mut index)| async move {
            if streams.is_empty() {
                return None;
            }

            loop {
                if let Some(item) = streams[index].next().await {
                    index = (index + 1) % streams.len();
                    return Some((item, (streams, index)));
                } else {
                    let _ = streams.remove(index);
                    if streams.is_empty() {
                        return None;
                    }
                    index = index % streams.len();
                }
            }
        })
    }

    async fn priority_merge(self, _priorities: Vec<usize>) -> impl Stream<Item = T> {
        // ç®€åŒ–å®ç°ï¼šä½¿ç”¨è½®è¯¢
        self.round_robin_merge().await
    }

    async fn weighted_merge(self, _weights: Vec<f64>) -> impl Stream<Item = T> {
        // ç®€åŒ–å®ç°ï¼šä½¿ç”¨è½®è¯¢
        self.round_robin_merge().await
    }
}

/// 5. å¼‚æ­¥æµè¿‡æ»¤å™¨
pub struct AsyncStreamFilter<T> {
    filter_fn: Arc<Box<dyn Fn(&T) -> bool + Send + Sync>>,
    metrics: Arc<RwLock<FilterMetrics>>,
}

#[derive(Debug, Default, Clone)]
pub struct FilterMetrics {
    pub total_items: u64,
    pub filtered_items: u64,
    pub pass_rate: f64,
}

impl<T: Send + Sync + 'static> AsyncStreamFilter<T> {
    pub fn new(filter_fn: impl Fn(&T) -> bool + Send + Sync + 'static) -> Self {
        Self {
            filter_fn: Arc::new(Box::new(filter_fn)),
            metrics: Arc::new(RwLock::new(FilterMetrics::default())),
        }
    }

    pub async fn filter_stream(
        &self,
        input_stream: impl Stream<Item = T> + Send + 'static,
    ) -> Pin<Box<dyn Stream<Item = T> + Send>> {
        let metrics = self.metrics.clone();
        let filter_fn = self.filter_fn.clone();
        
        Box::pin(input_stream.filter_map(move |item: T| {
            let metrics = metrics.clone();
            let filter_fn = filter_fn.clone();
            async move {
                let mut m = metrics.write().await;
                m.total_items += 1;

                let passes = (filter_fn)(&item);
                if passes {
                    m.filtered_items += 1;
                }

                m.pass_rate = m.filtered_items as f64 / m.total_items as f64;
                
                if passes {
                    Some(item)
                } else {
                    None
                }
            }
        }))
    }

    pub async fn get_metrics(&self) -> FilterMetrics {
        self.metrics.read().await.clone()
    }
}

/// 6. å¼‚æ­¥æµè½¬æ¢å™¨å®ç°
#[derive(Clone)]
pub struct AsyncDataTransformer;

impl StreamTransformImpl for AsyncDataTransformer {
    async fn transform(&mut self, mut event: StreamEvent) -> Result<Option<StreamEvent>> {
        // æ¨¡æ‹Ÿæ•°æ®è½¬æ¢
        sleep(Duration::from_millis(10)).await;
        
        event.data = format!("Transformed: {}", event.data);
        event.timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)?
            .as_millis() as u64;

        Ok(Some(event))
    }

    fn name(&self) -> &str {
        "DataTransformer"
    }
}

#[derive(Clone)]
pub struct AsyncErrorFilter;

impl StreamTransformImpl for AsyncErrorFilter {
    async fn transform(&mut self, event: StreamEvent) -> Result<Option<StreamEvent>> {
        // è¿‡æ»¤æ‰é”™è¯¯äº‹ä»¶
        match event.event_type {
            EventType::Error => {
                debug!("è¿‡æ»¤æ‰é”™è¯¯äº‹ä»¶: {}", event.id);
                Ok(None)
            }
            _ => Ok(Some(event)),
        }
    }

    fn name(&self) -> &str {
        "ErrorFilter"
    }
}

#[derive(Clone)]
pub struct AsyncEventEnricher {
    enrichment_data: HashMap<String, String>,
}

impl AsyncEventEnricher {
    pub fn new() -> Self {
        let mut enrichment_data = HashMap::new();
        enrichment_data.insert("source".to_string(), "async_stream".to_string());
        enrichment_data.insert("version".to_string(), "2025".to_string());

        Self { enrichment_data }
    }
}

impl StreamTransformImpl for AsyncEventEnricher {
    async fn transform(&mut self, mut event: StreamEvent) -> Result<Option<StreamEvent>> {
        // æ¨¡æ‹Ÿæ•°æ®ä¸°å¯ŒåŒ–
        sleep(Duration::from_millis(5)).await;
        
        for (key, value) in &self.enrichment_data {
            event.data = format!("{}[{}={}]", event.data, key, value);
        }

        Ok(Some(event))
    }

    fn name(&self) -> &str {
        "EventEnricher"
    }
}

/// 7. å¼‚æ­¥æµèƒŒå‹æ§åˆ¶å™¨
pub struct AsyncBackpressureController {
    max_buffer_size: usize,
    current_buffer_size: Arc<RwLock<usize>>,
    semaphore: Arc<Semaphore>,
}

impl AsyncBackpressureController {
    pub fn new(max_buffer_size: usize) -> Self {
        Self {
            max_buffer_size,
            current_buffer_size: Arc::new(RwLock::new(0)),
            semaphore: Arc::new(Semaphore::new(max_buffer_size)),
        }
    }

    pub async fn acquire_permit(&self) -> Result<()> {
        self.semaphore.acquire().await
            .map_err(|_| anyhow::anyhow!("Failed to acquire permit"))?
            .forget(); // å¿˜è®°permitï¼Œæ‰‹åŠ¨ç®¡ç†
        Ok(())
    }

    pub async fn release_permit(&self) {
        self.semaphore.add_permits(1);
    }

    pub async fn get_buffer_usage(&self) -> f64 {
        let current = *self.current_buffer_size.read().await;
        current as f64 / self.max_buffer_size as f64
    }
}

/// 8. å¼‚æ­¥æµç›‘æ§å™¨
pub struct AsyncStreamMonitor {
    metrics: Arc<RwLock<StreamMetrics>>,
    alert_thresholds: AlertThresholds,
}

#[derive(Debug, Clone)]
pub struct AlertThresholds {
    pub max_latency: Duration,
    pub min_throughput: f64,
    pub max_error_rate: f64,
}

impl AsyncStreamMonitor {
    pub fn new(alert_thresholds: AlertThresholds) -> Self {
        Self {
            metrics: Arc::new(RwLock::new(StreamMetrics::default())),
            alert_thresholds,
        }
    }

    pub async fn update_metrics(&self, processed_count: u64, error_count: u64, latency: Duration) {
        let mut metrics = self.metrics.write().await;
        metrics.processed_count = processed_count;
        metrics.error_count = error_count;
        metrics.average_latency = latency;
        metrics.last_processed_time = Some(Instant::now());
    }

    pub async fn check_alerts(&self) -> Vec<String> {
        let metrics = self.metrics.read().await;
        let mut alerts = Vec::new();

        if metrics.average_latency > self.alert_thresholds.max_latency {
            alerts.push(format!(
                "é«˜å»¶è¿Ÿå‘Šè­¦: {:?} > {:?}",
                metrics.average_latency,
                self.alert_thresholds.max_latency
            ));
        }

        if metrics.throughput_per_second < self.alert_thresholds.min_throughput {
            alerts.push(format!(
                "ä½ååé‡å‘Šè­¦: {:.2} < {:.2}",
                metrics.throughput_per_second,
                self.alert_thresholds.min_throughput
            ));
        }

        let error_rate = if metrics.processed_count > 0 {
            metrics.error_count as f64 / metrics.processed_count as f64
        } else {
            0.0
        };

        if error_rate > self.alert_thresholds.max_error_rate {
            alerts.push(format!(
                "é«˜é”™è¯¯ç‡å‘Šè­¦: {:.2}% > {:.2}%",
                error_rate * 100.0,
                self.alert_thresholds.max_error_rate * 100.0
            ));
        }

        alerts
    }
}

/// æ¼”ç¤ºé«˜çº§å¼‚æ­¥æµå¤„ç†
#[tokio::main]
async fn main() -> Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    tracing_subscriber::fmt()
        .with_env_filter("info")
        .init();

    info!("ğŸš€ å¼€å§‹ 2025 å¹´é«˜çº§å¼‚æ­¥æµå¤„ç†æ¼”ç¤º");

    // 1. åŸºç¡€æµå¤„ç†æ¼”ç¤º
    demo_basic_stream_processing().await?;

    // 2. æµèšåˆæ¼”ç¤º
    demo_stream_aggregation().await?;

    // 3. æµåˆ†ç‰‡æ¼”ç¤º
    demo_stream_sharding().await?;

    // 4. æµåˆå¹¶æ¼”ç¤º
    demo_stream_merging().await?;

    // 5. æµè¿‡æ»¤æ¼”ç¤º
    demo_stream_filtering().await?;

    // 6. èƒŒå‹æ§åˆ¶æ¼”ç¤º
    demo_backpressure_control().await?;

    // 7. æµç›‘æ§æ¼”ç¤º
    demo_stream_monitoring().await?;

    info!("âœ… 2025 å¹´é«˜çº§å¼‚æ­¥æµå¤„ç†æ¼”ç¤ºå®Œæˆ!");
    Ok(())
}

async fn demo_basic_stream_processing() -> Result<()> {
    info!("ğŸ“Š æ¼”ç¤ºåŸºç¡€å¼‚æ­¥æµå¤„ç†");

    // åˆ›å»ºè¾“å…¥æµ
    let input_stream = stream::iter(0..100)
        .map(|i| StreamEvent {
            id: i,
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_millis() as u64,
            data: format!("Event_{}", i),
            event_type: if i % 10 == 0 {
                EventType::Error
            } else {
                EventType::Data
            },
        });

    // åˆ›å»ºæµå¤„ç†å™¨
    let processor = AsyncStreamProcessor::new(input_stream)
        .add_processor(StreamTransform::EventEnricher(AsyncEventEnricher::new()))
        .add_processor(StreamTransform::ErrorFilter(AsyncErrorFilter))
        .add_processor(StreamTransform::DataTransformer(AsyncDataTransformer))
        .with_buffer_size(50);

    let mut processed_count = 0;
    processor.process_stream(|event| {
        processed_count += 1;
        if processed_count % 20 == 0 {
            info!("å¤„ç†äº‹ä»¶: {} - {}", event.id, event.data);
        }
        Ok(())
    }).await?;

    // ç”±äº processor å·²ç»è¢«æ¶ˆè´¹ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è·å–æŒ‡æ ‡
    let metrics = StreamMetrics::default();
    info!("æµå¤„ç†æŒ‡æ ‡: å¤„ç†äº† {} ä¸ªé¡¹ç›®ï¼Œå¹³å‡å»¶è¿Ÿ {:?}", 
          metrics.processed_count, metrics.average_latency);

    Ok(())
}

async fn demo_stream_aggregation() -> Result<()> {
    info!("ğŸ“ˆ æ¼”ç¤ºæµèšåˆ");

    let mut aggregator = AsyncStreamAggregator::new(
        Duration::from_millis(500),
        10,
        |acc: u64, event: StreamEvent| acc + event.id,
        0,
    );

    let input_stream = stream::iter(0..25)
        .map(|i| StreamEvent {
            id: i,
            timestamp: 0,
            data: format!("Agg_{}", i),
            event_type: EventType::Data,
        });

    let mut stream = input_stream;
    while let Some(event) = stream.next().await {
        if let Some(result) = aggregator.process_item(event).await {
            info!("èšåˆç»“æœ: {}", result);
        }
    }

    // å¼ºåˆ¶åˆ·æ–°å‰©ä½™æ•°æ®
    if let Some(result) = aggregator.force_flush().await {
        info!("æœ€ç»ˆèšåˆç»“æœ: {}", result);
    }

    Ok(())
}

async fn demo_stream_sharding() -> Result<()> {
    info!("ğŸ”€ æ¼”ç¤ºæµåˆ†ç‰‡");

    let sharder = AsyncStreamSharder::new(3, |event: &StreamEvent| event.id as usize);
    
    let input_stream = stream::iter(0..30)
        .map(|i| StreamEvent {
            id: i,
            timestamp: 0,
            data: format!("Shard_{}", i),
            event_type: EventType::Data,
        });

    let shard_streams = sharder.shard_stream(input_stream).await;

    // å¯åŠ¨å¤„ç†ä»»åŠ¡
    let mut handles = Vec::new();
    for (i, mut shard_stream) in shard_streams.into_iter().enumerate() {
        let handle = tokio::spawn(async move {
            let mut count = 0;
            while let Some(event) = shard_stream.recv().await {
                count += 1;
                if count % 5 == 0 {
                    info!("åˆ†ç‰‡ {} å¤„ç†äº‹ä»¶: {}", i, event.id);
                }
            }
            info!("åˆ†ç‰‡ {} å®Œæˆï¼Œå¤„ç†äº† {} ä¸ªäº‹ä»¶", i, count);
        });
        handles.push(handle);
    }

    // ç­‰å¾…æ‰€æœ‰åˆ†ç‰‡å®Œæˆ
    for handle in handles {
        handle.await?;
    }

    Ok(())
}

async fn demo_stream_merging() -> Result<()> {
    info!("ğŸ”— æ¼”ç¤ºæµåˆå¹¶");

    let stream1 = stream::iter(0..10)
        .map(|i| format!("Stream1_Event_{}", i));
    
    let stream2 = stream::iter(0..10)
        .map(|i| format!("Stream2_Event_{}", i));
    
    let stream3 = stream::iter(0..10)
        .map(|i| format!("Stream3_Event_{}", i));

    let merged_stream = AsyncStreamMerger::new()
        .add_stream(stream1)
        .add_stream(stream2)
        .add_stream(stream3)
        .with_strategy(MergeStrategy::RoundRobin)
        .merge_streams()
        .await;

    let mut merged = merged_stream;
    let mut count = 0;
    while let Some(event) = merged.next().await {
        count += 1;
        if count % 5 == 0 {
            info!("åˆå¹¶æµäº‹ä»¶: {}", event);
        }
    }

    info!("åˆå¹¶æµå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {} ä¸ªäº‹ä»¶", count);

    Ok(())
}

async fn demo_stream_filtering() -> Result<()> {
    info!("ğŸ” æ¼”ç¤ºæµè¿‡æ»¤");

    let filter = AsyncStreamFilter::new(|event: &StreamEvent| event.id % 2 == 0);

    let input_stream = stream::iter(0..20)
        .map(|i| StreamEvent {
            id: i,
            timestamp: 0,
            data: format!("Filter_{}", i),
            event_type: EventType::Data,
        });

    let filtered_stream = filter.filter_stream(input_stream).await;
    let mut stream = filtered_stream;
    let mut _count = 0;

    while let Some(event) = stream.next().await {
        _count += 1;
        info!("è¿‡æ»¤åäº‹ä»¶: {} - {}", event.id, event.data);
    }

    let metrics = filter.get_metrics().await;
    info!("è¿‡æ»¤æŒ‡æ ‡: æ€»äº‹ä»¶ {}ï¼Œé€šè¿‡äº‹ä»¶ {}ï¼Œé€šè¿‡ç‡ {:.2}%",
          metrics.total_items, metrics.filtered_items, metrics.pass_rate * 100.0);

    Ok(())
}

async fn demo_backpressure_control() -> Result<()> {
    info!("âš¡ æ¼”ç¤ºèƒŒå‹æ§åˆ¶");

    let controller = AsyncBackpressureController::new(5);
    let mut count = 0;

    for _i in 0..20 {
        controller.acquire_permit().await?;
        
        // æ¨¡æ‹Ÿå¤„ç†
        sleep(Duration::from_millis(50)).await;
        count += 1;
        
        if count % 5 == 0 {
            let usage = controller.get_buffer_usage().await;
            info!("èƒŒå‹æ§åˆ¶: å¤„ç†äº† {} ä¸ªäº‹ä»¶ï¼Œç¼“å†²åŒºä½¿ç”¨ç‡: {:.2}%", count, usage * 100.0);
        }

        controller.release_permit().await;
    }

    Ok(())
}

async fn demo_stream_monitoring() -> Result<()> {
    info!("ğŸ“Š æ¼”ç¤ºæµç›‘æ§");

    let monitor = AsyncStreamMonitor::new(AlertThresholds {
        max_latency: Duration::from_millis(100),
        min_throughput: 10.0,
        max_error_rate: 0.1,
    });

    // æ¨¡æ‹Ÿä¸€äº›æŒ‡æ ‡æ›´æ–°
    monitor.update_metrics(100, 5, Duration::from_millis(50)).await;
    let alerts = monitor.check_alerts().await;
    
    if alerts.is_empty() {
        info!("ç›‘æ§æ­£å¸¸ï¼Œæ— å‘Šè­¦");
    } else {
        for alert in alerts {
            warn!("ç›‘æ§å‘Šè­¦: {}", alert);
        }
    }

    // æ¨¡æ‹Ÿé«˜å»¶è¿Ÿåœºæ™¯
    monitor.update_metrics(100, 5, Duration::from_millis(150)).await;
    let alerts = monitor.check_alerts().await;
    
    for alert in alerts {
        warn!("ç›‘æ§å‘Šè­¦: {}", alert);
    }

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_async_stream_aggregator() {
        let mut aggregator = AsyncStreamAggregator::new(
            Duration::from_millis(100),
            5,
            |acc: u64, event: StreamEvent| acc + event.id,
            0,
        );

        let event = StreamEvent {
            id: 10,
            timestamp: 0,
            data: "test".to_string(),
            event_type: EventType::Data,
        };

        let result = aggregator.process_item(event).await;
        assert!(result.is_none()); // çª—å£æœªæ»¡ï¼Œä¸åº”è¿”å›ç»“æœ

        // å¡«å……çª—å£
        for i in 1..5 {
            let event = StreamEvent {
                id: i,
                timestamp: 0,
                data: "test".to_string(),
                event_type: EventType::Data,
            };
            aggregator.process_item(event).await;
        }

        let result = aggregator.force_flush().await;
        assert!(result.is_some());
        assert_eq!(result.unwrap(), 10 + 1 + 2 + 3 + 4);
    }

    #[tokio::test]
    async fn test_async_stream_filter() {
        let filter = AsyncStreamFilter::new(|event: &StreamEvent| event.id % 2 == 0);
        
        let input_stream = stream::iter(vec![
            StreamEvent { id: 1, timestamp: 0, data: "odd".to_string(), event_type: EventType::Data },
            StreamEvent { id: 2, timestamp: 0, data: "even".to_string(), event_type: EventType::Data },
            StreamEvent { id: 3, timestamp: 0, data: "odd".to_string(), event_type: EventType::Data },
        ]);

        let filtered_stream = filter.filter_stream(input_stream).await;
        let results: Vec<_> = filtered_stream.collect().await;
        
        assert_eq!(results.len(), 1);
        assert_eq!(results[0].id, 2);
    }
}
