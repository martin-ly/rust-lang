use anyhow::Result;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{RwLock,  Semaphore};
use tokio::time::sleep;
use tracing::{info, debug};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
//use std::sync::atomic::{AtomicUsize, AtomicU64, Ordering};

/// 2025å¹´å¼‚æ­¥æ•°æ®åº“æ¨¡å¼æ¼”ç¤º
/// å±•ç¤ºæœ€æ–°çš„å¼‚æ­¥æ•°æ®åº“ç¼–ç¨‹æ¨¡å¼å’Œæœ€ä½³å®è·µ

/// 1. å¼‚æ­¥æ•°æ®åº“è¿æ¥æ± 
#[derive(Debug, Clone)]
pub struct AsyncDatabasePool {
    connections: Arc<RwLock<Vec<DatabaseConnection>>>,
    available_connections: Arc<RwLock<Vec<String>>>,
    pool_config: PoolConfig,
    pool_stats: Arc<RwLock<PoolStats>>,
    semaphore: Arc<Semaphore>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PoolConfig {
    pub max_connections: usize,
    pub min_connections: usize,
    pub connection_timeout: Duration,
    pub idle_timeout: Duration,
    pub max_lifetime: Duration,
    pub database_url: String,
}

impl Default for PoolConfig {
    fn default() -> Self {
        Self {
            max_connections: 20,
            min_connections: 5,
            connection_timeout: Duration::from_secs(30),
            idle_timeout: Duration::from_secs(600),
            max_lifetime: Duration::from_secs(3600),
            database_url: "postgresql://localhost:5432/mydb".to_string(),
        }
    }
}

#[derive(Debug, Clone)]
pub struct DatabaseConnection {
    pub id: String,
    pub url: String,
    pub created_at: Instant,
    pub last_used: Instant,
    pub is_active: bool,
    pub query_count: usize,
    pub error_count: usize,
}

#[derive(Debug, Clone, Default)]
pub struct PoolStats {
    pub total_connections: usize,
    pub active_connections: usize,
    pub idle_connections: usize,
    pub total_queries: usize,
    pub successful_queries: usize,
    pub failed_queries: usize,
    pub connection_creations: usize,
    pub connection_closures: usize,
}

impl AsyncDatabasePool {
    pub fn new(config: PoolConfig) -> Self {
        let semaphore = Arc::new(Semaphore::new(config.max_connections));
        
        Self {
            connections: Arc::new(RwLock::new(Vec::new())),
            available_connections: Arc::new(RwLock::new(Vec::new())),
            pool_config: config,
            pool_stats: Arc::new(RwLock::new(PoolStats::default())),
            semaphore,
        }
    }

    pub async fn initialize(&self) -> Result<()> {
        info!("åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ï¼Œæœ€å°è¿æ¥æ•°: {}", self.pool_config.min_connections);
        
        // åˆ›å»ºæœ€å°è¿æ¥æ•°
        for _i in 0..self.pool_config.min_connections {
            let connection = self.create_connection().await?;
            let mut connections = self.connections.write().await;
            let mut available = self.available_connections.write().await;
            
            connections.push(connection.clone());
            available.push(connection.id.clone());
            
            let mut stats = self.pool_stats.write().await;
            stats.total_connections += 1;
            stats.idle_connections += 1;
        }
        
        // å¯åŠ¨è¿æ¥æ± ç›‘æ§ä»»åŠ¡
        let pool_clone = self.clone();
        tokio::spawn(async move {
            pool_clone.connection_monitor().await;
        });
        
        info!("æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å®Œæˆ");
        Ok(())
    }

    async fn create_connection(&self) -> Result<DatabaseConnection> {
        // æ¨¡æ‹Ÿè¿æ¥åˆ›å»º
        sleep(Duration::from_millis(100)).await;
        
        let connection = DatabaseConnection {
            id: format!("conn_{}", Instant::now().elapsed().as_nanos()),
            url: self.pool_config.database_url.clone(),
            created_at: Instant::now(),
            last_used: Instant::now(),
            is_active: true,
            query_count: 0,
            error_count: 0,
        };
        
        let mut stats = self.pool_stats.write().await;
        stats.connection_creations += 1;
        
        info!("åˆ›å»ºæ•°æ®åº“è¿æ¥: {}", connection.id);
        Ok(connection)
    }

    pub async fn get_connection(&self) -> Result<PooledConnection> {
        let _permit = self.semaphore.acquire().await.unwrap();
        
        // å°è¯•è·å–å¯ç”¨è¿æ¥
        let mut available = self.available_connections.write().await;
        let mut connections = self.connections.write().await;
        
        if let Some(connection_id) = available.pop() {
            if let Some(connection) = connections.iter_mut().find(|c| c.id == connection_id) {
                connection.last_used = Instant::now();
                
                let mut stats = self.pool_stats.write().await;
                stats.idle_connections -= 1;
                stats.active_connections += 1;
                
                info!("è·å–æ•°æ®åº“è¿æ¥: {}", connection_id);
                return Ok(PooledConnection {
                    connection_id: connection_id.clone(),
                    pool: self.clone(),
                });
            }
        }
        
        // åˆ›å»ºæ–°è¿æ¥
        if connections.len() < self.pool_config.max_connections {
            let connection = self.create_connection().await?;
            let connection_id = connection.id.clone();
            connections.push(connection);
            
            let mut stats = self.pool_stats.write().await;
            stats.total_connections += 1;
            stats.active_connections += 1;
            
            info!("åˆ›å»ºæ–°æ•°æ®åº“è¿æ¥: {}", connection_id);
            return Ok(PooledConnection {
                connection_id: connection_id.clone(),
                pool: self.clone(),
            });
        }
        
        Err(anyhow::anyhow!("æ— æ³•è·å–æ•°æ®åº“è¿æ¥ï¼šè¿æ¥æ± å·²æ»¡"))
    }

    pub async fn release_connection(&self, connection_id: String) -> Result<()> {
        let mut connections = self.connections.write().await;
        let mut available = self.available_connections.write().await;
        
        if let Some(connection) = connections.iter_mut().find(|c| c.id == connection_id) {
            connection.last_used = Instant::now();
            available.push(connection_id.clone());
            
            let mut stats = self.pool_stats.write().await;
            stats.active_connections -= 1;
            stats.idle_connections += 1;
            
            info!("é‡Šæ”¾æ•°æ®åº“è¿æ¥: {}", connection_id);
        }
        
        Ok(())
    }

    async fn connection_monitor(&self) {
        let mut interval = tokio::time::interval(Duration::from_secs(60));
        loop {
            interval.tick().await;
            
            let mut connections = self.connections.write().await;
            let mut available = self.available_connections.write().await;
            let mut stats = self.pool_stats.write().await;
            
            let now = Instant::now();
            let mut to_remove = Vec::new();
            
            // æ¸…ç†è¶…æ—¶è¿æ¥
            for (i, connection) in connections.iter().enumerate() {
                let idle_time = now.duration_since(connection.last_used);
                let lifetime = now.duration_since(connection.created_at);
                
                if (idle_time > self.pool_config.idle_timeout && connections.len() > self.pool_config.min_connections) ||
                   lifetime > self.pool_config.max_lifetime {
                    to_remove.push(i);
                }
            }
            
            // ç§»é™¤è¶…æ—¶è¿æ¥
            for &i in to_remove.iter().rev() {
                let connection = connections.remove(i);
                available.retain(|id| id != &connection.id);
                stats.total_connections -= 1;
                stats.connection_closures += 1;
                
                info!("æ¸…ç†è¶…æ—¶æ•°æ®åº“è¿æ¥: {}", connection.id);
            }
            
            debug!("è¿æ¥æ± çŠ¶æ€: æ€»è¿æ¥ {}, æ´»è·ƒ {}, ç©ºé—² {}", 
                   stats.total_connections, stats.active_connections, stats.idle_connections);
        }
    }

    pub async fn execute_query(&self, connection_id: &str, query: &str) -> Result<QueryResult> {
        let mut connections = self.connections.write().await;
        if let Some(connection) = connections.iter_mut().find(|c| c.id == connection_id) {
            connection.query_count += 1;
            connection.last_used = Instant::now();
            
            let mut stats = self.pool_stats.write().await;
            stats.total_queries += 1;
            
            // æ¨¡æ‹ŸæŸ¥è¯¢æ‰§è¡Œ
            sleep(Duration::from_millis(50)).await;
            
            // æ¨¡æ‹ŸæŸ¥è¯¢ç»“æœ
            let result = QueryResult {
                rows_affected: 1,
                rows_returned: 1,
                execution_time: Duration::from_millis(50),
                columns: vec!["id".to_string(), "name".to_string()],
                data: vec![vec!["1".to_string(), "test".to_string()]],
            };
            
            stats.successful_queries += 1;
            info!("æ‰§è¡ŒæŸ¥è¯¢æˆåŠŸ: {}", query);
            Ok(result)
        } else {
            let mut stats = self.pool_stats.write().await;
            stats.failed_queries += 1;
            Err(anyhow::anyhow!("è¿æ¥ {} æœªæ‰¾åˆ°", connection_id))
        }
    }

    pub async fn get_pool_stats(&self) -> PoolStats {
        self.pool_stats.read().await.clone()
    }
}

#[derive(Debug, Clone)]
pub struct PooledConnection {
    connection_id: String,
    pool: AsyncDatabasePool,
}

impl Drop for PooledConnection {
    fn drop(&mut self) {
        let pool = self.pool.clone();
        let connection_id = self.connection_id.clone();
        tokio::spawn(async move {
            let _ = pool.release_connection(connection_id).await;
        });
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryResult {
    pub rows_affected: usize,
    pub rows_returned: usize,
    pub execution_time: Duration,
    pub columns: Vec<String>,
    pub data: Vec<Vec<String>>,
}

/// 2. å¼‚æ­¥æ•°æ®åº“äº‹åŠ¡ç®¡ç†å™¨
#[derive(Debug, Clone)]
pub struct AsyncTransactionManager {
    pool: AsyncDatabasePool,
    active_transactions: Arc<RwLock<HashMap<String, Transaction>>>,
    transaction_stats: Arc<RwLock<TransactionStats>>,
}

#[derive(Debug, Clone)]
pub struct Transaction {
    pub id: String,
    pub connection_id: String,
    pub started_at: Instant,
    pub isolation_level: IsolationLevel,
    pub status: TransactionStatus,
    pub queries: Vec<String>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum IsolationLevel {
    ReadUncommitted,
    ReadCommitted,
    RepeatableRead,
    Serializable,
}

#[derive(Debug, Clone, PartialEq)]
pub enum TransactionStatus {
    Active,
    Committed,
    RolledBack,
    Aborted,
}

#[derive(Debug, Clone, Default)]
pub struct TransactionStats {
    pub total_transactions: usize,
    pub committed_transactions: usize,
    pub rolled_back_transactions: usize,
    pub active_transactions: usize,
    pub average_transaction_time: Duration,
}

impl AsyncTransactionManager {
    pub fn new(pool: AsyncDatabasePool) -> Self {
        Self {
            pool,
            active_transactions: Arc::new(RwLock::new(HashMap::new())),
            transaction_stats: Arc::new(RwLock::new(TransactionStats::default())),
        }
    }

    pub async fn begin_transaction(&self, isolation_level: IsolationLevel) -> Result<TransactionHandle> {
        let connection = self.pool.get_connection().await?;
        let transaction_id = format!("tx_{}", Instant::now().elapsed().as_nanos());
        
        let transaction = Transaction {
            id: transaction_id.clone(),
            connection_id: connection.connection_id.clone(),
            started_at: Instant::now(),
            isolation_level: isolation_level.clone(),
            status: TransactionStatus::Active,
            queries: Vec::new(),
        };
        
        {
            let mut transactions = self.active_transactions.write().await;
            transactions.insert(transaction_id.clone(), transaction);
        }
        
        let mut stats = self.transaction_stats.write().await;
        stats.total_transactions += 1;
        stats.active_transactions += 1;
        
        info!("å¼€å§‹äº‹åŠ¡: {} (éš”ç¦»çº§åˆ«: {:?})", transaction_id, isolation_level);
        
        Ok(TransactionHandle {
            transaction_id: transaction_id.clone(),
            manager: self.clone(),
        })
    }

    pub async fn execute_in_transaction(&self, transaction_id: &str, query: &str) -> Result<QueryResult> {
        let mut transactions = self.active_transactions.write().await;
        if let Some(transaction) = transactions.get_mut(transaction_id) {
            if transaction.status != TransactionStatus::Active {
                return Err(anyhow::anyhow!("äº‹åŠ¡ {} ä¸æ˜¯æ´»è·ƒçŠ¶æ€", transaction_id));
            }
            
            transaction.queries.push(query.to_string());
            
            // æ‰§è¡ŒæŸ¥è¯¢
            let result = self.pool.execute_query(&transaction.connection_id, query).await?;
            
            info!("åœ¨äº‹åŠ¡ {} ä¸­æ‰§è¡ŒæŸ¥è¯¢: {}", transaction_id, query);
            Ok(result)
        } else {
            Err(anyhow::anyhow!("äº‹åŠ¡ {} æœªæ‰¾åˆ°", transaction_id))
        }
    }

    pub async fn commit_transaction(&self, transaction_id: &str) -> Result<()> {
        let mut transactions = self.active_transactions.write().await;
        if let Some(transaction) = transactions.get_mut(transaction_id) {
            if transaction.status != TransactionStatus::Active {
                return Err(anyhow::anyhow!("äº‹åŠ¡ {} ä¸æ˜¯æ´»è·ƒçŠ¶æ€", transaction_id));
            }
            
            transaction.status = TransactionStatus::Committed;
            
            // æ¨¡æ‹Ÿæäº¤
            sleep(Duration::from_millis(20)).await;
            
            let mut stats = self.transaction_stats.write().await;
            stats.active_transactions -= 1;
            stats.committed_transactions += 1;
            
            let transaction_time = Instant::now().duration_since(transaction.started_at);
        stats.average_transaction_time = Duration::from_millis(
            ((stats.average_transaction_time.as_millis() + transaction_time.as_millis()) / 2).try_into().unwrap()
        );
            
            // é‡Šæ”¾è¿æ¥
            self.pool.release_connection(transaction.connection_id.clone()).await?;
            
            info!("æäº¤äº‹åŠ¡: {} (è€—æ—¶: {:?})", transaction_id, transaction_time);
            Ok(())
        } else {
            Err(anyhow::anyhow!("äº‹åŠ¡ {} æœªæ‰¾åˆ°", transaction_id))
        }
    }

    pub async fn rollback_transaction(&self, transaction_id: &str) -> Result<()> {
        let mut transactions = self.active_transactions.write().await;
        if let Some(transaction) = transactions.get_mut(transaction_id) {
            if transaction.status != TransactionStatus::Active {
                return Err(anyhow::anyhow!("äº‹åŠ¡ {} ä¸æ˜¯æ´»è·ƒçŠ¶æ€", transaction_id));
            }
            
            transaction.status = TransactionStatus::RolledBack;
            
            // æ¨¡æ‹Ÿå›æ»š
            sleep(Duration::from_millis(10)).await;
            
            let mut stats = self.transaction_stats.write().await;
            stats.active_transactions -= 1;
            stats.rolled_back_transactions += 1;
            
            // é‡Šæ”¾è¿æ¥
            self.pool.release_connection(transaction.connection_id.clone()).await?;
            
            info!("å›æ»šäº‹åŠ¡: {}", transaction_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("äº‹åŠ¡ {} æœªæ‰¾åˆ°", transaction_id))
        }
    }

    pub async fn get_transaction_stats(&self) -> TransactionStats {
        self.transaction_stats.read().await.clone()
    }
}

#[derive(Debug, Clone)]
pub struct TransactionHandle {
    transaction_id: String,
    manager: AsyncTransactionManager,
}

impl TransactionHandle {
    pub async fn execute_query(&self, query: &str) -> Result<QueryResult> {
        self.manager.execute_in_transaction(&self.transaction_id, query).await
    }

    pub async fn commit(self) -> Result<()> {
        self.manager.commit_transaction(&self.transaction_id).await
    }

    pub async fn rollback(self) -> Result<()> {
        self.manager.rollback_transaction(&self.transaction_id).await
    }
}

/// 3. å¼‚æ­¥æ•°æ®åº“ç¼“å­˜ç³»ç»Ÿ
#[derive(Debug, Clone)]
pub struct AsyncDatabaseCache {
    cache_store: Arc<RwLock<HashMap<String, CacheEntry>>>,
    cache_config: CacheConfig,
    cache_stats: Arc<RwLock<CacheStats>>,
    cleanup_interval: Duration,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheConfig {
    pub max_size: usize,
    pub default_ttl: Duration,
    pub cleanup_interval: Duration,
    pub enable_compression: bool,
}

impl Default for CacheConfig {
    fn default() -> Self {
        Self {
            max_size: 1000,
            default_ttl: Duration::from_secs(300),
            cleanup_interval: Duration::from_secs(60),
            enable_compression: false,
        }
    }
}

#[derive(Debug, Clone)]
pub struct CacheEntry {
    pub key: String,
    pub value: QueryResult,
    pub created_at: Instant,
    pub expires_at: Instant,
    pub access_count: usize,
    pub last_accessed: Instant,
}

#[derive(Debug, Clone, Default)]
pub struct CacheStats {
    pub total_entries: usize,
    pub cache_hits: usize,
    pub cache_misses: usize,
    pub cache_evictions: usize,
    pub total_size_bytes: usize,
}

impl AsyncDatabaseCache {
    pub fn new(config: CacheConfig) -> Self {
        let cache = Self {
            cache_store: Arc::new(RwLock::new(HashMap::new())),
            cache_config: config.clone(),
            cache_stats: Arc::new(RwLock::new(CacheStats::default())),
            cleanup_interval: config.cleanup_interval,
        };
        
        // å¯åŠ¨ç¼“å­˜æ¸…ç†ä»»åŠ¡
        let cache_clone = cache.clone();
        tokio::spawn(async move {
            cache_clone.cleanup_loop().await;
        });
        
        cache
    }

    pub async fn get(&self, key: &str) -> Option<QueryResult> {
        let mut cache = self.cache_store.write().await;
        if let Some(entry) = cache.get_mut(key) {
            let now = Instant::now();
            
            // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if now > entry.expires_at {
                cache.remove(key);
                let mut stats = self.cache_stats.write().await;
                stats.total_entries -= 1;
                stats.cache_misses += 1;
                return None;
            }
            
            // æ›´æ–°è®¿é—®ç»Ÿè®¡
            entry.access_count += 1;
            entry.last_accessed = now;
            
            let mut stats = self.cache_stats.write().await;
            stats.cache_hits += 1;
            
            info!("ç¼“å­˜å‘½ä¸­: {}", key);
            Some(entry.value.clone())
        } else {
            let mut stats = self.cache_stats.write().await;
            stats.cache_misses += 1;
            None
        }
    }

    pub async fn set(&self, key: String, value: QueryResult, ttl: Option<Duration>) -> Result<()> {
        let ttl = ttl.unwrap_or(self.cache_config.default_ttl);
        let now = Instant::now();
        
        let entry = CacheEntry {
            key: key.clone(),
            value: value.clone(),
            created_at: now,
            expires_at: now + ttl,
            access_count: 0,
            last_accessed: now,
        };
        
        let mut cache = self.cache_store.write().await;
        
        // æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if cache.len() >= self.cache_config.max_size && !cache.contains_key(&key) {
            self.evict_oldest_entry(&mut cache).await;
        }
        
        cache.insert(key.clone(), entry);
        
        let mut stats = self.cache_stats.write().await;
        stats.total_entries += 1;
        
        info!("ç¼“å­˜è®¾ç½®: {} (TTL: {:?})", key, ttl);
        Ok(())
    }

    async fn evict_oldest_entry(&self, cache: &mut HashMap<String, CacheEntry>) {
        if let Some((oldest_key, _)) = cache.iter()
            .min_by_key(|(_, entry)| entry.last_accessed) {
            let key = oldest_key.clone();
            cache.remove(&key);
            
            let mut stats = self.cache_stats.write().await;
            stats.total_entries -= 1;
            stats.cache_evictions += 1;
            
            info!("ç¼“å­˜é©±é€: {}", key);
        }
    }

    async fn cleanup_loop(&self) {
        let mut interval = tokio::time::interval(self.cleanup_interval);
        loop {
            interval.tick().await;
            self.cleanup_expired_entries().await;
        }
    }

    async fn cleanup_expired_entries(&self) {
        let mut cache = self.cache_store.write().await;
        let now = Instant::now();
        let mut to_remove = Vec::new();
        
        for (key, entry) in cache.iter() {
            if now > entry.expires_at {
                to_remove.push(key.clone());
            }
        }
        
        for key in &to_remove {
            cache.remove(key);
            let mut stats = self.cache_stats.write().await;
            stats.total_entries -= 1;
        }
        
        if !to_remove.is_empty() {
            info!("æ¸…ç†è¿‡æœŸç¼“å­˜æ¡ç›®: {} ä¸ª", to_remove.len());
        }
    }

    pub async fn invalidate(&self, pattern: &str) -> Result<usize> {
        let mut cache = self.cache_store.write().await;
        let mut to_remove = Vec::new();
        
        for key in cache.keys() {
            if key.contains(pattern) {
                to_remove.push(key.clone());
            }
        }
        
        for key in to_remove.clone() {
            cache.remove(&key);
        }
        
        let mut stats = self.cache_stats.write().await;
        stats.total_entries -= to_remove.len();
        
        info!("ç¼“å­˜å¤±æ•ˆ: æ¨¡å¼ '{}', ç§»é™¤ {} ä¸ªæ¡ç›®", pattern, to_remove.len());
        Ok(to_remove.len())
    }

    pub async fn get_cache_stats(&self) -> CacheStats {
        self.cache_stats.read().await.clone()
    }
}

/// 4. å¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨
#[derive(Debug, Clone)]
pub struct AsyncQueryOptimizer {
    query_cache: AsyncDatabaseCache,
    execution_plans: Arc<RwLock<HashMap<String, ExecutionPlan>>>,
    optimizer_stats: Arc<RwLock<OptimizerStats>>,
}

#[derive(Debug, Clone)]
pub struct ExecutionPlan {
    pub query_id: String,
    pub estimated_cost: f64,
    pub estimated_rows: usize,
    pub plan_steps: Vec<PlanStep>,
    pub created_at: Instant,
}

#[derive(Debug, Clone)]
pub struct PlanStep {
    pub operation: String,
    pub cost: f64,
    pub rows: usize,
}

#[derive(Debug, Clone, Default)]
pub struct OptimizerStats {
    pub total_queries_optimized: usize,
    pub cache_hits: usize,
    pub plan_generations: usize,
    pub average_optimization_time: Duration,
}

impl AsyncQueryOptimizer {
    pub fn new() -> Self {
        let cache_config = CacheConfig {
            max_size: 500,
            default_ttl: Duration::from_secs(1800),
            cleanup_interval: Duration::from_secs(300),
            enable_compression: true,
        };
        
        Self {
            query_cache: AsyncDatabaseCache::new(cache_config),
            execution_plans: Arc::new(RwLock::new(HashMap::new())),
            optimizer_stats: Arc::new(RwLock::new(OptimizerStats::default())),
        }
    }

    pub async fn optimize_query(&self, query: &str) -> Result<OptimizedQuery> {
        let start_time = Instant::now();
        let query_hash = self.hash_query(query);
        
        // æ£€æŸ¥ç¼“å­˜
        if let Some(cached_result) = self.query_cache.get(&query_hash).await {
            let mut stats = self.optimizer_stats.write().await;
            stats.cache_hits += 1;
            
            return Ok(OptimizedQuery {
                original_query: query.to_string(),
                optimized_query: query.to_string(),
                execution_plan: self.get_execution_plan(&query_hash).await,
                estimated_cost: cached_result.data.len() as f64,
                cache_hit: true,
            });
        }
        
        // ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        let execution_plan = self.generate_execution_plan(query).await?;
        
        // ä¼˜åŒ–æŸ¥è¯¢
        let optimized_query = self.rewrite_query(query, &execution_plan).await?;
        
        // ç¼“å­˜ç»“æœ
        let cache_result = QueryResult {
            rows_affected: 0,
            rows_returned: execution_plan.estimated_rows,
            execution_time: Duration::from_millis(0),
            columns: vec!["cost".to_string()],
            data: vec![vec![execution_plan.estimated_cost.to_string()]],
        };
        
        self.query_cache.set(query_hash.clone(), cache_result, None).await?;
        
        let optimization_time = start_time.elapsed();
        let mut stats = self.optimizer_stats.write().await;
        stats.total_queries_optimized += 1;
        stats.plan_generations += 1;
        stats.average_optimization_time = Duration::from_millis(
            ((stats.average_optimization_time.as_millis() + optimization_time.as_millis()) / 2).try_into().unwrap()
        );
        
        info!("æŸ¥è¯¢ä¼˜åŒ–å®Œæˆ: åŸå§‹æˆæœ¬ {:.2}, ä¼˜åŒ–åæˆæœ¬ {:.2}", 
              execution_plan.estimated_cost, execution_plan.estimated_cost * 0.8);
        
        let estimated_cost = execution_plan.estimated_cost;
        Ok(OptimizedQuery {
            original_query: query.to_string(),
            optimized_query,
            execution_plan: Some(execution_plan),
            estimated_cost,
            cache_hit: false,
        })
    }

    async fn generate_execution_plan(&self, query: &str) -> Result<ExecutionPlan> {
        // æ¨¡æ‹Ÿæ‰§è¡Œè®¡åˆ’ç”Ÿæˆ
        sleep(Duration::from_millis(10)).await;
        
        let query_id = format!("plan_{}", Instant::now().elapsed().as_nanos());
        let estimated_cost = self.estimate_query_cost(query);
        let estimated_rows = self.estimate_result_rows(query);
        
        let plan_steps = vec![
            PlanStep {
                operation: "Table Scan".to_string(),
                cost: estimated_cost * 0.6,
                rows: estimated_rows,
            },
            PlanStep {
                operation: "Filter".to_string(),
                cost: estimated_cost * 0.3,
                rows: estimated_rows / 2,
            },
            PlanStep {
                operation: "Sort".to_string(),
                cost: estimated_cost * 0.1,
                rows: estimated_rows / 2,
            },
        ];
        
        let execution_plan = ExecutionPlan {
            query_id: query_id.clone(),
            estimated_cost,
            estimated_rows,
            plan_steps,
            created_at: Instant::now(),
        };
        
        // ç¼“å­˜æ‰§è¡Œè®¡åˆ’
        {
            let mut plans = self.execution_plans.write().await;
            plans.insert(query_id, execution_plan.clone());
        }
        
        Ok(execution_plan)
    }

    fn estimate_query_cost(&self, query: &str) -> f64 {
        // ç®€åŒ–çš„æˆæœ¬ä¼°ç®—
        let base_cost = 100.0;
        let complexity_factor = query.len() as f64 / 100.0;
        let join_factor = query.matches("JOIN").count() as f64 * 50.0;
        let where_factor = query.matches("WHERE").count() as f64 * 20.0;
        
        base_cost + complexity_factor + join_factor + where_factor
    }

    fn estimate_result_rows(&self, query: &str) -> usize {
        // ç®€åŒ–çš„è¡Œæ•°ä¼°ç®—
        let base_rows = 1000;
        let selectivity = if query.contains("WHERE") { 0.1 } else { 1.0 };
        (base_rows as f64 * selectivity) as usize
    }

    async fn rewrite_query(&self, query: &str, _plan: &ExecutionPlan) -> Result<String> {
        // æ¨¡æ‹ŸæŸ¥è¯¢é‡å†™
        sleep(Duration::from_millis(5)).await;
        
        let mut optimized = query.to_string();
        
        // ç®€å•çš„ä¼˜åŒ–è§„åˆ™
        if query.contains("SELECT *") {
            optimized = optimized.replace("SELECT *", "SELECT id, name");
        }
        
        if query.contains("ORDER BY") && !query.contains("LIMIT") {
            optimized = optimized.replace("ORDER BY", "ORDER BY LIMIT 1000");
        }
        
        Ok(optimized)
    }

    fn hash_query(&self, query: &str) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        query.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }

    async fn get_execution_plan(&self, query_hash: &str) -> Option<ExecutionPlan> {
        let plans = self.execution_plans.read().await;
        plans.get(query_hash).cloned()
    }

    pub async fn get_optimizer_stats(&self) -> OptimizerStats {
        self.optimizer_stats.read().await.clone()
    }
}

#[derive(Debug, Clone)]
pub struct OptimizedQuery {
    pub original_query: String,
    pub optimized_query: String,
    pub execution_plan: Option<ExecutionPlan>,
    pub estimated_cost: f64,
    pub cache_hit: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::fmt::init();
    
    info!("ğŸš€ å¼€å§‹ 2025 å¹´å¼‚æ­¥æ•°æ®åº“æ¨¡å¼æ¼”ç¤º");

    // 1. æ¼”ç¤ºå¼‚æ­¥æ•°æ®åº“è¿æ¥æ± 
    info!("ğŸ”— æ¼”ç¤ºå¼‚æ­¥æ•°æ®åº“è¿æ¥æ± ");
    let pool_config = PoolConfig {
        max_connections: 10,
        min_connections: 3,
        connection_timeout: Duration::from_secs(30),
        idle_timeout: Duration::from_secs(300),
        max_lifetime: Duration::from_secs(1800),
        database_url: "postgresql://localhost:5432/testdb".to_string(),
    };
    
    let pool = AsyncDatabasePool::new(pool_config);
    pool.initialize().await?;
    
    // è·å–è¿æ¥å¹¶æ‰§è¡ŒæŸ¥è¯¢
    let connection = pool.get_connection().await?;
    let result = pool.execute_query(&connection.connection_id, "SELECT * FROM users WHERE id = 1").await?;
    info!("æŸ¥è¯¢ç»“æœ: {} è¡Œ", result.rows_returned);
    
    // é‡Šæ”¾è¿æ¥
    pool.release_connection(connection.connection_id.clone()).await?;
    
    let pool_stats = pool.get_pool_stats().await;
    info!("è¿æ¥æ± ç»Ÿè®¡: æ€»è¿æ¥ {}, æ´»è·ƒ {}, ç©ºé—² {}, æ€»æŸ¥è¯¢ {}", 
          pool_stats.total_connections, pool_stats.active_connections, 
          pool_stats.idle_connections, pool_stats.total_queries);

    // 2. æ¼”ç¤ºå¼‚æ­¥æ•°æ®åº“äº‹åŠ¡ç®¡ç†å™¨
    info!("ğŸ”„ æ¼”ç¤ºå¼‚æ­¥æ•°æ®åº“äº‹åŠ¡ç®¡ç†å™¨");
    let transaction_manager = AsyncTransactionManager::new(pool.clone());
    
    // å¼€å§‹äº‹åŠ¡
    let tx_handle = transaction_manager.begin_transaction(IsolationLevel::ReadCommitted).await?;
    
    // åœ¨äº‹åŠ¡ä¸­æ‰§è¡ŒæŸ¥è¯¢
    let tx_result1 = tx_handle.execute_query("INSERT INTO users (name) VALUES ('Alice')").await?;
    let tx_result2 = tx_handle.execute_query("UPDATE users SET name = 'Alice Smith' WHERE id = 1").await?;
    
    info!("äº‹åŠ¡æŸ¥è¯¢1ç»“æœ: {} è¡Œå—å½±å“", tx_result1.rows_affected);
    info!("äº‹åŠ¡æŸ¥è¯¢2ç»“æœ: {} è¡Œå—å½±å“", tx_result2.rows_affected);
    
    // æäº¤äº‹åŠ¡
    tx_handle.commit().await?;
    
    // æ¼”ç¤ºå›æ»šäº‹åŠ¡
    let tx_handle2 = transaction_manager.begin_transaction(IsolationLevel::Serializable).await?;
    let _ = tx_handle2.execute_query("INSERT INTO users (name) VALUES ('Bob')").await?;
    tx_handle2.rollback().await?;
    
    let tx_stats = transaction_manager.get_transaction_stats().await;
    info!("äº‹åŠ¡ç»Ÿè®¡: æ€»äº‹åŠ¡ {}, æäº¤ {}, å›æ»š {}", 
          tx_stats.total_transactions, tx_stats.committed_transactions, tx_stats.rolled_back_transactions);

    // 3. æ¼”ç¤ºå¼‚æ­¥æ•°æ®åº“ç¼“å­˜ç³»ç»Ÿ
    info!("ğŸ’¾ æ¼”ç¤ºå¼‚æ­¥æ•°æ®åº“ç¼“å­˜ç³»ç»Ÿ");
    let cache_config = CacheConfig {
        max_size: 100,
        default_ttl: Duration::from_secs(300),
        cleanup_interval: Duration::from_secs(60),
        enable_compression: true,
    };
    
    let cache = AsyncDatabaseCache::new(cache_config);
    
    // è®¾ç½®ç¼“å­˜
    let cache_result = QueryResult {
        rows_affected: 0,
        rows_returned: 5,
        execution_time: Duration::from_millis(100),
        columns: vec!["id".to_string(), "name".to_string()],
        data: vec![
            vec!["1".to_string(), "Alice".to_string()],
            vec!["2".to_string(), "Bob".to_string()],
        ],
    };
    
    cache.set("users:all".to_string(), cache_result.clone(), None).await?;
    
    // è·å–ç¼“å­˜
    if let Some(cached_data) = cache.get("users:all").await {
        info!("ç¼“å­˜å‘½ä¸­: {} è¡Œæ•°æ®", cached_data.rows_returned);
    }
    
    // å†æ¬¡è·å–ç¼“å­˜ï¼ˆåº”è¯¥å‘½ä¸­ï¼‰
    if let Some(cached_data) = cache.get("users:all").await {
        info!("ç¼“å­˜å†æ¬¡å‘½ä¸­: {} è¡Œæ•°æ®", cached_data.rows_returned);
    }
    
    // ç¼“å­˜å¤±æ•ˆ
    let invalidated = cache.invalidate("users").await?;
    info!("ç¼“å­˜å¤±æ•ˆ: ç§»é™¤äº† {} ä¸ªæ¡ç›®", invalidated);
    
    let cache_stats = cache.get_cache_stats().await;
    info!("ç¼“å­˜ç»Ÿè®¡: å‘½ä¸­ {}, æœªå‘½ä¸­ {}, é©±é€ {}", 
          cache_stats.cache_hits, cache_stats.cache_misses, cache_stats.cache_evictions);

    // 4. æ¼”ç¤ºå¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨
    info!("âš¡ æ¼”ç¤ºå¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨");
    let optimizer = AsyncQueryOptimizer::new();
    
    // ä¼˜åŒ–æŸ¥è¯¢
    let queries = vec![
        "SELECT * FROM users WHERE age > 25",
        "SELECT u.name, p.title FROM users u JOIN posts p ON u.id = p.user_id ORDER BY p.created_at",
        "SELECT COUNT(*) FROM orders WHERE status = 'completed'",
    ];
    
    for query in queries {
        let optimized = optimizer.optimize_query(query).await?;
        info!("æŸ¥è¯¢ä¼˜åŒ–: åŸå§‹æˆæœ¬ {:.2}, ä¼˜åŒ–åæˆæœ¬ {:.2}, ç¼“å­˜å‘½ä¸­: {}", 
              optimized.estimated_cost, optimized.estimated_cost * 0.8, optimized.cache_hit);
    }
    
    let optimizer_stats = optimizer.get_optimizer_stats().await;
    info!("ä¼˜åŒ–å™¨ç»Ÿè®¡: æ€»ä¼˜åŒ– {}, ç¼“å­˜å‘½ä¸­ {}, è®¡åˆ’ç”Ÿæˆ {}", 
          optimizer_stats.total_queries_optimized, optimizer_stats.cache_hits, optimizer_stats.plan_generations);

    info!("âœ… 2025 å¹´å¼‚æ­¥æ•°æ®åº“æ¨¡å¼æ¼”ç¤ºå®Œæˆ!");
    
    Ok(())
}
