//! å¼‚æ­¥æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º
//! 
//! æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¼‚æ­¥ç¼–ç¨‹ä¸­çš„å„ç§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼š
//! - å†…å­˜ä¼˜åŒ–å’Œé›¶æ‹·è´
//! - å¹¶å‘æ§åˆ¶å’Œèµ„æºç®¡ç†
//! - æ‰¹é‡å¤„ç†ä¼˜åŒ–
//! - ç¼“å­˜ç­–ç•¥
//! - å¼‚æ­¥I/Oä¼˜åŒ–
//! - æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡
//! 
//! è¿è¡Œæ–¹å¼ï¼š
//! ```bash
//! cargo run --example async_performance_demo
//! ```

use std::sync::Arc;
use std::time::{Duration, Instant};
use std::collections::HashMap;
use tokio::sync::{Mutex, RwLock, Semaphore, mpsc};
use tokio::time::sleep;
use futures::StreamExt;
use anyhow::Result;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
struct PerformanceMetrics {
    throughput: f64,
    latency: Duration,
    memory_usage: usize,
    cpu_usage: f64,
    error_rate: f64,
}

struct PerformanceMonitor {
    metrics: Arc<RwLock<HashMap<String, PerformanceMetrics>>>,
    #[allow(dead_code)]
    start_time: Instant,
}

impl PerformanceMonitor {
    fn new() -> Self {
        Self {
            metrics: Arc::new(RwLock::new(HashMap::new())),
            start_time: Instant::now(),
        }
    }

    async fn record_metrics(&self, name: &str, metrics: PerformanceMetrics) {
        let mut map = self.metrics.write().await;
        map.insert(name.to_string(), metrics);
    }

    #[allow(dead_code)]
    async fn get_metrics(&self, name: &str) -> Option<PerformanceMetrics> {
        let map = self.metrics.read().await;
        map.get(name).cloned()
    }

    async fn print_summary(&self) {
        let map = self.metrics.read().await;
        println!("ğŸ“Š æ€§èƒ½ç›‘æ§æ€»ç»“");
        println!("==================");
        
        for (name, metrics) in map.iter() {
            println!("  {}:", name);
            println!("    ååé‡: {:.2} ops/sec", metrics.throughput);
            println!("    å»¶è¿Ÿ: {:?}", metrics.latency);
            println!("    å†…å­˜ä½¿ç”¨: {} MB", metrics.memory_usage / 1024 / 1024);
            println!("    CPUä½¿ç”¨ç‡: {:.1}%", metrics.cpu_usage);
            println!("    é”™è¯¯ç‡: {:.1}%", metrics.error_rate * 100.0);
        }
    }
}

struct PerformanceDemo {
    monitor: PerformanceMonitor,
}

impl PerformanceDemo {
    fn new() -> Self {
        Self {
            monitor: PerformanceMonitor::new(),
        }
    }

    async fn run(&self) -> Result<()> {
        println!("âš¡ å¼‚æ­¥æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º");
        println!("================================");

        // 1. å†…å­˜ä¼˜åŒ–æ¼”ç¤º
        println!("\nğŸ§  1. å†…å­˜ä¼˜åŒ–æ¼”ç¤º");
        self.memory_optimization_demo().await?;

        // 2. å¹¶å‘æ§åˆ¶æ¼”ç¤º
        println!("\nğŸ¯ 2. å¹¶å‘æ§åˆ¶æ¼”ç¤º");
        self.concurrency_control_demo().await?;

        // 3. æ‰¹é‡å¤„ç†ä¼˜åŒ–æ¼”ç¤º
        println!("\nğŸ“¦ 3. æ‰¹é‡å¤„ç†ä¼˜åŒ–æ¼”ç¤º");
        self.batch_processing_demo().await?;

        // 4. ç¼“å­˜ç­–ç•¥æ¼”ç¤º
        println!("\nğŸ’¾ 4. ç¼“å­˜ç­–ç•¥æ¼”ç¤º");
        self.caching_strategy_demo().await?;

        // 5. å¼‚æ­¥I/Oä¼˜åŒ–æ¼”ç¤º
        println!("\nğŸ“¡ 5. å¼‚æ­¥I/Oä¼˜åŒ–æ¼”ç¤º");
        self.async_io_optimization_demo().await?;

        // 6. æ€§èƒ½ç›‘æ§æ¼”ç¤º
        println!("\nğŸ“ˆ 6. æ€§èƒ½ç›‘æ§æ¼”ç¤º");
        self.performance_monitoring_demo().await?;

        // æ˜¾ç¤ºæ€§èƒ½æ€»ç»“
        self.monitor.print_summary().await;

        Ok(())
    }

    async fn memory_optimization_demo(&self) -> Result<()> {
        println!("  å†…å­˜ä¼˜åŒ–æŠ€æœ¯æ¼”ç¤º...");

        // 1. é¿å…ä¸å¿…è¦çš„å…‹éš†
        println!("    1. é¿å…ä¸å¿…è¦çš„å…‹éš†");
        let start = Instant::now();
        
        let large_data = Arc::new(vec![0u8; 1024 * 1024]); // 1MBæ•°æ®
        
        let mut handles = vec![];
        for _i in 0..10 {
            let data = Arc::clone(&large_data); // åªå…‹éš†Arcï¼Œä¸å…‹éš†æ•°æ®
            let handle = tokio::spawn(async move {
                // ä½¿ç”¨å…±äº«æ•°æ®
                data.len()
            });
            handles.push(handle);
        }
        
        let results = futures::future::join_all(handles).await;
        let no_clone_time = start.elapsed();
        println!("      æ— å…‹éš†æ–¹å¼è€—æ—¶: {:?}, ç»“æœæ•°: {}", no_clone_time, results.len());

        // 2. ä½¿ç”¨å¯¹è±¡æ± 
        println!("    2. å¯¹è±¡æ± ä¼˜åŒ–");
        let start = Instant::now();
        
        let object_pool = Arc::new(Semaphore::new(5)); // æœ€å¤š5ä¸ªå¯¹è±¡
        let mut handles = vec![];
        
        for i in 0..20 {
            let pool = Arc::clone(&object_pool);
            let handle = tokio::spawn(async move {
                let _permit = pool.acquire().await.unwrap();
                // æ¨¡æ‹Ÿå¯¹è±¡ä½¿ç”¨
                sleep(Duration::from_millis(50)).await;
                i
            });
            handles.push(handle);
        }
        
        futures::future::join_all(handles).await;
        let pool_time = start.elapsed();
        println!("      å¯¹è±¡æ± æ–¹å¼è€—æ—¶: {:?}", pool_time);

        // 3. é›¶æ‹·è´ä¼˜åŒ–
        println!("    3. é›¶æ‹·è´ä¼˜åŒ–");
        let start = Instant::now();
        
        let data = b"Hello, World!";
        let mut handles = vec![];
        
        for i in 0..100 {
            let handle = tokio::spawn(async move {
                // ä½¿ç”¨å­—èŠ‚åˆ‡ç‰‡çš„å¼•ç”¨ï¼Œé¿å…æ‹·è´
                let slice = &data[..];
                slice.len() + i
            });
            handles.push(handle);
        }
        
        futures::future::join_all(handles).await;
        let zero_copy_time = start.elapsed();
        println!("      é›¶æ‹·è´æ–¹å¼è€—æ—¶: {:?}", zero_copy_time);

        // è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.monitor.record_metrics("å†…å­˜ä¼˜åŒ–", PerformanceMetrics {
            throughput: 1000.0 / no_clone_time.as_secs_f64(),
            latency: no_clone_time,
            memory_usage: 1024 * 1024, // 1MB
            cpu_usage: 10.0,
            error_rate: 0.0,
        }).await;

        Ok(())
    }

    async fn concurrency_control_demo(&self) -> Result<()> {
        println!("  å¹¶å‘æ§åˆ¶æŠ€æœ¯æ¼”ç¤º...");

        // 1. ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        println!("    1. ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°");
        let start = Instant::now();
        
        let semaphore = Arc::new(Semaphore::new(5)); // æœ€å¤š5ä¸ªå¹¶å‘
        let mut handles = vec![];
        
        for i in 0..20 {
            let sem = Arc::clone(&semaphore);
            let handle = tokio::spawn(async move {
                let _permit = sem.acquire().await.unwrap();
                sleep(Duration::from_millis(100)).await;
                i
            });
            handles.push(handle);
        }
        
        futures::future::join_all(handles).await;
        let semaphore_time = start.elapsed();
        println!("      ä¿¡å·é‡æ§åˆ¶è€—æ—¶: {:?}", semaphore_time);

        // 2. æ— é™åˆ¶å¹¶å‘ï¼ˆå¯¹æ¯”ï¼‰
        println!("    2. æ— é™åˆ¶å¹¶å‘ï¼ˆå¯¹æ¯”ï¼‰");
        let start = Instant::now();
        
        let mut handles = vec![];
        for i in 0..20 {
            let handle = tokio::spawn(async move {
                sleep(Duration::from_millis(100)).await;
                i
            });
            handles.push(handle);
        }
        
        futures::future::join_all(handles).await;
        let unlimited_time = start.elapsed();
        println!("      æ— é™åˆ¶å¹¶å‘è€—æ—¶: {:?}", unlimited_time);

        // 3. è‡ªé€‚åº”å¹¶å‘æ§åˆ¶
        println!("    3. è‡ªé€‚åº”å¹¶å‘æ§åˆ¶");
        let start = Instant::now();
        
        let adaptive_semaphore = Arc::new(Mutex::new(Semaphore::new(1)));
        let mut handles = vec![];
        
        for i in 0..10 {
            let sem = Arc::clone(&adaptive_semaphore);
            let handle = tokio::spawn(async move {
                let semaphore = sem.lock().await;
                let _permit = semaphore.acquire().await.unwrap();
                sleep(Duration::from_millis(100)).await;
                i
            });
            handles.push(handle);
        }
        
        futures::future::join_all(handles).await;
        let adaptive_time = start.elapsed();
        println!("      è‡ªé€‚åº”å¹¶å‘è€—æ—¶: {:?}", adaptive_time);

        // è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.monitor.record_metrics("å¹¶å‘æ§åˆ¶", PerformanceMetrics {
            throughput: 20.0 / semaphore_time.as_secs_f64(),
            latency: semaphore_time,
            memory_usage: 1024 * 100, // 100KB
            cpu_usage: 15.0,
            error_rate: 0.0,
        }).await;

        Ok(())
    }

    async fn batch_processing_demo(&self) -> Result<()> {
        println!("  æ‰¹é‡å¤„ç†ä¼˜åŒ–æ¼”ç¤º...");

        let total_items = 1000;
        let batch_sizes = vec![1, 10, 50, 100];

        for batch_size in batch_sizes {
            println!("    æ‰¹é‡å¤§å°: {}", batch_size);
            let start = Instant::now();
            
            let mut handles = vec![];
            
            for batch_start in (0..total_items).step_by(batch_size) {
                let batch_end = (batch_start + batch_size).min(total_items);
                let items_in_batch = batch_end - batch_start;
                
                let handle = tokio::spawn(async move {
                    // æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†
                    sleep(Duration::from_millis(items_in_batch as u64 * 10)).await;
                    items_in_batch
                });
                
                handles.push(handle);
            }
            
            let results = futures::future::join_all(handles).await;
            let duration = start.elapsed();
            let total_processed: usize = results.iter().map(|r| r.as_ref().unwrap_or(&0)).sum();
            
            println!("      å¤„ç† {} é¡¹ï¼Œè€—æ—¶: {:?}, ååé‡: {:.2} items/sec", 
                total_processed, duration, total_processed as f64 / duration.as_secs_f64());
        }

        Ok(())
    }

    async fn caching_strategy_demo(&self) -> Result<()> {
        println!("  ç¼“å­˜ç­–ç•¥æ¼”ç¤º...");

        // 1. LRUç¼“å­˜
        println!("    1. LRUç¼“å­˜ç­–ç•¥");
        let start = Instant::now();
        
        let cache = Arc::new(RwLock::new(HashMap::new()));
        let mut handles = vec![];
        
        for i in 0..100 {
            let cache = Arc::clone(&cache);
            let handle = tokio::spawn(async move {
                let key = format!("key_{}", i % 20); // åªæœ‰20ä¸ªä¸åŒçš„key
                
                // å°è¯•ä»ç¼“å­˜è¯»å–
                {
                    let cache = cache.read().await;
                    if cache.contains_key(&key) {
                        return ("hit", key);
                    }
                }
                
                // ç¼“å­˜æœªå‘½ä¸­ï¼Œæ¨¡æ‹Ÿè®¡ç®—
                sleep(Duration::from_millis(10)).await;
                
                // å†™å…¥ç¼“å­˜
                {
                    let mut cache = cache.write().await;
                    cache.insert(key.clone(), format!("value_{}", i));
                }
                
                ("miss", key)
            });
            handles.push(handle);
        }
        
        let results = futures::future::join_all(handles).await;
        let cache_time = start.elapsed();
        
        let hits = results.iter().filter(|r| r.as_ref().unwrap().0 == "hit").count();
        let misses = results.len() - hits;
        let hit_rate = hits as f64 / results.len() as f64;
        
        println!("      ç¼“å­˜å‘½ä¸­ç‡: {:.1}%", hit_rate * 100.0);
        println!("      å‘½ä¸­: {}, æœªå‘½ä¸­: {}", hits, misses);
        println!("      ç¼“å­˜æ“ä½œè€—æ—¶: {:?}", cache_time);

        // 2. ç¼“å­˜é¢„çƒ­
        println!("    2. ç¼“å­˜é¢„çƒ­ç­–ç•¥");
        let start = Instant::now();
        
        let warmup_cache = Arc::new(RwLock::new(HashMap::new()));
        
        // é¢„çƒ­ç¼“å­˜
        {
            let mut cache = warmup_cache.write().await;
            for i in 0..50 {
                cache.insert(format!("warmup_key_{}", i), format!("warmup_value_{}", i));
            }
        }
        
        // æµ‹è¯•é¢„çƒ­åçš„æ€§èƒ½
        let mut handles = vec![];
        for i in 0..100 {
            let cache = Arc::clone(&warmup_cache);
            let handle = tokio::spawn(async move {
                let key = format!("warmup_key_{}", i % 50);
                
                let cache = cache.read().await;
                cache.get(&key).is_some()
            });
            handles.push(handle);
        }
        
        futures::future::join_all(handles).await;
        let warmup_time = start.elapsed();
        
        println!("      é¢„çƒ­ç¼“å­˜è€—æ—¶: {:?}", warmup_time);

        // è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.monitor.record_metrics("ç¼“å­˜ç­–ç•¥", PerformanceMetrics {
            throughput: 100.0 / cache_time.as_secs_f64(),
            latency: cache_time,
            memory_usage: 1024 * 50, // 50KB
            cpu_usage: 5.0,
            error_rate: 1.0 - hit_rate,
        }).await;

        Ok(())
    }

    async fn async_io_optimization_demo(&self) -> Result<()> {
        println!("  å¼‚æ­¥I/Oä¼˜åŒ–æ¼”ç¤º...");

        // 1. æ‰¹é‡I/Oæ“ä½œ
        println!("    1. æ‰¹é‡I/Oæ“ä½œ");
        let start = Instant::now();
        
        let (tx, mut rx) = mpsc::channel(1000);
        
        // ç”Ÿäº§è€…ï¼šæ‰¹é‡å‘é€
        let producer = tokio::spawn(async move {
            let mut batch = Vec::new();
            for i in 0..100 {
                batch.push(format!("data_{}", i));
                
                if batch.len() >= 10 {
                    let batch_data = batch.drain(..).collect::<Vec<_>>();
                    if tx.send(batch_data).await.is_err() {
                        break;
                    }
                }
            }
            
            // å‘é€å‰©ä½™æ•°æ®
            if !batch.is_empty() {
                let _ = tx.send(batch).await;
            }
        });
        
        // æ¶ˆè´¹è€…ï¼šæ‰¹é‡æ¥æ”¶
        let consumer = tokio::spawn(async move {
            let mut total_received = 0;
            while let Some(batch) = rx.recv().await {
                total_received += batch.len();
                // æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†
                sleep(Duration::from_millis(batch.len() as u64 * 5)).await;
            }
            total_received
        });
        
        let (_, total_received) = tokio::join!(producer, consumer);
        let batch_io_time = start.elapsed();
        
        println!("      æ‰¹é‡I/Oå¤„ç† {} é¡¹ï¼Œè€—æ—¶: {:?}", total_received.unwrap_or(0), batch_io_time);

        // 2. æµå¼å¤„ç†
        println!("    2. æµå¼å¤„ç†ä¼˜åŒ–");
        let start = Instant::now();
        
        let stream = futures::stream::iter(0..100)
            .map(|i| async move {
                sleep(Duration::from_millis(10)).await;
                i
            })
            .buffer_unordered(10); // å¹¶å‘åº¦10
        
        let results: Vec<_> = stream.collect().await;
        let stream_time = start.elapsed();
        
        println!("      æµå¼å¤„ç† {} é¡¹ï¼Œè€—æ—¶: {:?}", results.len(), stream_time);

        // 3. èƒŒå‹å¤„ç†
        println!("    3. èƒŒå‹å¤„ç†ä¼˜åŒ–");
        let start = Instant::now();
        
        let (tx, mut rx) = mpsc::channel(5); // å°ç¼“å†²åŒº
        let semaphore = Arc::new(Semaphore::new(3)); // é™åˆ¶ç”Ÿäº§è€…
        
        // å¿«é€Ÿç”Ÿäº§è€…
        let producer = {
            let tx = tx.clone();
            let semaphore = Arc::clone(&semaphore);
            tokio::spawn(async move {
                for i in 0..20 {
                    let _permit = semaphore.acquire().await.unwrap();
                    
                    match tx.try_send(i) {
                        Ok(_) => {
                            println!("        å‘é€æ•°æ®: {}", i);
                        }
                        Err(_) => {
                            println!("        èƒŒå‹ï¼šç­‰å¾…æ¶ˆè´¹è€…å¤„ç†");
                            sleep(Duration::from_millis(100)).await;
                            continue;
                        }
                    }
                    
                    sleep(Duration::from_millis(50)).await;
                }
            })
        };
        
        // æ…¢é€Ÿæ¶ˆè´¹è€…
        let consumer = tokio::spawn(async move {
            while let Some(data) = rx.recv().await {
                println!("        å¤„ç†æ•°æ®: {}", data);
                sleep(Duration::from_millis(200)).await; // æ…¢é€Ÿå¤„ç†
            }
        });
        
        let _ = tokio::join!(producer, consumer);
        let backpressure_time = start.elapsed();
        
        println!("      èƒŒå‹å¤„ç†è€—æ—¶: {:?}", backpressure_time);

        // è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.monitor.record_metrics("å¼‚æ­¥I/Oä¼˜åŒ–", PerformanceMetrics {
            throughput: 100.0 / batch_io_time.as_secs_f64(),
            latency: batch_io_time,
            memory_usage: 1024 * 20, // 20KB
            cpu_usage: 20.0,
            error_rate: 0.0,
        }).await;

        Ok(())
    }

    async fn performance_monitoring_demo(&self) -> Result<()> {
        println!("  æ€§èƒ½ç›‘æ§æ¼”ç¤º...");

        // æ¨¡æ‹Ÿæ€§èƒ½ç›‘æ§
        let start = Instant::now();
        
        let mut handles = vec![];
        
        for i in 0..10 {
            let handle = tokio::spawn(async move {
                let task_start = Instant::now();
                
                // æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½
                sleep(Duration::from_millis(100 + i * 10)).await;
                
                let task_duration = task_start.elapsed();
                
                // æ¨¡æ‹Ÿéšæœºå¤±è´¥
                if rand::random::<f32>() < 0.1 {
                    return (task_duration, true);
                }
                
                (task_duration, false)
            });
            
            handles.push(handle);
        }
        
        let results = futures::future::join_all(handles).await;
        let total_time = start.elapsed();
        
        let mut total_latency = Duration::ZERO;
        let mut error_count = 0;
        
        for result in results {
            match result {
                Ok((duration, is_error)) => {
                    total_latency += duration;
                    if is_error {
                        error_count += 1;
                    }
                }
                Err(_) => {
                    error_count += 1;
                }
            }
        }
        
        let avg_latency = total_latency / 10;
        let error_rate = error_count as f64 / 10.0;
        let throughput = 10.0 / total_time.as_secs_f64();
        
        println!("      æ€»è€—æ—¶: {:?}", total_time);
        println!("      å¹³å‡å»¶è¿Ÿ: {:?}", avg_latency);
        println!("      ååé‡: {:.2} ops/sec", throughput);
        println!("      é”™è¯¯ç‡: {:.1}%", error_rate * 100.0);
        
        // è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.monitor.record_metrics("æ€§èƒ½ç›‘æ§", PerformanceMetrics {
            throughput,
            latency: avg_latency,
            memory_usage: 1024 * 10, // 10KB
            cpu_usage: 25.0,
            error_rate,
        }).await;

        Ok(())
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let demo = PerformanceDemo::new();
    demo.run().await?;

    println!("\nğŸ‰ å¼‚æ­¥æ€§èƒ½ä¼˜åŒ–æ¼”ç¤ºå®Œæˆï¼");
    Ok(())
}
