# 并发算法模式

> **文档类型**: Tier 4 - 高级主题
> **最后更新**: 2025-10-23
> **状态**: ✅ 完成

---

## 目录

- [并发算法模式](#并发算法模式)
  - [目录](#目录)
  - [1. 并发计算模型](#1-并发计算模型)
    - [1.1 Actor 模型](#11-actor-模型)
    - [1.2 CSP (Communicating Sequential Processes)](#12-csp-communicating-sequential-processes)
    - [1.3 Reactor 模式](#13-reactor-模式)
  - [2. 无锁数据结构](#2-无锁数据结构)
    - [2.1 无锁队列](#21-无锁队列)
    - [2.2 无锁栈](#22-无锁栈)
    - [2.3 CAS 操作与 ABA 问题](#23-cas-操作与-aba-问题)
  - [3. 并发算法设计](#3-并发算法设计)
    - [3.1 数据并行](#31-数据并行)
    - [3.2 任务并行](#32-任务并行)
    - [3.3 流水线并行](#33-流水线并行)
  - [4. 同步原语](#4-同步原语)
    - [4.1 屏障 (Barrier)](#41-屏障-barrier)
    - [4.2 信号量 (Semaphore)](#42-信号量-semaphore)
    - [4.3 读写锁 (RwLock)](#43-读写锁-rwlock)
  - [5. 并发算法分析](#5-并发算法分析)
    - [5.1 加速比与效率](#51-加速比与效率)
    - [5.2 工作深度模型](#52-工作深度模型)
    - [5.3 负载均衡](#53-负载均衡)
  - [6. 实践案例](#6-实践案例)
    - [6.1 并发归并排序](#61-并发归并排序)
    - [6.2 并发图算法](#62-并发图算法)
    - [6.3 并发哈希表](#63-并发哈希表)
  - [7. 参考资料](#7-参考资料)
    - [教材](#教材)
    - [Rust 并发库](#rust-并发库)
    - [形式化验证](#形式化验证)

---

## 1. 并发计算模型

### 1.1 Actor 模型

**核心思想**: 每个 Actor 是独立的计算单元，通过消息传递通信。

```rust
use tokio::sync::mpsc;
use tokio::task;

/// Actor 特征
#[async_trait::async_trait]
pub trait Actor: Send + 'static {
    type Message: Send;

    async fn handle(&mut self, msg: Self::Message);
}

/// Actor 系统
pub struct ActorSystem<A: Actor> {
    sender: mpsc::Sender<A::Message>,
}

impl<A: Actor> ActorSystem<A> {
    /// 创建 Actor 系统
    pub fn spawn(mut actor: A) -> Self {
        let (tx, mut rx) = mpsc::channel(100);

        task::spawn(async move {
            while let Some(msg) = rx.recv().await {
                actor.handle(msg).await;
            }
        });

        Self { sender: tx }
    }

    /// 发送消息
    pub async fn send(&self, msg: A::Message) -> Result<(), mpsc::error::SendError<A::Message>> {
        self.sender.send(msg).await
    }
}

/// 示例：计数器 Actor
pub struct Counter {
    count: i32,
}

pub enum CounterMsg {
    Increment,
    Decrement,
    Get(tokio::sync::oneshot::Sender<i32>),
}

#[async_trait::async_trait]
impl Actor for Counter {
    type Message = CounterMsg;

    async fn handle(&mut self, msg: Self::Message) {
        match msg {
            CounterMsg::Increment => self.count += 1,
            CounterMsg::Decrement => self.count -= 1,
            CounterMsg::Get(reply) => {
                let _ = reply.send(self.count);
            }
        }
    }
}

/// 使用示例
pub async fn actor_example() {
    let counter = Counter { count: 0 };
    let system = ActorSystem::spawn(counter);

    // 并发发送消息
    for _ in 0..100 {
        let sys = system.clone();
        task::spawn(async move {
            let _ = sys.send(CounterMsg::Increment).await;
        });
    }

    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;

    let (tx, rx) = tokio::sync::oneshot::channel();
    system.send(CounterMsg::Get(tx)).await.unwrap();
    let count = rx.await.unwrap();
    println!("最终计数: {}", count);
}
```

### 1.2 CSP (Communicating Sequential Processes)

**核心思想**: 通过 channel 进行同步通信。

```rust
use std::sync::mpsc;
use std::thread;

/// CSP 风格的并发素数筛
pub fn prime_sieve_csp(n: usize) -> Vec<usize> {
    let (mut sender, mut receiver) = mpsc::channel();

    // 生成器：发送 2..n
    thread::spawn(move || {
        for i in 2..=n {
            if sender.send(i).is_err() {
                break;
            }
        }
    });

    let mut primes = Vec::new();

    while let Ok(prime) = receiver.recv() {
        primes.push(prime);

        // 创建新的筛选器
        let (new_sender, new_receiver) = mpsc::channel();

        thread::spawn(move || {
            while let Ok(num) = receiver.recv() {
                if num % prime != 0 {
                    if new_sender.send(num).is_err() {
                        break;
                    }
                }
            }
        });

        receiver = new_receiver;
    }

    primes
}

/// CSP 风格的生产者-消费者
pub fn producer_consumer_csp() {
    let (tx, rx) = mpsc::sync_channel(10); // 有界 channel

    // 生产者
    let producer = thread::spawn(move || {
        for i in 0..100 {
            tx.send(i).unwrap();
            println!("生产: {}", i);
        }
    });

    // 消费者
    let consumer = thread::spawn(move || {
        while let Ok(item) = rx.recv() {
            println!("消费: {}", item);
            thread::sleep(std::time::Duration::from_millis(10));
        }
    });

    producer.join().unwrap();
    consumer.join().unwrap();
}
```

### 1.3 Reactor 模式

**核心思想**: 事件驱动的异步 I/O 处理。

```rust
use tokio::io::{AsyncReadExt, AsyncWriteExt};
use tokio::net::{TcpListener, TcpStream};

/// Reactor 模式示例：Echo 服务器
pub async fn reactor_echo_server(addr: &str) -> std::io::Result<()> {
    let listener = TcpListener::bind(addr).await?;
    println!("服务器监听: {}", addr);

    loop {
        let (socket, addr) = listener.accept().await?;
        println!("新连接: {}", addr);

        // 为每个连接创建异步任务
        tokio::spawn(async move {
            handle_client(socket).await;
        });
    }
}

async fn handle_client(mut socket: TcpStream) {
    let mut buf = vec![0u8; 1024];

    loop {
        match socket.read(&mut buf).await {
            Ok(0) => break, // 连接关闭
            Ok(n) => {
                if socket.write_all(&buf[..n]).await.is_err() {
                    break;
                }
            }
            Err(_) => break,
        }
    }
}

/// Reactor 模式：多路复用 HTTP 请求
pub async fn reactor_http_requests(urls: Vec<String>) -> Vec<String> {
    use tokio::time::{timeout, Duration};

    let tasks: Vec<_> = urls
        .into_iter()
        .map(|url| {
            tokio::spawn(async move {
                let client = reqwest::Client::new();
                match timeout(Duration::from_secs(5), client.get(&url).send()).await {
                    Ok(Ok(response)) => response.text().await.unwrap_or_default(),
                    _ => String::from("请求失败"),
                }
            })
        })
        .collect();

    let results = futures::future::join_all(tasks).await;
    results.into_iter().map(|r| r.unwrap_or_default()).collect()
}
```

---

## 2. 无锁数据结构

### 2.1 无锁队列

```rust
use std::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};
use std::ptr;

/// Michael-Scott 无锁队列
pub struct LockFreeQueue<T> {
    head: AtomicPtr<Node<T>>,
    tail: AtomicPtr<Node<T>>,
    size: AtomicUsize,
}

struct Node<T> {
    data: Option<T>,
    next: AtomicPtr<Node<T>>,
}

impl<T> LockFreeQueue<T> {
    pub fn new() -> Self {
        let dummy = Box::into_raw(Box::new(Node {
            data: None,
            next: AtomicPtr::new(ptr::null_mut()),
        }));

        Self {
            head: AtomicPtr::new(dummy),
            tail: AtomicPtr::new(dummy),
            size: AtomicUsize::new(0),
        }
    }

    /// 入队（无锁）
    pub fn enqueue(&self, value: T) {
        let node = Box::into_raw(Box::new(Node {
            data: Some(value),
            next: AtomicPtr::new(ptr::null_mut()),
        }));

        loop {
            let tail = self.tail.load(Ordering::Acquire);
            let next = unsafe { (*tail).next.load(Ordering::Acquire) };

            if tail == self.tail.load(Ordering::Acquire) {
                if next.is_null() {
                    // 尝试链接新节点
                    if unsafe {
                        (*tail).next.compare_exchange(
                            next,
                            node,
                            Ordering::Release,
                            Ordering::Relaxed,
                        ).is_ok()
                    } {
                        // 成功，更新 tail
                        let _ = self.tail.compare_exchange(
                            tail,
                            node,
                            Ordering::Release,
                            Ordering::Relaxed,
                        );
                        self.size.fetch_add(1, Ordering::Relaxed);
                        return;
                    }
                } else {
                    // 帮助其他线程更新 tail
                    let _ = self.tail.compare_exchange(
                        tail,
                        next,
                        Ordering::Release,
                        Ordering::Relaxed,
                    );
                }
            }
        }
    }

    /// 出队（无锁）
    pub fn dequeue(&self) -> Option<T> {
        loop {
            let head = self.head.load(Ordering::Acquire);
            let tail = self.tail.load(Ordering::Acquire);
            let next = unsafe { (*head).next.load(Ordering::Acquire) };

            if head == self.head.load(Ordering::Acquire) {
                if head == tail {
                    if next.is_null() {
                        return None; // 队列为空
                    }
                    // 帮助更新 tail
                    let _ = self.tail.compare_exchange(
                        tail,
                        next,
                        Ordering::Release,
                        Ordering::Relaxed,
                    );
                } else {
                    // 读取数据
                    let data = unsafe { (*next).data.take() };

                    // 尝试更新 head
                    if self.head.compare_exchange(
                        head,
                        next,
                        Ordering::Release,
                        Ordering::Relaxed,
                    ).is_ok() {
                        self.size.fetch_sub(1, Ordering::Relaxed);
                        // 释放旧 head（dummy 节点）
                        unsafe { Box::from_raw(head) };
                        return data;
                    }
                }
            }
        }
    }

    pub fn len(&self) -> usize {
        self.size.load(Ordering::Relaxed)
    }
}

impl<T> Drop for LockFreeQueue<T> {
    fn drop(&mut self) {
        while self.dequeue().is_some() {}
        unsafe {
            Box::from_raw(self.head.load(Ordering::Relaxed));
        }
    }
}
```

### 2.2 无锁栈

```rust
use std::sync::atomic::{AtomicPtr, Ordering};
use std::ptr;

/// Treiber 无锁栈
pub struct LockFreeStack<T> {
    head: AtomicPtr<StackNode<T>>,
}

struct StackNode<T> {
    data: T,
    next: *mut StackNode<T>,
}

impl<T> LockFreeStack<T> {
    pub fn new() -> Self {
        Self {
            head: AtomicPtr::new(ptr::null_mut()),
        }
    }

    /// 入栈（无锁）
    pub fn push(&self, value: T) {
        let node = Box::into_raw(Box::new(StackNode {
            data: value,
            next: ptr::null_mut(),
        }));

        loop {
            let old_head = self.head.load(Ordering::Relaxed);
            unsafe { (*node).next = old_head };

            if self.head.compare_exchange(
                old_head,
                node,
                Ordering::Release,
                Ordering::Relaxed,
            ).is_ok() {
                return;
            }
        }
    }

    /// 出栈（无锁）
    pub fn pop(&self) -> Option<T> {
        loop {
            let old_head = self.head.load(Ordering::Acquire);

            if old_head.is_null() {
                return None;
            }

            let next = unsafe { (*old_head).next };

            if self.head.compare_exchange(
                old_head,
                next,
                Ordering::Release,
                Ordering::Relaxed,
            ).is_ok() {
                let node = unsafe { Box::from_raw(old_head) };
                return Some(node.data);
            }
        }
    }
}

impl<T> Drop for LockFreeStack<T> {
    fn drop(&mut self) {
        while self.pop().is_some() {}
    }
}
```

### 2.3 CAS 操作与 ABA 问题

```rust
use std::sync::atomic::{AtomicUsize, Ordering};

/// ABA 问题演示
pub fn aba_problem_demo() {
    let shared = AtomicUsize::new(100);

    // 线程 1: 读取 A
    let a = shared.load(Ordering::Relaxed);
    assert_eq!(a, 100);

    // 线程 2: A -> B -> A
    shared.store(200, Ordering::Relaxed);
    shared.store(100, Ordering::Relaxed);

    // 线程 1: CAS 成功，但中间状态被改变
    let success = shared.compare_exchange(
        a,
        300,
        Ordering::Release,
        Ordering::Relaxed,
    ).is_ok();

    assert!(success); // CAS 成功，但没有检测到中间变化
    println!("ABA 问题：CAS 成功但状态已改变");
}

/// 解决 ABA 问题：版本号标记
pub struct VersionedPointer<T> {
    ptr: AtomicUsize, // 高位：版本号，低位：指针
}

impl<T> VersionedPointer<T> {
    const VERSION_BITS: usize = 16;
    const PTR_MASK: usize = (1 << (usize::BITS - Self::VERSION_BITS as u32)) - 1;
    const VERSION_MASK: usize = !Self::PTR_MASK;

    pub fn new(ptr: *mut T) -> Self {
        Self {
            ptr: AtomicUsize::new(ptr as usize),
        }
    }

    pub fn load(&self) -> (*mut T, usize) {
        let val = self.ptr.load(Ordering::Acquire);
        let ptr = (val & Self::PTR_MASK) as *mut T;
        let version = (val & Self::VERSION_MASK) >> (usize::BITS as usize - Self::VERSION_BITS);
        (ptr, version)
    }

    pub fn compare_exchange(
        &self,
        old_ptr: *mut T,
        old_version: usize,
        new_ptr: *mut T,
        new_version: usize,
    ) -> Result<(*mut T, usize), (*mut T, usize)> {
        let old_val = (old_ptr as usize & Self::PTR_MASK)
            | ((old_version << (usize::BITS as usize - Self::VERSION_BITS)) & Self::VERSION_MASK);
        let new_val = (new_ptr as usize & Self::PTR_MASK)
            | ((new_version << (usize::BITS as usize - Self::VERSION_BITS)) & Self::VERSION_MASK);

        self.ptr.compare_exchange(
            old_val,
            new_val,
            Ordering::Release,
            Ordering::Acquire,
        ).map(|_| self.load())
        .map_err(|_| self.load())
    }
}
```

---

## 3. 并发算法设计

### 3.1 数据并行

```rust
use rayon::prelude::*;

/// 并发矩阵乘法（数据并行）
pub fn parallel_matrix_multiply(
    a: &[Vec<f64>],
    b: &[Vec<f64>],
) -> Vec<Vec<f64>> {
    let n = a.len();
    let m = b[0].len();
    let k = b.len();

    // 转置 B 以提高缓存局部性
    let b_t: Vec<Vec<f64>> = (0..m)
        .map(|j| (0..k).map(|i| b[i][j]).collect())
        .collect();

    // 并行计算每一行
    a.par_iter()
        .map(|row| {
            b_t.iter()
                .map(|col| {
                    row.iter().zip(col.iter()).map(|(x, y)| x * y).sum()
                })
                .collect()
        })
        .collect()
}

/// 并发归约（数据并行）
pub fn parallel_reduce<T, F>(arr: &[T], init: T, op: F) -> T
where
    T: Send + Sync + Clone,
    F: Fn(T, T) -> T + Send + Sync,
{
    arr.par_iter()
        .cloned()
        .reduce(|| init.clone(), |a, b| op(a, b))
}

/// 示例：并发求和
pub fn parallel_sum(arr: &[i64]) -> i64 {
    parallel_reduce(arr, 0, |a, b| a + b)
}
```

### 3.2 任务并行

```rust
use std::thread;

/// 任务并行：斐波那契数列
pub fn fibonacci_task_parallel(n: u32) -> u64 {
    if n <= 1 {
        return n as u64;
    }

    if n < 30 {
        // 小问题直接计算
        return fibonacci_sequential(n);
    }

    // 并行计算 fib(n-1) 和 fib(n-2)
    let handle = thread::spawn(move || fibonacci_task_parallel(n - 1));
    let right = fibonacci_task_parallel(n - 2);
    let left = handle.join().unwrap();

    left + right
}

fn fibonacci_sequential(n: u32) -> u64 {
    match n {
        0 => 0,
        1 => 1,
        n => fibonacci_sequential(n - 1) + fibonacci_sequential(n - 2),
    }
}

/// 任务并行：快速排序
pub fn quicksort_task_parallel<T: Ord + Send>(arr: &mut [T], threshold: usize) {
    if arr.len() <= threshold {
        arr.sort_unstable();
        return;
    }

    let pivot_idx = partition_qs(arr);

    let (left, right) = arr.split_at_mut(pivot_idx);

    thread::scope(|s| {
        s.spawn(|| quicksort_task_parallel(left, threshold));
        s.spawn(|| quicksort_task_parallel(&mut right[1..], threshold));
    });
}

fn partition_qs<T: Ord>(arr: &mut [T]) -> usize {
    let mut i = 1;
    for j in 1..arr.len() {
        if arr[j] < arr[0] {
            arr.swap(i, j);
            i += 1;
        }
    }
    arr.swap(0, i - 1);
    i - 1
}
```

### 3.3 流水线并行

```rust
use std::sync::mpsc;
use std::thread;

/// 流水线并行：图像处理
pub fn pipeline_image_processing(images: Vec<Vec<u8>>) -> Vec<Vec<u8>> {
    let (tx1, rx1) = mpsc::channel();
    let (tx2, rx2) = mpsc::channel();
    let (tx3, rx3) = mpsc::channel();

    // 阶段 1: 灰度化
    let stage1 = thread::spawn(move || {
        while let Ok(img) = rx1.recv() {
            let gray = to_grayscale(img);
            if tx2.send(gray).is_err() {
                break;
            }
        }
    });

    // 阶段 2: 模糊
    let stage2 = thread::spawn(move || {
        while let Ok(img) = rx2.recv() {
            let blurred = blur(img);
            if tx3.send(blurred).is_err() {
                break;
            }
        }
    });

    // 阶段 3: 边缘检测
    let stage3 = thread::spawn(move || {
        let mut results = Vec::new();
        while let Ok(img) = rx3.recv() {
            let edges = edge_detection(img);
            results.push(edges);
        }
        results
    });

    // 输入数据
    for img in images {
        tx1.send(img).unwrap();
    }
    drop(tx1);

    stage1.join().unwrap();
    stage2.join().unwrap();
    stage3.join().unwrap()
}

fn to_grayscale(img: Vec<u8>) -> Vec<u8> {
    img // 简化实现
}

fn blur(img: Vec<u8>) -> Vec<u8> {
    img // 简化实现
}

fn edge_detection(img: Vec<u8>) -> Vec<u8> {
    img // 简化实现
}
```

---

## 4. 同步原语

### 4.1 屏障 (Barrier)

```rust
use std::sync::{Arc, Barrier};
use std::thread;

/// 并发屏障示例：矩阵迭代计算
pub fn matrix_iteration_with_barrier(matrix: Vec<Vec<f64>>, iterations: usize) -> Vec<Vec<f64>> {
    let n = matrix.len();
    let m = matrix[0].len();

    let shared_matrix = Arc::new(std::sync::RwLock::new(matrix));
    let barrier = Arc::new(Barrier::new(4)); // 4 个工作线程

    let handles: Vec<_> = (0..4)
        .map(|id| {
            let matrix = Arc::clone(&shared_matrix);
            let barrier = Arc::clone(&barrier);

            thread::spawn(move || {
                for _ in 0..iterations {
                    // 每个线程处理一部分行
                    let start = id * n / 4;
                    let end = (id + 1) * n / 4;

                    {
                        let mut mat = matrix.write().unwrap();
                        for i in start..end {
                            for j in 1..m - 1 {
                                mat[i][j] = (mat[i][j - 1] + mat[i][j + 1]) / 2.0;
                            }
                        }
                    }

                    // 等待所有线程完成本轮迭代
                    barrier.wait();
                }
            })
        })
        .collect();

    for handle in handles {
        handle.join().unwrap();
    }

    Arc::try_unwrap(shared_matrix).unwrap().into_inner().unwrap()
}
```

### 4.2 信号量 (Semaphore)

```rust
use tokio::sync::Semaphore;
use std::sync::Arc;

/// 并发限流：限制并发数
pub async fn rate_limited_tasks(tasks: Vec<String>, max_concurrent: usize) {
    let semaphore = Arc::new(Semaphore::new(max_concurrent));

    let handles: Vec<_> = tasks
        .into_iter()
        .map(|task| {
            let sem = Arc::clone(&semaphore);
            tokio::spawn(async move {
                let _permit = sem.acquire().await.unwrap();
                // 执行任务
                tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                println!("完成任务: {}", task);
            })
        })
        .collect();

    for handle in handles {
        handle.await.unwrap();
    }
}
```

### 4.3 读写锁 (RwLock)

```rust
use std::sync::{Arc, RwLock};
use std::thread;

/// 并发读写锁：共享计数器
pub fn concurrent_counter() -> i32 {
    let counter = Arc::new(RwLock::new(0));

    let mut handles = vec![];

    // 10 个读线程
    for _ in 0..10 {
        let counter = Arc::clone(&counter);
        handles.push(thread::spawn(move || {
            let val = counter.read().unwrap();
            println!("读取: {}", *val);
        }));
    }

    // 2 个写线程
    for _ in 0..2 {
        let counter = Arc::clone(&counter);
        handles.push(thread::spawn(move || {
            let mut val = counter.write().unwrap();
            *val += 1;
            println!("写入: {}", *val);
        }));
    }

    for handle in handles {
        handle.join().unwrap();
    }

    *counter.read().unwrap()
}
```

---

## 5. 并发算法分析

### 5.1 加速比与效率

```rust
use std::time::Instant;

/// 测量加速比
pub fn measure_speedup<F1, F2>(sequential: F1, parallel: F2) -> f64
where
    F1: Fn(),
    F2: Fn(),
{
    let start = Instant::now();
    sequential();
    let t_seq = start.elapsed().as_secs_f64();

    let start = Instant::now();
    parallel();
    let t_par = start.elapsed().as_secs_f64();

    let speedup = t_seq / t_par;
    let num_threads = num_cpus::get();
    let efficiency = speedup / num_threads as f64;

    println!("顺序时间: {:.3}s", t_seq);
    println!("并行时间: {:.3}s", t_par);
    println!("加速比: {:.2}x", speedup);
    println!("效率: {:.2}%", efficiency * 100.0);

    speedup
}

/// Amdahl 定律
/// S(p) = 1 / (f + (1-f)/p)
/// 其中 f 是串行部分比例，p 是处理器数量
pub fn amdahl_law(serial_fraction: f64, num_processors: usize) -> f64 {
    1.0 / (serial_fraction + (1.0 - serial_fraction) / num_processors as f64)
}

/// Gustafson 定律
/// S(p) = p - α(p-1)
/// 其中 α 是串行部分比例，p 是处理器数量
pub fn gustafson_law(serial_fraction: f64, num_processors: usize) -> f64 {
    num_processors as f64 - serial_fraction * (num_processors as f64 - 1.0)
}
```

### 5.2 工作深度模型

```rust
/// 工作深度分析
pub struct WorkDepthAnalysis {
    work: usize,   // 总操作数
    depth: usize,  // 关键路径长度
}

impl WorkDepthAnalysis {
    /// 计算并行度
    pub fn parallelism(&self) -> f64 {
        self.work as f64 / self.depth as f64
    }

    /// 计算理论加速比（p 个处理器）
    pub fn speedup(&self, num_processors: usize) -> f64 {
        let t_sequential = self.work as f64;
        let t_parallel = (self.work as f64 / num_processors as f64).max(self.depth as f64);
        t_sequential / t_parallel
    }
}

/// 归并排序的工作深度分析
/// Work: W(n) = O(n log n)
/// Depth: D(n) = O(log² n)
/// Parallelism: P(n) = O(n log n / log² n) = O(n / log n)
pub fn merge_sort_work_depth(n: usize) -> WorkDepthAnalysis {
    let work = n * (n as f64).log2().ceil() as usize;
    let depth = ((n as f64).log2().ceil() as usize).pow(2);

    WorkDepthAnalysis { work, depth }
}
```

### 5.3 负载均衡

```rust
use rayon::prelude::*;

/// 动态负载均衡：工作窃取
pub fn work_stealing_example(tasks: Vec<usize>) -> Vec<usize> {
    tasks
        .par_iter()
        .map(|&task_size| {
            // 模拟不均匀的工作负载
            let work = (0..task_size).map(|i| i * i).sum();
            work
        })
        .collect()
}

/// 静态负载均衡：块分配
pub fn static_block_scheduling<T, F>(data: &[T], num_threads: usize, f: F) -> Vec<T>
where
    T: Clone + Send + Sync,
    F: Fn(&T) -> T + Send + Sync,
{
    let chunk_size = (data.len() + num_threads - 1) / num_threads;

    data.par_chunks(chunk_size)
        .flat_map(|chunk| chunk.iter().map(&f).collect::<Vec<_>>())
        .collect()
}

/// 动态负载均衡：任务队列
pub fn dynamic_task_queue(tasks: Vec<Box<dyn Fn() -> i32 + Send>>) -> Vec<i32> {
    use std::sync::{Arc, Mutex};
    use std::thread;

    let tasks = Arc::new(Mutex::new(tasks));
    let results = Arc::new(Mutex::new(Vec::new()));

    let handles: Vec<_> = (0..num_cpus::get())
        .map(|_| {
            let tasks = Arc::clone(&tasks);
            let results = Arc::clone(&results);

            thread::spawn(move || {
                loop {
                    let task = {
                        let mut task_queue = tasks.lock().unwrap();
                        task_queue.pop()
                    };

                    match task {
                        Some(task) => {
                            let result = task();
                            results.lock().unwrap().push(result);
                        }
                        None => break,
                    }
                }
            })
        })
        .collect();

    for handle in handles {
        handle.join().unwrap();
    }

    Arc::try_unwrap(results).unwrap().into_inner().unwrap()
}
```

---

## 6. 实践案例

### 6.1 并发归并排序

```rust
use rayon::prelude::*;

/// Rayon 并发归并排序
pub fn parallel_merge_sort<T: Ord + Send>(arr: &mut [T]) {
    if arr.len() <= 1 {
        return;
    }

    let mid = arr.len() / 2;
    let (left, right) = arr.split_at_mut(mid);

    rayon::join(
        || parallel_merge_sort(left),
        || parallel_merge_sort(right),
    );

    merge_parallel(arr, mid);
}

fn merge_parallel<T: Ord + Clone>(arr: &mut [T], mid: usize) {
    let left = arr[..mid].to_vec();
    let right = arr[mid..].to_vec();

    let mut i = 0;
    let mut j = 0;
    let mut k = 0;

    while i < left.len() && j < right.len() {
        if left[i] <= right[j] {
            arr[k] = left[i].clone();
            i += 1;
        } else {
            arr[k] = right[j].clone();
            j += 1;
        }
        k += 1;
    }

    while i < left.len() {
        arr[k] = left[i].clone();
        i += 1;
        k += 1;
    }

    while j < right.len() {
        arr[k] = right[j].clone();
        j += 1;
        k += 1;
    }
}
```

### 6.2 并发图算法

```rust
/// 并发 Bellman-Ford 算法
pub fn parallel_bellman_ford(
    graph: &[Vec<(usize, i64)>],
    start: usize,
) -> Vec<i64> {
    use std::sync::Arc;
    use parking_lot::RwLock;

    let n = graph.len();
    let dist = Arc::new(RwLock::new(vec![i64::MAX; n]));
    dist.write()[start] = 0;

    for _ in 0..n - 1 {
        let updated = Arc::new(std::sync::atomic::AtomicBool::new(false));

        (0..n).into_par_iter().for_each(|u| {
            let d_u = dist.read()[u];
            if d_u == i64::MAX {
                return;
            }

            for &(v, w) in &graph[u] {
                let new_dist = d_u + w;
                let mut dist_write = dist.write();
                if new_dist < dist_write[v] {
                    dist_write[v] = new_dist;
                    updated.store(true, std::sync::atomic::Ordering::Relaxed);
                }
            }
        });

        if !updated.load(std::sync::atomic::Ordering::Relaxed) {
            break;
        }
    }

    Arc::try_unwrap(dist).unwrap().into_inner()
}
```

### 6.3 并发哈希表

```rust
use dashmap::DashMap;

/// 并发哈希表示例
pub fn concurrent_word_count(texts: Vec<String>) -> DashMap<String, usize> {
    let word_count = DashMap::new();

    texts.par_iter().for_each(|text| {
        for word in text.split_whitespace() {
            *word_count.entry(word.to_string()).or_insert(0) += 1;
        }
    });

    word_count
}
```

---

## 7. 参考资料

### 教材

- **[Herlihy-Shavit]** Herlihy, Shavit. *The Art of Multiprocessor Programming*
- **[McCool et al.]** McCool, Reinders, Robison. *Structured Parallel Programming*
- **[Mattson et al.]** Mattson, Sanders, Massingill. *Patterns for Parallel Programming*

### Rust 并发库

- **[Rayon]** <https://github.com/rayon-rs/rayon>
- **[Crossbeam]** <https://github.com/crossbeam-rs/crossbeam>
- **[DashMap]** <https://github.com/xacrimon/dashmap>
- **[Tokio]** <https://tokio.rs/>

### 形式化验证

- **[Loom]** <https://github.com/tokio-rs/loom> (并发测试)
- **[Shuttle]** <https://github.com/awslabs/shuttle> (确定性并发测试)

---

**文档完成度**: 100%
**代码示例数**: 30+
**并发模式数**: 10+
**无锁数据结构数**: 2

**下一步**: 参见 [`03_分布式算法.md`](./03_分布式算法.md)
