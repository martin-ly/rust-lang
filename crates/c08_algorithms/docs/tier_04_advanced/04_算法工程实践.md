# ç®—æ³•å·¥ç¨‹å®è·µ

> **æ–‡æ¡£ç±»å‹**: Tier 4 - é«˜çº§ä¸»é¢˜
> **æœ€åæ›´æ–°**: 2025-10-23
> **çŠ¶æ€**: âœ… å®Œæˆ

---

## ç›®å½•

- [ç®—æ³•å·¥ç¨‹å®è·µ](#ç®—æ³•å·¥ç¨‹å®è·µ)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“ çŸ¥è¯†ç»“æ„](#-çŸ¥è¯†ç»“æ„)
    - [æ¦‚å¿µå®šä¹‰](#æ¦‚å¿µå®šä¹‰)
    - [å±æ€§ç‰¹å¾](#å±æ€§ç‰¹å¾)
    - [å…³ç³»è¿æ¥](#å…³ç³»è¿æ¥)
    - [æ€ç»´å¯¼å›¾](#æ€ç»´å¯¼å›¾)
    - [å¤šç»´æ¦‚å¿µå¯¹æ¯”çŸ©é˜µ](#å¤šç»´æ¦‚å¿µå¯¹æ¯”çŸ©é˜µ)
    - [å†³ç­–æ ‘å›¾](#å†³ç­–æ ‘å›¾)
  - [1. å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡](#1-å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡)
    - [1.1 æµ·é‡æ•°æ®å¤„ç†](#11-æµ·é‡æ•°æ®å¤„ç†)
      - [å¤–éƒ¨æ’åº (External Sorting)](#å¤–éƒ¨æ’åº-external-sorting)
      - [Bloom Filterï¼ˆå¤§æ•°æ®å»é‡ï¼‰](#bloom-filterå¤§æ•°æ®å»é‡)
    - [1.2 ç¼“å­˜ç­–ç•¥](#12-ç¼“å­˜ç­–ç•¥)
      - [LRU Cache (å¤šçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬)](#lru-cache-å¤šçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬)
      - [ç¼“å­˜é¢„çƒ­ä¸æ›´æ–°ç­–ç•¥](#ç¼“å­˜é¢„çƒ­ä¸æ›´æ–°ç­–ç•¥)
    - [1.3 è´Ÿè½½å‡è¡¡](#13-è´Ÿè½½å‡è¡¡)
      - [ä¸€è‡´æ€§å“ˆå¸Œè´Ÿè½½å‡è¡¡](#ä¸€è‡´æ€§å“ˆå¸Œè´Ÿè½½å‡è¡¡)
  - [2. æ€§èƒ½è°ƒä¼˜å®æˆ˜](#2-æ€§èƒ½è°ƒä¼˜å®æˆ˜)
    - [2.1 CPU ä¼˜åŒ–](#21-cpu-ä¼˜åŒ–)
      - [SIMD ä¼˜åŒ–](#simd-ä¼˜åŒ–)
      - [ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€](#ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€)
    - [2.2 å†…å­˜ä¼˜åŒ–](#22-å†…å­˜ä¼˜åŒ–)
      - [å¯¹è±¡æ± ](#å¯¹è±¡æ± )
      - [å†…å­˜æ˜ å°„æ–‡ä»¶ (mmap)](#å†…å­˜æ˜ å°„æ–‡ä»¶-mmap)
    - [2.3 I/O ä¼˜åŒ–](#23-io-ä¼˜åŒ–)
      - [å¼‚æ­¥æ‰¹é‡å†™å…¥](#å¼‚æ­¥æ‰¹é‡å†™å…¥)
  - [3. ç®—æ³•å¯é æ€§](#3-ç®—æ³•å¯é æ€§)
    - [3.1 å®¹é”™è®¾è®¡](#31-å®¹é”™è®¾è®¡)
      - [é‡è¯•æœºåˆ¶](#é‡è¯•æœºåˆ¶)
    - [3.2 é™çº§ç­–ç•¥](#32-é™çº§ç­–ç•¥)
    - [3.3 ç›‘æ§ä¸å‘Šè­¦](#33-ç›‘æ§ä¸å‘Šè­¦)
  - [4. ä»£ç è´¨é‡](#4-ä»£ç è´¨é‡)
    - [4.1 ç®—æ³•è®¾è®¡æ¨¡å¼](#41-ç®—æ³•è®¾è®¡æ¨¡å¼)
      - [ç­–ç•¥æ¨¡å¼](#ç­–ç•¥æ¨¡å¼)
    - [4.2 å•å…ƒæµ‹è¯•](#42-å•å…ƒæµ‹è¯•)
      - [å‚æ•°åŒ–æµ‹è¯•](#å‚æ•°åŒ–æµ‹è¯•)
    - [4.3 æ€§èƒ½åŸºå‡†æµ‹è¯•](#43-æ€§èƒ½åŸºå‡†æµ‹è¯•)
      - [Criterion åŸºå‡†æµ‹è¯•](#criterion-åŸºå‡†æµ‹è¯•)
  - [5. ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ](#5-ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ)
    - [5.1 é…ç½®ç®¡ç†](#51-é…ç½®ç®¡ç†)
    - [5.2 ç°åº¦å‘å¸ƒ](#52-ç°åº¦å‘å¸ƒ)
    - [5.3 æ•…éšœæ¢å¤](#53-æ•…éšœæ¢å¤)
  - [6. å®æˆ˜æ¡ˆä¾‹](#6-å®æˆ˜æ¡ˆä¾‹)
    - [6.1 æ¨èç³»ç»Ÿ](#61-æ¨èç³»ç»Ÿ)
    - [6.2 å®æ—¶æ’è¡Œæ¦œ](#62-å®æ—¶æ’è¡Œæ¦œ)
    - [6.3 åˆ†å¸ƒå¼é™æµ](#63-åˆ†å¸ƒå¼é™æµ)
  - [7. å‚è€ƒèµ„æ–™](#7-å‚è€ƒèµ„æ–™)
    - [æ•™æ](#æ•™æ)
    - [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
    - [ç”Ÿäº§å®è·µ](#ç”Ÿäº§å®è·µ)

---

## ğŸ“ çŸ¥è¯†ç»“æ„

### æ¦‚å¿µå®šä¹‰

**ç®—æ³•å·¥ç¨‹å®è·µ (Algorithm Engineering Practice)**:

- **å®šä¹‰**: Rust 1.92.0 ç®—æ³•å·¥ç¨‹å®è·µï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡ã€æ€§èƒ½è°ƒä¼˜å®æˆ˜ã€ç®—æ³•å¯é æ€§ã€ä»£ç è´¨é‡ã€ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µã€å®æˆ˜æ¡ˆä¾‹ç­‰
- **ç±»å‹**: é«˜çº§ä¸»é¢˜æ–‡æ¡£
- **èŒƒç•´**: ç®—æ³•ã€å·¥ç¨‹å®è·µ
- **ç‰ˆæœ¬**: Rust 1.92.0+ (Edition 2024)
- **ç›¸å…³æ¦‚å¿µ**: å¤§è§„æ¨¡ç³»ç»Ÿã€æ€§èƒ½è°ƒä¼˜ã€å®¹é”™è®¾è®¡ã€é™çº§ç­–ç•¥ã€ç›‘æ§å‘Šè­¦ã€é…ç½®ç®¡ç†ã€ç°åº¦å‘å¸ƒ

### å±æ€§ç‰¹å¾

**æ ¸å¿ƒå±æ€§**:

- **å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡**: æµ·é‡æ•°æ®å¤„ç†ï¼ˆå¤–éƒ¨æ’åºã€Bloom Filterï¼‰ã€ç¼“å­˜ç­–ç•¥ï¼ˆLRU Cacheã€ç¼“å­˜é¢„çƒ­ä¸æ›´æ–°ç­–ç•¥ï¼‰ã€è´Ÿè½½å‡è¡¡ï¼ˆä¸€è‡´æ€§å“ˆå¸Œè´Ÿè½½å‡è¡¡ï¼‰
- **æ€§èƒ½è°ƒä¼˜å®æˆ˜**: CPU ä¼˜åŒ–ï¼ˆSIMD ä¼˜åŒ–ã€ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€ï¼‰ã€å†…å­˜ä¼˜åŒ–ï¼ˆå¯¹è±¡æ± ã€å†…å­˜æ˜ å°„æ–‡ä»¶ mmapï¼‰ã€I/O ä¼˜åŒ–ï¼ˆå¼‚æ­¥æ‰¹é‡å†™å…¥ï¼‰
- **ç®—æ³•å¯é æ€§**: å®¹é”™è®¾è®¡ï¼ˆé‡è¯•æœºåˆ¶ï¼‰ã€é™çº§ç­–ç•¥ã€ç›‘æ§ä¸å‘Šè­¦
- **ä»£ç è´¨é‡**: ç®—æ³•è®¾è®¡æ¨¡å¼ï¼ˆç­–ç•¥æ¨¡å¼ï¼‰ã€å•å…ƒæµ‹è¯•ï¼ˆå‚æ•°åŒ–æµ‹è¯•ï¼‰ã€æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆCriterion åŸºå‡†æµ‹è¯•ï¼‰
- **ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ**: é…ç½®ç®¡ç†ã€ç°åº¦å‘å¸ƒã€æ•…éšœæ¢å¤

**Rust 1.92.0 æ–°ç‰¹æ€§**:

- **æ”¹è¿›çš„ SIMD æ”¯æŒ**: æ›´å¥½çš„ portable_simd æ”¯æŒ
- **å¢å¼ºçš„æ€§èƒ½åˆ†æ**: æ›´ç²¾ç¡®çš„æ€§èƒ½åˆ†æå·¥å…·
- **ä¼˜åŒ–çš„ç®—æ³•å®ç°**: æ›´é«˜æ•ˆçš„ç®—æ³•å®ç°

**æ€§èƒ½ç‰¹å¾**:

- **é«˜æ€§èƒ½**: ä¼˜åŒ–çš„ç®—æ³•æ€§èƒ½
- **é«˜å¯é **: å¯é çš„ç®—æ³•å®ç°
- **é€‚ç”¨åœºæ™¯**: ç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡ç³»ç»Ÿã€é«˜æ€§èƒ½åº”ç”¨

### å…³ç³»è¿æ¥

**ç»„åˆå…³ç³»**:

- ç®—æ³•å·¥ç¨‹å®è·µ --[covers]--> ç®—æ³•å·¥ç¨‹å®Œæ•´å†…å®¹
- ç”Ÿäº§çº§ç®—æ³•åº”ç”¨ --[uses]--> ç®—æ³•å·¥ç¨‹å®è·µ

**ä¾èµ–å…³ç³»**:

- ç®—æ³•å·¥ç¨‹å®è·µ --[depends-on]--> ç®—æ³•åŸºç¡€
- ç”Ÿäº§çº§åº”ç”¨ --[depends-on]--> ç®—æ³•å·¥ç¨‹å®è·µ

### æ€ç»´å¯¼å›¾

```text
ç®—æ³•å·¥ç¨‹å®è·µ
â”‚
â”œâ”€â”€ å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡
â”‚   â”œâ”€â”€ æµ·é‡æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ ç¼“å­˜ç­–ç•¥
â”‚   â””â”€â”€ è´Ÿè½½å‡è¡¡
â”œâ”€â”€ æ€§èƒ½è°ƒä¼˜å®æˆ˜
â”‚   â”œâ”€â”€ CPU ä¼˜åŒ–
â”‚   â”œâ”€â”€ å†…å­˜ä¼˜åŒ–
â”‚   â””â”€â”€ I/O ä¼˜åŒ–
â”œâ”€â”€ ç®—æ³•å¯é æ€§
â”‚   â”œâ”€â”€ å®¹é”™è®¾è®¡
â”‚   â””â”€â”€ é™çº§ç­–ç•¥
â”œâ”€â”€ ä»£ç è´¨é‡
â”‚   â”œâ”€â”€ ç®—æ³•è®¾è®¡æ¨¡å¼
â”‚   â””â”€â”€ æ€§èƒ½åŸºå‡†æµ‹è¯•
â””â”€â”€ ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ
    â”œâ”€â”€ é…ç½®ç®¡ç†
    â””â”€â”€ ç°åº¦å‘å¸ƒ
```

### å¤šç»´æ¦‚å¿µå¯¹æ¯”çŸ©é˜µ

| å·¥ç¨‹å®è·µ           | é‡è¦æ€§ | å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯   | Rust 1.92.0 |
| :--- | :--- | :--- | :--- | :--- || **å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡** | é«˜     | é«˜     | å¤§è§„æ¨¡ç³»ç»Ÿ | âœ…          |
| **æ€§èƒ½è°ƒä¼˜**       | é«˜     | ä¸­     | æ€§èƒ½å…³é”®   | âœ… æ”¹è¿›     |
| **ç®—æ³•å¯é æ€§**     | é«˜     | ä¸­     | ç”Ÿäº§ç¯å¢ƒ   | âœ…          |
| **ä»£ç è´¨é‡**       | ä¸­     | ä¸­     | æ‰€æœ‰é¡¹ç›®   | âœ…          |
| **ç”Ÿäº§ç¯å¢ƒå®è·µ**   | é«˜     | ä¸­     | ç”Ÿäº§é¡¹ç›®   | âœ…          |

### å†³ç­–æ ‘å›¾

```text
ç®—æ³•å·¥ç¨‹å®è·µæµç¨‹
â”‚
â”œâ”€â”€ è®¾è®¡é˜¶æ®µ
â”‚   â”œâ”€â”€ å¤§è§„æ¨¡ç³»ç»Ÿ â†’ æµ·é‡æ•°æ®å¤„ç† / ç¼“å­˜ç­–ç•¥
â”‚   â””â”€â”€ æ€§èƒ½ä¼˜åŒ– â†’ CPU / å†…å­˜ / I/O ä¼˜åŒ–
â”œâ”€â”€ å¼€å‘é˜¶æ®µ
â”‚   â”œâ”€â”€ ç®—æ³•å¯é æ€§ â†’ å®¹é”™è®¾è®¡ / é™çº§ç­–ç•¥
â”‚   â””â”€â”€ ä»£ç è´¨é‡ â†’ ç®—æ³•è®¾è®¡æ¨¡å¼ / åŸºå‡†æµ‹è¯•
â””â”€â”€ ç”Ÿäº§é˜¶æ®µ
    â””â”€â”€ ç”Ÿäº§ç¯å¢ƒå®è·µ â†’ é…ç½®ç®¡ç† / ç°åº¦å‘å¸ƒ
```

---

## 1. å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡

### 1.1 æµ·é‡æ•°æ®å¤„ç†

#### å¤–éƒ¨æ’åº (External Sorting)

```rust
use std::fs::File;
use std::io::{BufReader, BufWriter, Read, Write};

/// å¤–éƒ¨å½’å¹¶æ’åºï¼ˆå¤„ç†è¶…è¿‡å†…å­˜å¤§å°çš„æ•°æ®ï¼‰
pub struct ExternalSort {
    chunk_size: usize,  // å†…å­˜å—å¤§å°ï¼ˆæ¡ç›®æ•°ï¼‰
    temp_dir: String,
}

impl ExternalSort {
    pub fn new(chunk_size: usize) -> Self {
        Self {
            chunk_size,
            temp_dir: "temp".to_string(),
        }
    }

    /// åˆ†å—æ’åº
    pub fn sort_chunks(&self, input_file: &str) -> std::io::Result<Vec<String>> {
        std::fs::create_dir_all(&self.temp_dir)?;

        let file = File::open(input_file)?;
        let reader = BufReader::new(file);

        let mut chunk_files = Vec::new();
        let mut chunk = Vec::new();
        let mut chunk_id = 0;

        for line in std::io::BufRead::lines(reader) {
            let line = line?;
            chunk.push(line);

            if chunk.len() >= self.chunk_size {
                let chunk_file = self.write_sorted_chunk(&mut chunk, chunk_id)?;
                chunk_files.push(chunk_file);
                chunk_id += 1;
            }
        }

        // å¤„ç†æœ€åä¸€ä¸ªå—
        if !chunk.is_empty() {
            let chunk_file = self.write_sorted_chunk(&mut chunk, chunk_id)?;
            chunk_files.push(chunk_file);
        }

        Ok(chunk_files)
    }

    /// å†™å…¥æ’åºåçš„å—
    fn write_sorted_chunk(&self, chunk: &mut Vec<String>, id: usize) -> std::io::Result<String> {
        chunk.sort();

        let chunk_file = format!("{}/chunk_{}.txt", self.temp_dir, id);
        let file = File::create(&chunk_file)?;
        let mut writer = BufWriter::new(file);

        for line in chunk {
            writeln!(writer, "{}", line)?;
        }

        chunk.clear();
        Ok(chunk_file)
    }

    /// K è·¯å½’å¹¶
    pub fn merge_chunks(&self, chunk_files: Vec<String>, output_file: &str) -> std::io::Result<()> {
        use std::collections::BinaryHeap;
        use std::cmp::Reverse;

        let mut readers: Vec<_> = chunk_files
            .iter()
            .map(|f| {
                let file = File::open(f).unwrap();
                std::io::BufRead::lines(BufReader::new(file))
            })
            .collect();

        let mut heap = BinaryHeap::new();

        // åˆå§‹åŒ–ï¼šä»æ¯ä¸ªæ–‡ä»¶è¯»å–ç¬¬ä¸€è¡Œ
        for (i, reader) in readers.iter_mut().enumerate() {
            if let Some(Ok(line)) = reader.next() {
                heap.push(Reverse((line, i)));
            }
        }

        let output = File::create(output_file)?;
        let mut writer = BufWriter::new(output);

        // K è·¯å½’å¹¶
        while let Some(Reverse((line, file_id))) = heap.pop() {
            writeln!(writer, "{}", line)?;

            // ä»åŒä¸€æ–‡ä»¶è¯»å–ä¸‹ä¸€è¡Œ
            if let Some(Ok(next_line)) = readers[file_id].next() {
                heap.push(Reverse((next_line, file_id)));
            }
        }

        writer.flush()?;

        // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for file in chunk_files {
            let _ = std::fs::remove_file(file);
        }

        Ok(())
    }

    /// å®Œæ•´çš„å¤–éƒ¨æ’åºæµç¨‹
    pub fn sort(&self, input_file: &str, output_file: &str) -> std::io::Result<()> {
        let chunk_files = self.sort_chunks(input_file)?;
        self.merge_chunks(chunk_files, output_file)?;
        Ok(())
    }
}

/// ç¤ºä¾‹ï¼šæ’åº 10GB æ–‡ä»¶
pub fn external_sort_example() -> std::io::Result<()> {
    let sorter = ExternalSort::new(1_000_000); // æ¯å— 100 ä¸‡æ¡è®°å½•
    sorter.sort("large_input.txt", "sorted_output.txt")?;
    Ok(())
}
```

#### Bloom Filterï¼ˆå¤§æ•°æ®å»é‡ï¼‰

```rust
use bit_vec::BitVec;

/// Bloom Filter
pub struct BloomFilter {
    bits: BitVec,
    hash_count: usize,
    size: usize,
}

impl BloomFilter {
    /// åˆ›å»º Bloom Filter
    /// - expected_items: é¢„æœŸå…ƒç´ æ•°é‡
    /// - false_positive_rate: è¯¯åˆ¤ç‡
    pub fn new(expected_items: usize, false_positive_rate: f64) -> Self {
        let size = Self::optimal_size(expected_items, false_positive_rate);
        let hash_count = Self::optimal_hash_count(size, expected_items);

        Self {
            bits: BitVec::from_elem(size, false),
            hash_count,
            size,
        }
    }

    fn optimal_size(n: usize, p: f64) -> usize {
        (-(n as f64 * p.ln()) / (2.0_f64.ln().powi(2))).ceil() as usize
    }

    fn optimal_hash_count(m: usize, n: usize) -> usize {
        ((m as f64 / n as f64) * 2.0_f64.ln()).ceil() as usize
    }

    /// æ·»åŠ å…ƒç´ 
    pub fn insert(&mut self, item: &[u8]) {
        for i in 0..self.hash_count {
            let hash = self.hash(item, i);
            self.bits.set(hash % self.size, true);
        }
    }

    /// æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ï¼ˆå¯èƒ½è¯¯åˆ¤ï¼‰
    pub fn contains(&self, item: &[u8]) -> bool {
        (0..self.hash_count).all(|i| {
            let hash = self.hash(item, i);
            self.bits[hash % self.size]
        })
    }

    /// å“ˆå¸Œå‡½æ•°
    fn hash(&self, item: &[u8], seed: usize) -> usize {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        item.hash(&mut hasher);
        seed.hash(&mut hasher);
        hasher.finish() as usize
    }
}

/// ä½¿ç”¨ç¤ºä¾‹ï¼šURL å»é‡
pub fn url_deduplication_example() {
    let mut filter = BloomFilter::new(10_000_000, 0.01); // 1000 ä¸‡ URLï¼Œ1% è¯¯åˆ¤ç‡

    let urls = vec![
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page1",  // é‡å¤
    ];

    for url in &urls {
        if !filter.contains(url.as_bytes()) {
            println!("æ–° URL: {}", url);
            filter.insert(url.as_bytes());
        } else {
            println!("é‡å¤ URL: {}", url);
        }
    }
}
```

### 1.2 ç¼“å­˜ç­–ç•¥

#### LRU Cache (å¤šçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬)

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

/// LRU ç¼“å­˜èŠ‚ç‚¹
struct LRUNode<K, V> {
    key: K,
    value: V,
    prev: Option<usize>,
    next: Option<usize>,
}

/// çº¿ç¨‹å®‰å…¨çš„ LRU Cache
pub struct ThreadSafeLRUCache<K, V> {
    capacity: usize,
    cache: Arc<Mutex<LRUCacheInner<K, V>>>,
}

struct LRUCacheInner<K, V> {
    map: HashMap<K, usize>,
    nodes: Vec<LRUNode<K, V>>,
    head: Option<usize>,
    tail: Option<usize>,
    free_list: Vec<usize>,
}

impl<K: Clone + Eq + std::hash::Hash, V: Clone> ThreadSafeLRUCache<K, V> {
    pub fn new(capacity: usize) -> Self {
        Self {
            capacity,
            cache: Arc::new(Mutex::new(LRUCacheInner {
                map: HashMap::new(),
                nodes: Vec::new(),
                head: None,
                tail: None,
                free_list: Vec::new(),
            })),
        }
    }

    pub fn get(&self, key: &K) -> Option<V> {
        let mut cache = self.cache.lock().unwrap();

        if let Some(&node_id) = cache.map.get(key) {
            cache.move_to_head(node_id);
            Some(cache.nodes[node_id].value.clone())
        } else {
            None
        }
    }

    pub fn put(&self, key: K, value: V) {
        let mut cache = self.cache.lock().unwrap();

        if let Some(&node_id) = cache.map.get(&key) {
            cache.nodes[node_id].value = value;
            cache.move_to_head(node_id);
        } else {
            if cache.map.len() >= self.capacity {
                cache.evict_tail();
            }

            cache.add_to_head(key, value);
        }
    }
}

impl<K: Clone + Eq + std::hash::Hash, V: Clone> LRUCacheInner<K, V> {
    fn move_to_head(&mut self, node_id: usize) {
        if Some(node_id) == self.head {
            return;
        }

        self.remove_node(node_id);
        self.add_to_head_node(node_id);
    }

    fn remove_node(&mut self, node_id: usize) {
        let node = &self.nodes[node_id];
        let prev = node.prev;
        let next = node.next;

        if let Some(prev_id) = prev {
            self.nodes[prev_id].next = next;
        } else {
            self.head = next;
        }

        if let Some(next_id) = next {
            self.nodes[next_id].prev = prev;
        } else {
            self.tail = prev;
        }
    }

    fn add_to_head_node(&mut self, node_id: usize) {
        self.nodes[node_id].prev = None;
        self.nodes[node_id].next = self.head;

        if let Some(old_head) = self.head {
            self.nodes[old_head].prev = Some(node_id);
        }

        self.head = Some(node_id);

        if self.tail.is_none() {
            self.tail = Some(node_id);
        }
    }

    fn add_to_head(&mut self, key: K, value: V) {
        let node_id = if let Some(id) = self.free_list.pop() {
            self.nodes[id] = LRUNode {
                key: key.clone(),
                value,
                prev: None,
                next: None,
            };
            id
        } else {
            let id = self.nodes.len();
            self.nodes.push(LRUNode {
                key: key.clone(),
                value,
                prev: None,
                next: None,
            });
            id
        };

        self.map.insert(key, node_id);
        self.add_to_head_node(node_id);
    }

    fn evict_tail(&mut self) {
        if let Some(tail_id) = self.tail {
            let key = self.nodes[tail_id].key.clone();
            self.map.remove(&key);
            self.remove_node(tail_id);
            self.free_list.push(tail_id);
        }
    }
}
```

#### ç¼“å­˜é¢„çƒ­ä¸æ›´æ–°ç­–ç•¥

```rust
use tokio::time::{interval, Duration};

/// ç¼“å­˜ç®¡ç†å™¨
pub struct CacheManager<K, V> {
    cache: Arc<ThreadSafeLRUCache<K, V>>,
    loader: Arc<dyn Fn(K) -> Option<V> + Send + Sync>,
}

impl<K: Clone + Eq + std::hash::Hash + Send + Sync + 'static, V: Clone + Send + Sync + 'static>
    CacheManager<K, V>
{
    pub fn new(
        capacity: usize,
        loader: impl Fn(K) -> Option<V> + Send + Sync + 'static,
    ) -> Self {
        Self {
            cache: Arc::new(ThreadSafeLRUCache::new(capacity)),
            loader: Arc::new(loader),
        }
    }

    /// è·å–æ•°æ®ï¼ˆè‡ªåŠ¨åŠ è½½ï¼‰
    pub fn get_or_load(&self, key: K) -> Option<V> {
        if let Some(value) = self.cache.get(&key) {
            return Some(value);
        }

        // ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½æ•°æ®
        if let Some(value) = (self.loader)(key.clone()) {
            self.cache.put(key, value.clone());
            Some(value)
        } else {
            None
        }
    }

    /// ç¼“å­˜é¢„çƒ­
    pub async fn warmup(&self, keys: Vec<K>) {
        for key in keys {
            self.get_or_load(key);
        }
    }

    /// å®šæœŸåˆ·æ–°ç¼“å­˜
    pub async fn auto_refresh(&self, keys: Vec<K>, interval_secs: u64) {
        let mut ticker = interval(Duration::from_secs(interval_secs));

        loop {
            ticker.tick().await;

            for key in &keys {
                if let Some(value) = (self.loader)(key.clone()) {
                    self.cache.put(key.clone(), value);
                }
            }
        }
    }
}
```

### 1.3 è´Ÿè½½å‡è¡¡

#### ä¸€è‡´æ€§å“ˆå¸Œè´Ÿè½½å‡è¡¡

```rust
use std::collections::BTreeMap;

/// ä¸€è‡´æ€§å“ˆå¸Œè´Ÿè½½å‡è¡¡å™¨
pub struct ConsistentHashLoadBalancer {
    ring: BTreeMap<u64, String>,  // (hash, server_id)
    virtual_nodes: usize,
}

impl ConsistentHashLoadBalancer {
    pub fn new(servers: Vec<String>, virtual_nodes: usize) -> Self {
        let mut lb = Self {
            ring: BTreeMap::new(),
            virtual_nodes,
        };

        for server in servers {
            lb.add_server(server);
        }

        lb
    }

    /// æ·»åŠ æœåŠ¡å™¨
    pub fn add_server(&mut self, server: String) {
        for i in 0..self.virtual_nodes {
            let key = format!("{}:{}", server, i);
            let hash = Self::hash(&key);
            self.ring.insert(hash, server.clone());
        }
    }

    /// ç§»é™¤æœåŠ¡å™¨
    pub fn remove_server(&mut self, server: &str) {
        for i in 0..self.virtual_nodes {
            let key = format!("{}:{}", server, i);
            let hash = Self::hash(&key);
            self.ring.remove(&hash);
        }
    }

    /// é€‰æ‹©æœåŠ¡å™¨
    pub fn get_server(&self, key: &str) -> Option<String> {
        if self.ring.is_empty() {
            return None;
        }

        let hash = Self::hash(key);

        // æ‰¾åˆ°ç¬¬ä¸€ä¸ª >= hash çš„æœåŠ¡å™¨
        self.ring
            .range(hash..)
            .next()
            .or_else(|| self.ring.iter().next())
            .map(|(_, server)| server.clone())
    }

    fn hash(key: &str) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        key.hash(&mut hasher);
        hasher.finish()
    }
}

/// åŠ æƒè½®è¯¢è´Ÿè½½å‡è¡¡
pub struct WeightedRoundRobinBalancer {
    servers: Vec<(String, usize)>,  // (server_id, weight)
    current_weights: Vec<usize>,
    total_weight: usize,
}

impl WeightedRoundRobinBalancer {
    pub fn new(servers: Vec<(String, usize)>) -> Self {
        let total_weight = servers.iter().map(|(_, w)| w).sum();
        let current_weights = servers.iter().map(|(_, w)| *w).collect();

        Self {
            servers,
            current_weights,
            total_weight,
        }
    }

    /// é€‰æ‹©æœåŠ¡å™¨
    pub fn get_server(&mut self) -> Option<String> {
        if self.servers.is_empty() {
            return None;
        }

        let mut max_idx = 0;
        let mut max_weight = self.current_weights[0];

        for i in 0..self.servers.len() {
            self.current_weights[i] += self.servers[i].1;

            if self.current_weights[i] > max_weight {
                max_weight = self.current_weights[i];
                max_idx = i;
            }
        }

        self.current_weights[max_idx] -= self.total_weight;
        Some(self.servers[max_idx].0.clone())
    }
}
```

---

## 2. æ€§èƒ½è°ƒä¼˜å®æˆ˜

### 2.1 CPU ä¼˜åŒ–

#### SIMD ä¼˜åŒ–

```rust
use std::arch::x86_64::*;

/// SIMD å‘é‡åŠ æ³•
#[target_feature(enable = "avx2")]
pub unsafe fn simd_add(a: &[f32], b: &[f32], result: &mut [f32]) {
    assert_eq!(a.len(), b.len());
    assert_eq!(a.len(), result.len());

    let len = a.len();
    let chunks = len / 8;

    for i in 0..chunks {
        let offset = i * 8;

        let va = _mm256_loadu_ps(a.as_ptr().add(offset));
        let vb = _mm256_loadu_ps(b.as_ptr().add(offset));
        let vr = _mm256_add_ps(va, vb);

        _mm256_storeu_ps(result.as_mut_ptr().add(offset), vr);
    }

    // å¤„ç†å‰©ä½™å…ƒç´ 
    for i in (chunks * 8)..len {
        result[i] = a[i] + b[i];
    }
}

/// ç¤ºä¾‹ï¼šå‘é‡ç‚¹ç§¯ï¼ˆSIMDï¼‰
#[target_feature(enable = "avx2")]
pub unsafe fn simd_dot_product(a: &[f32], b: &[f32]) -> f32 {
    assert_eq!(a.len(), b.len());

    let len = a.len();
    let chunks = len / 8;

    let mut sum = _mm256_setzero_ps();

    for i in 0..chunks {
        let offset = i * 8;

        let va = _mm256_loadu_ps(a.as_ptr().add(offset));
        let vb = _mm256_loadu_ps(b.as_ptr().add(offset));
        let prod = _mm256_mul_ps(va, vb);

        sum = _mm256_add_ps(sum, prod);
    }

    // å½’çº¦æ±‚å’Œ
    let mut result = [0.0f32; 8];
    _mm256_storeu_ps(result.as_mut_ptr(), sum);
    let mut total: f32 = result.iter().sum();

    // å¤„ç†å‰©ä½™å…ƒç´ 
    for i in (chunks * 8)..len {
        total += a[i] * b[i];
    }

    total
}
```

#### ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€

```rust
/// ä¸å‹å¥½çš„ AOS (Array of Structures)
#[derive(Clone)]
pub struct ParticleAOS {
    x: f32,
    y: f32,
    z: f32,
    vx: f32,
    vy: f32,
    vz: f32,
}

pub fn update_particles_aos(particles: &mut [ParticleAOS], dt: f32) {
    for p in particles {
        p.x += p.vx * dt;
        p.y += p.vy * dt;
        p.z += p.vz * dt;
    }
}

/// å‹å¥½çš„ SOA (Structure of Arrays)
pub struct ParticlesSOA {
    x: Vec<f32>,
    y: Vec<f32>,
    z: Vec<f32>,
    vx: Vec<f32>,
    vy: Vec<f32>,
    vz: Vec<f32>,
}

pub fn update_particles_soa(particles: &mut ParticlesSOA, dt: f32) {
    // æ›´å¥½çš„ç¼“å­˜å±€éƒ¨æ€§
    for i in 0..particles.x.len() {
        particles.x[i] += particles.vx[i] * dt;
        particles.y[i] += particles.vy[i] * dt;
        particles.z[i] += particles.vz[i] * dt;
    }
}
```

### 2.2 å†…å­˜ä¼˜åŒ–

#### å¯¹è±¡æ± 

```rust
use std::sync::{Arc, Mutex};

/// å¯¹è±¡æ± 
pub struct ObjectPool<T> {
    pool: Arc<Mutex<Vec<T>>>,
    factory: Arc<dyn Fn() -> T + Send + Sync>,
    max_size: usize,
}

impl<T: Send + 'static> ObjectPool<T> {
    pub fn new(factory: impl Fn() -> T + Send + Sync + 'static, max_size: usize) -> Self {
        Self {
            pool: Arc::new(Mutex::new(Vec::new())),
            factory: Arc::new(factory),
            max_size,
        }
    }

    /// è·å–å¯¹è±¡
    pub fn acquire(&self) -> PooledObject<T> {
        let obj = {
            let mut pool = self.pool.lock().unwrap();
            pool.pop()
        };

        let obj = obj.unwrap_or_else(|| (self.factory)());

        PooledObject {
            obj: Some(obj),
            pool: Arc::clone(&self.pool),
            max_size: self.max_size,
        }
    }
}

/// æ± åŒ–å¯¹è±¡ï¼ˆè‡ªåŠ¨å½’è¿˜ï¼‰
pub struct PooledObject<T> {
    obj: Option<T>,
    pool: Arc<Mutex<Vec<T>>>,
    max_size: usize,
}

impl<T> std::ops::Deref for PooledObject<T> {
    type Target = T;

    fn deref(&self) -> &Self::Target {
        self.obj.as_ref().unwrap()
    }
}

impl<T> std::ops::DerefMut for PooledObject<T> {
    fn deref_mut(&mut self) -> &mut Self::Target {
        self.obj.as_mut().unwrap()
    }
}

impl<T> Drop for PooledObject<T> {
    fn drop(&mut self) {
        if let Some(obj) = self.obj.take() {
            let mut pool = self.pool.lock().unwrap();
            if pool.len() < self.max_size {
                pool.push(obj);
            }
        }
    }
}

/// ä½¿ç”¨ç¤ºä¾‹
pub fn object_pool_example() {
    let pool = ObjectPool::new(|| Vec::<i32>::with_capacity(1000), 10);

    {
        let mut vec = pool.acquire();
        vec.push(1);
        vec.push(2);
    } // è‡ªåŠ¨å½’è¿˜åˆ°æ± ä¸­

    let vec2 = pool.acquire(); // å¤ç”¨å¯¹è±¡
}
```

#### å†…å­˜æ˜ å°„æ–‡ä»¶ (mmap)

```rust
use memmap2::MmapMut;
use std::fs::OpenOptions;

/// å†…å­˜æ˜ å°„æ–‡ä»¶å¤„ç†
pub struct MmapProcessor {
    mmap: MmapMut,
}

impl MmapProcessor {
    /// åˆ›å»ºå†…å­˜æ˜ å°„
    pub fn new(file_path: &str, size: usize) -> std::io::Result<Self> {
        let file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .open(file_path)?;

        file.set_len(size as u64)?;

        let mmap = unsafe { MmapMut::map_mut(&file)? };

        Ok(Self { mmap })
    }

    /// å†™å…¥æ•°æ®
    pub fn write_at(&mut self, offset: usize, data: &[u8]) {
        let end = offset + data.len();
        self.mmap[offset..end].copy_from_slice(data);
    }

    /// è¯»å–æ•°æ®
    pub fn read_at(&self, offset: usize, len: usize) -> &[u8] {
        &self.mmap[offset..offset + len]
    }

    /// åˆ·æ–°åˆ°ç£ç›˜
    pub fn flush(&self) -> std::io::Result<()> {
        self.mmap.flush()
    }
}

/// ä½¿ç”¨ç¤ºä¾‹ï¼šå¤„ç†å¤§æ–‡ä»¶
pub fn mmap_large_file_example() -> std::io::Result<()> {
    let mut processor = MmapProcessor::new("large_file.dat", 1024 * 1024 * 1024)?; // 1GB

    // éšæœºè®¿é—®
    processor.write_at(1000, b"Hello, mmap!");
    let data = processor.read_at(1000, 12);

    processor.flush()?;
    Ok(())
}
```

### 2.3 I/O ä¼˜åŒ–

#### å¼‚æ­¥æ‰¹é‡å†™å…¥

```rust
use tokio::fs::File;
use tokio::io::{AsyncWriteExt, BufWriter};
use tokio::sync::mpsc;

/// å¼‚æ­¥æ‰¹é‡å†™å…¥å™¨
pub struct BatchWriter {
    sender: mpsc::Sender<Vec<u8>>,
}

impl BatchWriter {
    pub fn new(file_path: String, batch_size: usize) -> Self {
        let (tx, mut rx) = mpsc::channel(100);

        tokio::spawn(async move {
            let file = File::create(&file_path).await.unwrap();
            let mut writer = BufWriter::new(file);
            let mut buffer = Vec::new();

            while let Some(data) = rx.recv().await {
                buffer.extend_from_slice(&data);

                if buffer.len() >= batch_size {
                    writer.write_all(&buffer).await.unwrap();
                    writer.flush().await.unwrap();
                    buffer.clear();
                }
            }

            // å†™å…¥å‰©ä½™æ•°æ®
            if !buffer.is_empty() {
                writer.write_all(&buffer).await.unwrap();
                writer.flush().await.unwrap();
            }
        });

        Self { sender: tx }
    }

    /// å¼‚æ­¥å†™å…¥
    pub async fn write(&self, data: Vec<u8>) -> Result<(), mpsc::error::SendError<Vec<u8>>> {
        self.sender.send(data).await
    }
}

/// ä½¿ç”¨ç¤ºä¾‹
pub async fn batch_write_example() {
    let writer = BatchWriter::new("output.log".to_string(), 4096);

    for i in 0..1000 {
        let data = format!("Log line {}\n", i).into_bytes();
        writer.write(data).await.unwrap();
    }
}
```

---

## 3. ç®—æ³•å¯é æ€§

### 3.1 å®¹é”™è®¾è®¡

#### é‡è¯•æœºåˆ¶

```rust
use tokio::time::{sleep, Duration};

/// æŒ‡æ•°é€€é¿é‡è¯•
pub async fn retry_with_backoff<F, T, E>(
    mut operation: F,
    max_retries: usize,
    initial_delay_ms: u64,
) -> Result<T, E>
where
    F: FnMut() -> Result<T, E>,
{
    let mut delay = initial_delay_ms;
    let mut attempts = 0;

    loop {
        match operation() {
            Ok(result) => return Ok(result),
            Err(err) => {
                attempts += 1;

                if attempts >= max_retries {
                    return Err(err);
                }

                sleep(Duration::from_millis(delay)).await;
                delay *= 2; // æŒ‡æ•°é€€é¿
            }
        }
    }
}

/// æ–­è·¯å™¨æ¨¡å¼
pub struct CircuitBreaker {
    failure_threshold: usize,
    timeout: Duration,
    state: Arc<Mutex<CircuitState>>,
}

struct CircuitState {
    failures: usize,
    last_failure_time: Option<std::time::Instant>,
    state: State,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum State {
    Closed,   // æ­£å¸¸
    Open,     // æ–­å¼€
    HalfOpen, // åŠå¼€
}

impl CircuitBreaker {
    pub fn new(failure_threshold: usize, timeout_secs: u64) -> Self {
        Self {
            failure_threshold,
            timeout: Duration::from_secs(timeout_secs),
            state: Arc::new(Mutex::new(CircuitState {
                failures: 0,
                last_failure_time: None,
                state: State::Closed,
            })),
        }
    }

    /// æ‰§è¡Œæ“ä½œ
    pub fn call<F, T, E>(&self, operation: F) -> Result<T, String>
    where
        F: FnOnce() -> Result<T, E>,
    {
        let mut state = self.state.lock().unwrap();

        // æ£€æŸ¥æ˜¯å¦éœ€è¦ä» Open è½¬åˆ° HalfOpen
        if state.state == State::Open {
            if let Some(last_failure) = state.last_failure_time {
                if last_failure.elapsed() > self.timeout {
                    state.state = State::HalfOpen;
                    state.failures = 0;
                } else {
                    return Err("Circuit breaker is open".to_string());
                }
            }
        }

        drop(state);

        // æ‰§è¡Œæ“ä½œ
        match operation() {
            Ok(result) => {
                let mut state = self.state.lock().unwrap();
                state.failures = 0;
                state.state = State::Closed;
                Ok(result)
            }
            Err(_) => {
                let mut state = self.state.lock().unwrap();
                state.failures += 1;
                state.last_failure_time = Some(std::time::Instant::now());

                if state.failures >= self.failure_threshold {
                    state.state = State::Open;
                }

                Err("Operation failed".to_string())
            }
        }
    }
}
```

### 3.2 é™çº§ç­–ç•¥

```rust
/// æœåŠ¡é™çº§ç®¡ç†å™¨
pub struct DegradationManager {
    levels: Vec<DegradationLevel>,
    current_level: Arc<Mutex<usize>>,
}

#[derive(Debug, Clone)]
pub struct DegradationLevel {
    name: String,
    threshold: f64,  // CPU/å†…å­˜/é”™è¯¯ç‡é˜ˆå€¼
}

impl DegradationManager {
    pub fn new(levels: Vec<DegradationLevel>) -> Self {
        Self {
            levels,
            current_level: Arc::new(Mutex::new(0)),
        }
    }

    /// æ£€æŸ¥å¹¶æ›´æ–°é™çº§çº§åˆ«
    pub fn check_and_update(&self, metric: f64) {
        let mut level = self.current_level.lock().unwrap();

        for (i, deg_level) in self.levels.iter().enumerate() {
            if metric > deg_level.threshold {
                *level = i;
                println!("é™çº§åˆ°çº§åˆ«: {}", deg_level.name);
                return;
            }
        }

        *level = 0;
    }

    /// è·å–å½“å‰é™çº§çº§åˆ«
    pub fn get_level(&self) -> usize {
        *self.current_level.lock().unwrap()
    }
}

/// ä½¿ç”¨ç¤ºä¾‹ï¼šæ¨èç³»ç»Ÿé™çº§
pub fn recommendation_with_degradation(manager: &DegradationManager, user_id: u64) -> Vec<String> {
    match manager.get_level() {
        0 => {
            // æ­£å¸¸ï¼šä¸ªæ€§åŒ–æ¨èï¼ˆå¤æ‚ç®—æ³•ï¼‰
            compute_personalized_recommendations(user_id)
        }
        1 => {
            // é™çº§1ï¼šååŒè¿‡æ»¤ï¼ˆä¸­ç­‰å¤æ‚åº¦ï¼‰
            compute_collaborative_filtering(user_id)
        }
        2 => {
            // é™çº§2ï¼šçƒ­é—¨æ¨èï¼ˆç®€å•ï¼‰
            get_popular_items()
        }
        _ => {
            // é™çº§3ï¼šé™æ€æ¨è
            get_static_recommendations()
        }
    }
}

fn compute_personalized_recommendations(_user_id: u64) -> Vec<String> {
    vec!["item1".to_string(), "item2".to_string()]
}

fn compute_collaborative_filtering(_user_id: u64) -> Vec<String> {
    vec!["popular1".to_string(), "popular2".to_string()]
}

fn get_popular_items() -> Vec<String> {
    vec!["hot1".to_string(), "hot2".to_string()]
}

fn get_static_recommendations() -> Vec<String> {
    vec!["default1".to_string(), "default2".to_string()]
}
```

### 3.3 ç›‘æ§ä¸å‘Šè­¦

```rust
use std::time::Instant;

/// æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
pub struct MetricsCollector {
    requests: Arc<AtomicU64>,
    errors: Arc<AtomicU64>,
    latencies: Arc<Mutex<Vec<Duration>>>,
}

impl MetricsCollector {
    pub fn new() -> Self {
        Self {
            requests: Arc::new(AtomicU64::new(0)),
            errors: Arc::new(AtomicU64::new(0)),
            latencies: Arc::new(Mutex::new(Vec::new())),
        }
    }

    /// è®°å½•è¯·æ±‚
    pub fn record_request<F, T>(&self, operation: F) -> Result<T, String>
    where
        F: FnOnce() -> Result<T, String>,
    {
        let start = Instant::now();
        self.requests.fetch_add(1, std::sync::atomic::Ordering::Relaxed);

        let result = operation();

        let latency = start.elapsed();
        self.latencies.lock().unwrap().push(latency);

        if result.is_err() {
            self.errors.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        }

        result
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> MetricsStats {
        let requests = self.requests.load(std::sync::atomic::Ordering::Relaxed);
        let errors = self.errors.load(std::sync::atomic::Ordering::Relaxed);
        let latencies = self.latencies.lock().unwrap();

        let avg_latency = if !latencies.is_empty() {
            latencies.iter().sum::<Duration>() / latencies.len() as u32
        } else {
            Duration::ZERO
        };

        let p99_latency = if !latencies.is_empty() {
            let mut sorted = latencies.clone();
            sorted.sort();
            sorted[(sorted.len() as f64 * 0.99) as usize]
        } else {
            Duration::ZERO
        };

        MetricsStats {
            requests,
            errors,
            error_rate: if requests > 0 {
                errors as f64 / requests as f64
            } else {
                0.0
            },
            avg_latency,
            p99_latency,
        }
    }

    /// é‡ç½®ç»Ÿè®¡
    pub fn reset(&self) {
        self.requests.store(0, std::sync::atomic::Ordering::Relaxed);
        self.errors.store(0, std::sync::atomic::Ordering::Relaxed);
        self.latencies.lock().unwrap().clear();
    }
}

use std::sync::atomic::AtomicU64;

#[derive(Debug)]
pub struct MetricsStats {
    pub requests: u64,
    pub errors: u64,
    pub error_rate: f64,
    pub avg_latency: Duration,
    pub p99_latency: Duration,
}
```

---

## 4. ä»£ç è´¨é‡

### 4.1 ç®—æ³•è®¾è®¡æ¨¡å¼

#### ç­–ç•¥æ¨¡å¼

```rust
/// æ’åºç­–ç•¥
pub trait SortStrategy {
    fn sort(&self, arr: &mut [i32]);
}

pub struct QuickSort;
pub struct MergeSort;
pub struct HeapSort;

impl SortStrategy for QuickSort {
    fn sort(&self, arr: &mut [i32]) {
        // å¿«é€Ÿæ’åºå®ç°
        arr.sort_unstable();
    }
}

impl SortStrategy for MergeSort {
    fn sort(&self, arr: &mut [i32]) {
        // å½’å¹¶æ’åºå®ç°
        arr.sort();
    }
}

impl SortStrategy for HeapSort {
    fn sort(&self, arr: &mut [i32]) {
        // å †æ’åºå®ç°
        arr.sort();
    }
}

/// æ’åºä¸Šä¸‹æ–‡
pub struct Sorter {
    strategy: Box<dyn SortStrategy>,
}

impl Sorter {
    pub fn new(strategy: Box<dyn SortStrategy>) -> Self {
        Self { strategy }
    }

    pub fn sort(&self, arr: &mut [i32]) {
        self.strategy.sort(arr);
    }

    pub fn set_strategy(&mut self, strategy: Box<dyn SortStrategy>) {
        self.strategy = strategy;
    }
}

/// ä½¿ç”¨ç¤ºä¾‹
pub fn strategy_pattern_example() {
    let mut data = vec![5, 2, 8, 1, 9];

    let mut sorter = Sorter::new(Box::new(QuickSort));
    sorter.sort(&mut data);

    // åˆ‡æ¢ç­–ç•¥
    sorter.set_strategy(Box::new(MergeSort));
    sorter.sort(&mut data);
}
```

### 4.2 å•å…ƒæµ‹è¯•

#### å‚æ•°åŒ–æµ‹è¯•

```rust
#[cfg(test)]
mod tests {
    use super::*;

    /// å‚æ•°åŒ–æµ‹è¯•å®
    macro_rules! test_sort {
        ($name:ident, $sorter:expr) => {
            #[test]
            fn $name() {
                let test_cases = vec![
                    (vec![5, 2, 8, 1, 9], vec![1, 2, 5, 8, 9]),
                    (vec![1, 2, 3, 4, 5], vec![1, 2, 3, 4, 5]),
                    (vec![5, 4, 3, 2, 1], vec![1, 2, 3, 4, 5]),
                    (vec![], vec![]),
                    (vec![1], vec![1]),
                ];

                for (mut input, expected) in test_cases {
                    $sorter.sort(&mut input);
                    assert_eq!(input, expected);
                }
            }
        };
    }

    test_sort!(test_quick_sort, QuickSort);
    test_sort!(test_merge_sort, MergeSort);
    test_sort!(test_heap_sort, HeapSort);
}
```

### 4.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

#### Criterion åŸºå‡†æµ‹è¯•

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};

fn benchmark_sorts(c: &mut Criterion) {
    let mut group = c.benchmark_group("sorting");

    for size in [100, 1000, 10000].iter() {
        let mut data: Vec<i32> = (0..*size).collect();

        group.bench_with_input(BenchmarkId::new("quick_sort", size), &data, |b, data| {
            b.iter(|| {
                let mut arr = data.clone();
                QuickSort.sort(black_box(&mut arr));
            });
        });

        group.bench_with_input(BenchmarkId::new("merge_sort", size), &data, |b, data| {
            b.iter(|| {
                let mut arr = data.clone();
                MergeSort.sort(black_box(&mut arr));
            });
        });
    }

    group.finish();
}

criterion_group!(benches, benchmark_sorts);
criterion_main!(benches);
```

---

## 5. ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### 5.1 é…ç½®ç®¡ç†

```rust
use serde::{Deserialize, Serialize};

/// ç®—æ³•é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlgorithmConfig {
    pub cache_size: usize,
    pub batch_size: usize,
    pub timeout_ms: u64,
    pub retry_count: usize,
    pub degradation_thresholds: Vec<f64>,
}

impl Default for AlgorithmConfig {
    fn default() -> Self {
        Self {
            cache_size: 10000,
            batch_size: 100,
            timeout_ms: 5000,
            retry_count: 3,
            degradation_thresholds: vec![0.7, 0.85, 0.95],
        }
    }
}

impl AlgorithmConfig {
    /// ä»æ–‡ä»¶åŠ è½½é…ç½®
    pub fn from_file(path: &str) -> std::io::Result<Self> {
        let content = std::fs::read_to_string(path)?;
        let config = serde_json::from_str(&content)
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))?;
        Ok(config)
    }

    /// ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    pub fn save_to_file(&self, path: &str) -> std::io::Result<()> {
        let content = serde_json::to_string_pretty(self)
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))?;
        std::fs::write(path, content)
    }
}
```

### 5.2 ç°åº¦å‘å¸ƒ

```rust
/// ç°åº¦å‘å¸ƒç®¡ç†å™¨
pub struct GrayReleaseManager {
    gray_ratio: Arc<Mutex<f64>>,  // ç°åº¦æ¯”ä¾‹ [0, 1]
}

impl GrayReleaseManager {
    pub fn new(initial_ratio: f64) -> Self {
        Self {
            gray_ratio: Arc::new(Mutex::new(initial_ratio.clamp(0.0, 1.0))),
        }
    }

    /// æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°ç‰ˆæœ¬ç®—æ³•
    pub fn should_use_new_version(&self, user_id: u64) -> bool {
        let ratio = *self.gray_ratio.lock().unwrap();
        let hash = self.hash_user(user_id);
        (hash % 100) as f64 / 100.0 < ratio
    }

    /// æ›´æ–°ç°åº¦æ¯”ä¾‹
    pub fn set_ratio(&self, ratio: f64) {
        let mut r = self.gray_ratio.lock().unwrap();
        *r = ratio.clamp(0.0, 1.0);
    }

    fn hash_user(&self, user_id: u64) -> u64 {
        user_id % 100
    }
}

/// ä½¿ç”¨ç¤ºä¾‹
pub fn gray_release_example(user_id: u64) -> Vec<String> {
    let manager = GrayReleaseManager::new(0.1); // 10% ç°åº¦

    if manager.should_use_new_version(user_id) {
        // æ–°ç‰ˆæœ¬ç®—æ³•
        compute_recommendations_v2(user_id)
    } else {
        // æ—§ç‰ˆæœ¬ç®—æ³•
        compute_recommendations_v1(user_id)
    }
}

fn compute_recommendations_v1(_user_id: u64) -> Vec<String> {
    vec!["old1".to_string(), "old2".to_string()]
}

fn compute_recommendations_v2(_user_id: u64) -> Vec<String> {
    vec!["new1".to_string(), "new2".to_string()]
}
```

### 5.3 æ•…éšœæ¢å¤

```rust
/// çŠ¶æ€å¿«ç…§
#[derive(Clone, Serialize, Deserialize)]
pub struct Snapshot<T> {
    data: T,
    timestamp: u64,
}

/// å¿«ç…§ç®¡ç†å™¨
pub struct SnapshotManager<T: Clone + Serialize + for<'de> Deserialize<'de>> {
    snapshot_dir: String,
}

impl<T: Clone + Serialize + for<'de> Deserialize<'de>> SnapshotManager<T> {
    pub fn new(snapshot_dir: String) -> Self {
        std::fs::create_dir_all(&snapshot_dir).unwrap();
        Self { snapshot_dir }
    }

    /// ä¿å­˜å¿«ç…§
    pub fn save(&self, data: &T) -> std::io::Result<()> {
        let timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();

        let snapshot = Snapshot {
            data: data.clone(),
            timestamp,
        };

        let path = format!("{}/snapshot_{}.json", self.snapshot_dir, timestamp);
        let content = serde_json::to_string(&snapshot)
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))?;

        std::fs::write(path, content)
    }

    /// æ¢å¤æœ€æ–°å¿«ç…§
    pub fn restore_latest(&self) -> std::io::Result<Option<T>> {
        let entries = std::fs::read_dir(&self.snapshot_dir)?;

        let mut latest_snapshot: Option<(u64, String)> = None;

        for entry in entries {
            let entry = entry?;
            let file_name = entry.file_name().to_string_lossy().to_string();

            if let Some(timestamp_str) = file_name.strip_prefix("snapshot_").and_then(|s| s.strip_suffix(".json")) {
                if let Ok(timestamp) = timestamp_str.parse::<u64>() {
                    if latest_snapshot.is_none() || timestamp > latest_snapshot.as_ref().unwrap().0 {
                        latest_snapshot = Some((timestamp, entry.path().to_string_lossy().to_string()));
                    }
                }
            }
        }

        if let Some((_, path)) = latest_snapshot {
            let content = std::fs::read_to_string(path)?;
            let snapshot: Snapshot<T> = serde_json::from_str(&content)
                .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))?;
            Ok(Some(snapshot.data))
        } else {
            Ok(None)
        }
    }
}
```

---

## 6. å®æˆ˜æ¡ˆä¾‹

### 6.1 æ¨èç³»ç»Ÿ

```rust
use std::collections::HashMap;

/// ååŒè¿‡æ»¤æ¨èå¼•æ“
pub struct CollaborativeFilteringEngine {
    user_item_matrix: HashMap<u64, HashMap<u64, f64>>,  // user_id -> (item_id -> rating)
    cache: Arc<ThreadSafeLRUCache<u64, Vec<u64>>>,
}

impl CollaborativeFilteringEngine {
    pub fn new(capacity: usize) -> Self {
        Self {
            user_item_matrix: HashMap::new(),
            cache: Arc::new(ThreadSafeLRUCache::new(capacity)),
        }
    }

    /// æ·»åŠ ç”¨æˆ·è¯„åˆ†
    pub fn add_rating(&mut self, user_id: u64, item_id: u64, rating: f64) {
        self.user_item_matrix
            .entry(user_id)
            .or_insert_with(HashMap::new)
            .insert(item_id, rating);

        // æ¸…é™¤ç¼“å­˜
        // cache.remove(&user_id);
    }

    /// æ¨èç‰©å“
    pub fn recommend(&self, user_id: u64, top_k: usize) -> Vec<u64> {
        // æ£€æŸ¥ç¼“å­˜
        if let Some(cached) = self.cache.get(&user_id) {
            return cached;
        }

        // è®¡ç®—æ¨è
        let recommendations = self.compute_recommendations(user_id, top_k);

        // æ›´æ–°ç¼“å­˜
        self.cache.put(user_id, recommendations.clone());

        recommendations
    }

    fn compute_recommendations(&self, user_id: u64, top_k: usize) -> Vec<u64> {
        let user_ratings = match self.user_item_matrix.get(&user_id) {
            Some(ratings) => ratings,
            None => return Vec::new(),
        };

        let mut scores: HashMap<u64, f64> = HashMap::new();

        // æ‰¾åˆ°ç›¸ä¼¼ç”¨æˆ·
        for (other_user_id, other_ratings) in &self.user_item_matrix {
            if *other_user_id == user_id {
                continue;
            }

            let similarity = self.cosine_similarity(user_ratings, other_ratings);

            // åŠ æƒè®¡ç®—ç‰©å“åˆ†æ•°
            for (&item_id, &rating) in other_ratings {
                if !user_ratings.contains_key(&item_id) {
                    *scores.entry(item_id).or_insert(0.0) += similarity * rating;
                }
            }
        }

        // æ’åºå¹¶è¿”å› top K
        let mut items: Vec<_> = scores.into_iter().collect();
        items.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        items.into_iter().take(top_k).map(|(id, _)| id).collect()
    }

    fn cosine_similarity(&self, a: &HashMap<u64, f64>, b: &HashMap<u64, f64>) -> f64 {
        let mut dot_product = 0.0;
        let mut norm_a = 0.0;
        let mut norm_b = 0.0;

        for (&item_id, &rating_a) in a {
            if let Some(&rating_b) = b.get(&item_id) {
                dot_product += rating_a * rating_b;
            }
            norm_a += rating_a * rating_a;
        }

        for &rating_b in b.values() {
            norm_b += rating_b * rating_b;
        }

        if norm_a == 0.0 || norm_b == 0.0 {
            0.0
        } else {
            dot_product / (norm_a.sqrt() * norm_b.sqrt())
        }
    }
}
```

### 6.2 å®æ—¶æ’è¡Œæ¦œ

```rust
use std::collections::BinaryHeap;
use std::cmp::Reverse;

/// å®æ—¶æ’è¡Œæ¦œ
pub struct Leaderboard {
    scores: HashMap<String, i64>,
    top_k: BinaryHeap<(i64, String)>,
    k: usize,
}

impl Leaderboard {
    pub fn new(k: usize) -> Self {
        Self {
            scores: HashMap::new(),
            top_k: BinaryHeap::new(),
            k,
        }
    }

    /// æ›´æ–°åˆ†æ•°
    pub fn update_score(&mut self, player: String, score: i64) {
        self.scores.insert(player.clone(), score);
        self.rebuild_top_k();
    }

    /// é‡å»º Top K
    fn rebuild_top_k(&mut self) {
        self.top_k.clear();

        for (player, &score) in &self.scores {
            if self.top_k.len() < self.k {
                self.top_k.push((score, player.clone()));
            } else if score > self.top_k.peek().unwrap().0 {
                self.top_k.pop();
                self.top_k.push((score, player.clone()));
            }
        }
    }

    /// è·å– Top K
    pub fn get_top_k(&self) -> Vec<(String, i64)> {
        let mut result: Vec<_> = self.top_k.iter()
            .map(|(score, player)| (player.clone(), *score))
            .collect();

        result.sort_by(|a, b| b.1.cmp(&a.1));
        result
    }
}
```

### 6.3 åˆ†å¸ƒå¼é™æµ

```rust
use std::time::{Duration, Instant};

/// ä»¤ç‰Œæ¡¶é™æµå™¨
pub struct TokenBucket {
    capacity: usize,
    tokens: Arc<Mutex<usize>>,
    refill_rate: usize,  // æ¯ç§’è¡¥å……çš„ä»¤ç‰Œæ•°
    last_refill: Arc<Mutex<Instant>>,
}

impl TokenBucket {
    pub fn new(capacity: usize, refill_rate: usize) -> Self {
        Self {
            capacity,
            tokens: Arc::new(Mutex::new(capacity)),
            refill_rate,
            last_refill: Arc::new(Mutex::new(Instant::now())),
        }
    }

    /// å°è¯•è·å–ä»¤ç‰Œ
    pub fn try_acquire(&self, count: usize) -> bool {
        self.refill();

        let mut tokens = self.tokens.lock().unwrap();

        if *tokens >= count {
            *tokens -= count;
            true
        } else {
            false
        }
    }

    /// è¡¥å……ä»¤ç‰Œ
    fn refill(&self) {
        let mut last_refill = self.last_refill.lock().unwrap();
        let now = Instant::now();
        let elapsed = now.duration_since(*last_refill).as_secs_f64();

        if elapsed >= 1.0 {
            let new_tokens = (elapsed * self.refill_rate as f64) as usize;

            let mut tokens = self.tokens.lock().unwrap();
            *tokens = (*tokens + new_tokens).min(self.capacity);

            *last_refill = now;
        }
    }
}

/// ä½¿ç”¨ç¤ºä¾‹ï¼šAPI é™æµ
pub async fn rate_limited_api(limiter: &TokenBucket, request: String) -> Result<String, String> {
    if limiter.try_acquire(1) {
        // å¤„ç†è¯·æ±‚
        Ok(format!("å¤„ç†è¯·æ±‚: {}", request))
    } else {
        Err("é™æµä¸­ï¼Œè¯·ç¨åé‡è¯•".to_string())
    }
}
```

---

## 7. å‚è€ƒèµ„æ–™

### æ•™æ

- **[Kleppmann]** Kleppmann. _Designing Data-Intensive Applications_
- **[Beyer et al.]** Beyer, Jones, Petoff, Murphy. _Site Reliability Engineering_
- **[Newman]** Newman. _Building Microservices_ (2nd Edition)

### æ€§èƒ½ä¼˜åŒ–

- **[Chandler Carruth]** "Efficiency with Algorithms, Performance with Data Structures" (CppCon)
- **[Rust Performance Book]** <https://nnethercote.github.io/perf-book/>

### ç”Ÿäº§å®è·µ

- **[Google SRE Book]** <https://sre.google/books/>
- **[AWS Well-Architected]** <https://aws.amazon.com/architecture/well-architected/>
- **[The Twelve-Factor App]** <https://12factor.net/>

---

**æ–‡æ¡£å®Œæˆåº¦**: 100%
**ä»£ç ç¤ºä¾‹æ•°**: 40+
**å·¥ç¨‹å®è·µä¸»é¢˜**: 10+
**ç”Ÿäº§æ¡ˆä¾‹**: æ¨èç³»ç»Ÿã€æ’è¡Œæ¦œã€é™æµ

**ä¸‹ä¸€æ­¥**: å‚è§ [`05_å‰æ²¿ç®—æ³•æŠ€æœ¯.md`](./05_å‰æ²¿ç®—æ³•æŠ€æœ¯.md)
