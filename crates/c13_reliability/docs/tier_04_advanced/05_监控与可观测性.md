# 05 监控与可观测性

## 目录

- [05 监控与可观测性](#05-监控与可观测性)
  - [目录](#目录)
  - [1. 概述](#1-概述)
  - [2. 高级指标设计](#2-高级指标设计)
    - [2.1 SLO/SLI/SLA](#21-sloslisla)
      - [2.1.1 概念定义](#211-概念定义)
      - [2.1.2 Rust 实现 SLI 跟踪](#212-rust-实现-sli-跟踪)
      - [2.1.3 Prometheus 查询 SLO](#213-prometheus-查询-slo)
    - [2.2 RED 方法](#22-red-方法)
      - [2.2.1 实现 RED 指标](#221-实现-red-指标)
      - [2.2.2 Grafana 仪表板](#222-grafana-仪表板)
    - [2.3 USE 方法](#23-use-方法)
      - [2.3.1 系统资源监控](#231-系统资源监控)
    - [2.4 自定义业务指标](#24-自定义业务指标)
      - [2.4.1 电商系统示例](#241-电商系统示例)
  - [3. 分布式追踪](#3-分布式追踪)
    - [3.1 OpenTelemetry 集成](#31-opentelemetry-集成)
      - [3.1.1 初始化 Tracer](#311-初始化-tracer)
      - [3.1.2 手动 Span 创建](#312-手动-span-创建)
    - [3.2 Jaeger 部署](#32-jaeger-部署)
      - [3.2.1 Docker Compose](#321-docker-compose)
    - [3.3 追踪上下文传播](#33-追踪上下文传播)
      - [3.3.1 HTTP 头传播](#331-http-头传播)
      - [3.3.2 跨服务调用](#332-跨服务调用)
    - [3.4 性能分析](#34-性能分析)
      - [3.4.1 识别慢请求](#341-识别慢请求)
      - [3.4.2 分析 Span 依赖](#342-分析-span-依赖)
  - [4. 日志聚合](#4-日志聚合)
    - [4.1 结构化日志](#41-结构化日志)
      - [4.1.1 使用 `tracing` + `tracing-subscriber`](#411-使用-tracing--tracing-subscriber)
    - [4.2 ELK Stack 集成](#42-elk-stack-集成)
      - [4.2.1 Filebeat 配置](#421-filebeat-配置)
      - [4.2.2 Rust 直接发送到 Elasticsearch](#422-rust-直接发送到-elasticsearch)
    - [4.3 Loki 轻量级方案](#43-loki-轻量级方案)
      - [4.3.1 Promtail 配置](#431-promtail-配置)
      - [4.3.2 Docker Compose](#432-docker-compose)
    - [4.4 日志采样](#44-日志采样)
      - [4.4.1 采样策略](#441-采样策略)
      - [4.4.2 成本优化策略](#442-成本优化策略)
  - [5. 告警策略](#5-告警策略)
    - [5.1 基于阈值](#51-基于阈值)
      - [5.1.1 Prometheus Alerting Rules](#511-prometheus-alerting-rules)
    - [5.2 基于趋势](#52-基于趋势)
      - [5.2.1 预测告警](#521-预测告警)
    - [5.3 异常检测](#53-异常检测)
      - [5.3.1 使用统计方法](#531-使用统计方法)
    - [5.4 告警降噪](#54-告警降噪)
      - [5.4.1 告警分组与抑制](#541-告警分组与抑制)
  - [6. 可观测性即代码](#6-可观测性即代码)
    - [6.1 Terraform 基础设施](#61-terraform-基础设施)
    - [6.2 Grafana as Code](#62-grafana-as-code)
    - [6.3 配置版本化](#63-配置版本化)
  - [7. 实战案例](#7-实战案例)
    - [7.1 微服务全链路追踪](#71-微服务全链路追踪)
    - [7.2 高基数指标优化](#72-高基数指标优化)
    - [7.3 日志成本控制](#73-日志成本控制)
  - [8. 相关资源](#8-相关资源)
    - [8.1 工具与框架](#81-工具与框架)
    - [8.2 学习资源](#82-学习资源)

---

## 1. 概述

**监控与可观测性**是现代分布式系统可靠性的基石。
监控关注系统的**已知问题**（如 CPU、内存），而可观测性则让我们能够**探索未知问题**（通过指标、日志、追踪的关联分析）。

**本文档涵盖**：

- **高级指标设计**：SLO/SLI/SLA、RED/USE 方法
- **分布式追踪**：OpenTelemetry、Jaeger、上下文传播
- **日志聚合**：结构化日志、ELK/Loki、采样策略
- **告警策略**：阈值/趋势/异常检测、降噪
- **可观测性即代码**：Terraform、Grafana as Code

**适用场景**：

- ✅ 构建生产级微服务监控
- ✅ 实现 SLO 驱动的可靠性工程
- ✅ 优化大规模系统的可观测性成本
- ✅ 设计智能告警系统

**学习路径**：

1. 掌握 SLO/SLI 指标体系设计
2. 实践分布式追踪与性能分析
3. 构建高效日志聚合管道
4. 设计智能告警策略
5. 应用基础设施即代码管理可观测性

---

## 2. 高级指标设计

### 2.1 SLO/SLI/SLA

#### 2.1.1 概念定义

| 术语 | 含义 | 示例 |
|------|------|------|
| **SLI** (Service Level Indicator) | 服务质量指标 | 成功请求率、P99 延迟 |
| **SLO** (Service Level Objective) | 服务质量目标 | 99.9% 可用性、P99 < 200ms |
| **SLA** (Service Level Agreement) | 服务质量协议 | 未达 SLO 则赔偿 |

**错误预算 (Error Budget)**：

- 如果 SLO 是 99.9% 可用性，则错误预算是 0.1%
- 每月 = 30天 × 24小时 × 60分钟 × 0.1% = **43.2 分钟**

#### 2.1.2 Rust 实现 SLI 跟踪

```rust
use prometheus::{Counter, Histogram, HistogramOpts, Registry, Opts};
use std::time::Instant;

pub struct SLITracker {
    // 请求总数
    total_requests: Counter,
    // 成功请求数
    successful_requests: Counter,
    // 请求延迟
    request_duration: Histogram,
}

impl SLITracker {
    pub fn new(registry: &Registry) -> Self {
        let total = Counter::with_opts(
            Opts::new("http_requests_total", "Total HTTP requests")
        ).unwrap();
        
        let success = Counter::with_opts(
            Opts::new("http_requests_success_total", "Successful HTTP requests")
        ).unwrap();
        
        let duration = Histogram::with_opts(
            HistogramOpts::new("http_request_duration_seconds", "HTTP request latency")
                .buckets(vec![0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0])
        ).unwrap();
        
        registry.register(Box::new(total.clone())).unwrap();
        registry.register(Box::new(success.clone())).unwrap();
        registry.register(Box::new(duration.clone())).unwrap();
        
        Self {
            total_requests: total,
            successful_requests: success,
            request_duration: duration,
        }
    }
    
    pub fn track_request<F, T>(&self, f: F) -> Result<T, Box<dyn std::error::Error>>
    where
        F: FnOnce() -> Result<T, Box<dyn std::error::Error>>,
    {
        self.total_requests.inc();
        let start = Instant::now();
        
        let result = f();
        
        let duration = start.elapsed().as_secs_f64();
        self.request_duration.observe(duration);
        
        if result.is_ok() {
            self.successful_requests.inc();
        }
        
        result
    }
    
    // 计算当前成功率
    pub fn success_rate(&self) -> f64 {
        let total = self.total_requests.get();
        if total == 0.0 {
            return 1.0;
        }
        self.successful_requests.get() / total
    }
    
    // 检查是否符合 SLO
    pub fn meets_slo(&self, target_success_rate: f64) -> bool {
        self.success_rate() >= target_success_rate
    }
}

// 使用示例
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_sli_tracking() {
        let registry = Registry::new();
        let tracker = SLITracker::new(&registry);
        
        // 模拟 100 个请求
        for i in 0..100 {
            let _ = tracker.track_request(|| {
                if i < 99 {
                    Ok(()) // 99% 成功
                } else {
                    Err("Simulated error".into())
                }
            });
        }
        
        // 检查 SLO (目标 99%)
        assert!(tracker.meets_slo(0.99));
        assert!(!tracker.meets_slo(0.999)); // 未达到 99.9%
    }
}
```

#### 2.1.3 Prometheus 查询 SLO

```promql
# 计算过去 30 天的可用性
sum(increase(http_requests_success_total[30d])) 
/ 
sum(increase(http_requests_total[30d]))

# 剩余错误预算 (假设 SLO 99.9%)
1 - (
    sum(increase(http_requests_success_total[30d])) 
    / 
    sum(increase(http_requests_total[30d]))
) / 0.001

# P99 延迟
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

---

### 2.2 RED 方法

**RED** 适用于**面向请求的服务**：

- **R**ate: 请求速率
- **E**rrors: 错误率
- **D**uration: 请求时长

#### 2.2.1 实现 RED 指标

```rust
use prometheus::{Counter, Histogram, HistogramVec, CounterVec, Registry, Opts, HistogramOpts};

pub struct REDMetrics {
    // Rate: 请求总数（通过 rate() 计算速率）
    requests_total: CounterVec,
    // Errors: 错误请求数
    errors_total: CounterVec,
    // Duration: 请求时长
    duration_seconds: HistogramVec,
}

impl REDMetrics {
    pub fn new(registry: &Registry) -> Self {
        let requests = CounterVec::new(
            Opts::new("http_requests_total", "Total HTTP requests"),
            &["method", "endpoint", "status"]
        ).unwrap();
        
        let errors = CounterVec::new(
            Opts::new("http_errors_total", "Total HTTP errors"),
            &["method", "endpoint", "error_type"]
        ).unwrap();
        
        let duration = HistogramVec::new(
            HistogramOpts::new("http_duration_seconds", "HTTP request duration")
                .buckets(vec![0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]),
            &["method", "endpoint"]
        ).unwrap();
        
        registry.register(Box::new(requests.clone())).unwrap();
        registry.register(Box::new(errors.clone())).unwrap();
        registry.register(Box::new(duration.clone())).unwrap();
        
        Self {
            requests_total: requests,
            errors_total: errors,
            duration_seconds: duration,
        }
    }
    
    pub fn record_request(
        &self,
        method: &str,
        endpoint: &str,
        status: u16,
        duration: f64,
        error: Option<&str>,
    ) {
        // Rate
        self.requests_total
            .with_label_values(&[method, endpoint, &status.to_string()])
            .inc();
        
        // Duration
        self.duration_seconds
            .with_label_values(&[method, endpoint])
            .observe(duration);
        
        // Errors
        if let Some(err) = error {
            self.errors_total
                .with_label_values(&[method, endpoint, err])
                .inc();
        }
    }
}

// Axum 集成示例
use axum::{
    extract::State,
    http::{Request, StatusCode},
    middleware::Next,
    response::Response,
};
use std::sync::Arc;
use std::time::Instant;

async fn metrics_middleware<B>(
    State(metrics): State<Arc<REDMetrics>>,
    request: Request<B>,
    next: Next<B>,
) -> Response {
    let start = Instant::now();
    let method = request.method().to_string();
    let path = request.uri().path().to_string();
    
    let response = next.run(request).await;
    
    let duration = start.elapsed().as_secs_f64();
    let status = response.status().as_u16();
    let error = if status >= 500 {
        Some("server_error")
    } else if status >= 400 {
        Some("client_error")
    } else {
        None
    };
    
    metrics.record_request(&method, &path, status, duration, error);
    
    response
}
```

#### 2.2.2 Grafana 仪表板

```json
{
  "panels": [
    {
      "title": "Request Rate (req/s)",
      "targets": [
        {
          "expr": "sum(rate(http_requests_total[5m])) by (endpoint)"
        }
      ]
    },
    {
      "title": "Error Rate (%)",
      "targets": [
        {
          "expr": "sum(rate(http_errors_total[5m])) / sum(rate(http_requests_total[5m])) * 100"
        }
      ]
    },
    {
      "title": "P99 Duration (ms)",
      "targets": [
        {
          "expr": "histogram_quantile(0.99, rate(http_duration_seconds_bucket[5m])) * 1000"
        }
      ]
    }
  ]
}
```

---

### 2.3 USE 方法

**USE** 适用于**资源监控**：

- **U**tilization: 利用率（如 CPU 80%）
- **S**aturation: 饱和度（如等待队列长度）
- **E**rrors: 错误数（如网络丢包）

#### 2.3.1 系统资源监控

```rust
use sysinfo::{System, SystemExt, ProcessExt, CpuExt};
use prometheus::{Gauge, GaugeVec, Registry, Opts};

pub struct USEMetrics {
    // Utilization
    cpu_usage: GaugeVec,
    memory_usage: Gauge,
    disk_usage: GaugeVec,
    
    // Saturation
    load_average: GaugeVec,
    thread_count: Gauge,
    
    // Errors
    io_errors: GaugeVec,
}

impl USEMetrics {
    pub fn new(registry: &Registry) -> Self {
        let cpu = GaugeVec::new(
            Opts::new("system_cpu_usage_percent", "CPU usage percentage"),
            &["cpu"]
        ).unwrap();
        
        let memory = Gauge::new("system_memory_usage_bytes", "Memory usage in bytes").unwrap();
        
        let disk = GaugeVec::new(
            Opts::new("system_disk_usage_percent", "Disk usage percentage"),
            &["mount"]
        ).unwrap();
        
        let load = GaugeVec::new(
            Opts::new("system_load_average", "System load average"),
            &["period"]
        ).unwrap();
        
        let threads = Gauge::new("process_thread_count", "Number of threads").unwrap();
        
        let io = GaugeVec::new(
            Opts::new("system_io_errors_total", "I/O errors"),
            &["device"]
        ).unwrap();
        
        registry.register(Box::new(cpu.clone())).unwrap();
        registry.register(Box::new(memory.clone())).unwrap();
        registry.register(Box::new(disk.clone())).unwrap();
        registry.register(Box::new(load.clone())).unwrap();
        registry.register(Box::new(threads.clone())).unwrap();
        registry.register(Box::new(io.clone())).unwrap();
        
        Self {
            cpu_usage: cpu,
            memory_usage: memory,
            disk_usage: disk,
            load_average: load,
            thread_count: threads,
            io_errors: io,
        }
    }
    
    pub fn update(&self) {
        let mut sys = System::new_all();
        sys.refresh_all();
        
        // CPU Utilization
        for (i, cpu) in sys.cpus().iter().enumerate() {
            self.cpu_usage
                .with_label_values(&[&format!("cpu{}", i)])
                .set(cpu.cpu_usage() as f64);
        }
        
        // Memory Utilization
        self.memory_usage.set(sys.used_memory() as f64);
        
        // Load Average (Saturation)
        let load = sys.load_average();
        self.load_average.with_label_values(&["1m"]).set(load.one);
        self.load_average.with_label_values(&["5m"]).set(load.five);
        self.load_average.with_label_values(&["15m"]).set(load.fifteen);
        
        // Thread Count (Saturation)
        if let Some(process) = sys.process(sysinfo::get_current_pid().unwrap()) {
            // Note: sysinfo doesn't directly provide thread count, use std::thread
            self.thread_count.set(1.0); // Placeholder
        }
    }
}

// 后台更新任务
use tokio::time::{interval, Duration};

pub async fn metrics_collector(metrics: Arc<USEMetrics>) {
    let mut ticker = interval(Duration::from_secs(15));
    
    loop {
        ticker.tick().await;
        metrics.update();
    }
}
```

---

### 2.4 自定义业务指标

#### 2.4.1 电商系统示例

```rust
use prometheus::{Counter, Histogram, Gauge, Registry, Opts, HistogramOpts};

pub struct BusinessMetrics {
    // 订单相关
    orders_created: Counter,
    orders_completed: Counter,
    orders_cancelled: Counter,
    order_value: Histogram,
    
    // 库存相关
    inventory_level: Gauge,
    low_stock_items: Gauge,
    
    // 支付相关
    payment_success: Counter,
    payment_failures: Counter,
    payment_duration: Histogram,
}

impl BusinessMetrics {
    pub fn new(registry: &Registry) -> Self {
        let orders_created = Counter::with_opts(
            Opts::new("orders_created_total", "Total orders created")
        ).unwrap();
        
        let order_value = Histogram::with_opts(
            HistogramOpts::new("order_value_usd", "Order value in USD")
                .buckets(vec![10.0, 50.0, 100.0, 500.0, 1000.0, 5000.0])
        ).unwrap();
        
        let inventory = Gauge::with_opts(
            Opts::new("inventory_items_count", "Current inventory count")
        ).unwrap();
        
        // ... 注册所有指标
        
        Self {
            orders_created,
            orders_completed: Counter::default(),
            orders_cancelled: Counter::default(),
            order_value,
            inventory_level: inventory,
            low_stock_items: Gauge::default(),
            payment_success: Counter::default(),
            payment_failures: Counter::default(),
            payment_duration: Histogram::default(),
        }
    }
    
    pub fn record_order(&self, value: f64) {
        self.orders_created.inc();
        self.order_value.observe(value);
    }
    
    pub fn record_payment(&self, duration: f64, success: bool) {
        if success {
            self.payment_success.inc();
        } else {
            self.payment_failures.inc();
        }
        self.payment_duration.observe(duration);
    }
}
```

---

## 3. 分布式追踪

### 3.1 OpenTelemetry 集成

#### 3.1.1 初始化 Tracer

```toml
[dependencies]
opentelemetry = "0.21"
opentelemetry-jaeger = "0.20"
tracing = "0.1"
tracing-subscriber = "0.3"
tracing-opentelemetry = "0.22"
```

```rust
use opentelemetry::global;
use opentelemetry::sdk::trace::{self, Sampler};
use opentelemetry_jaeger::JaegerPipeline;
use tracing_subscriber::layer::SubscriberExt;
use tracing_subscriber::Registry;

pub fn init_tracer(service_name: &str) -> Result<(), Box<dyn std::error::Error>> {
    // 配置 Jaeger exporter
    let tracer = opentelemetry_jaeger::new_agent_pipeline()
        .with_service_name(service_name)
        .with_trace_config(
            trace::config()
                .with_sampler(Sampler::AlwaysOn) // 生产环境使用 TraceIdRatioBased(0.01)
                .with_max_attributes_per_span(64)
        )
        .install_batch(opentelemetry::runtime::Tokio)?;
    
    // 创建 tracing 层
    let telemetry = tracing_opentelemetry::layer().with_tracer(tracer);
    
    let subscriber = Registry::default()
        .with(telemetry)
        .with(tracing_subscriber::fmt::layer());
    
    tracing::subscriber::set_global_default(subscriber)?;
    
    Ok(())
}

pub fn shutdown_tracer() {
    global::shutdown_tracer_provider();
}
```

#### 3.1.2 手动 Span 创建

```rust
use tracing::{info, instrument, Span};
use std::time::Duration;

#[instrument(name = "process_order", fields(order_id = %order_id))]
async fn process_order(order_id: u64) -> Result<(), Box<dyn std::error::Error>> {
    info!("Starting order processing");
    
    // 子 span 1: 验证库存
    let _guard = tracing::info_span!("check_inventory", order_id = %order_id).entered();
    tokio::time::sleep(Duration::from_millis(50)).await;
    info!("Inventory checked");
    drop(_guard);
    
    // 子 span 2: 处理支付
    let payment_result = {
        let _guard = tracing::info_span!("process_payment", order_id = %order_id).entered();
        tokio::time::sleep(Duration::from_millis(100)).await;
        info!("Payment processed");
        Ok::<_, Box<dyn std::error::Error>>(())
    };
    
    payment_result?;
    
    // 子 span 3: 发货
    {
        let _guard = tracing::info_span!("ship_order", order_id = %order_id).entered();
        tokio::time::sleep(Duration::from_millis(200)).await;
        info!("Order shipped");
    }
    
    info!("Order processing completed");
    Ok(())
}
```

---

### 3.2 Jaeger 部署

#### 3.2.1 Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  jaeger:
    image: jaegertracing/all-in-one:1.49
    container_name: jaeger
    ports:
      - "5775:5775/udp"   # Zipkin compatible
      - "6831:6831/udp"   # Compact thrift
      - "6832:6832/udp"   # Binary thrift
      - "5778:5778"       # Sampling config
      - "16686:16686"     # Web UI
      - "14268:14268"     # HTTP collector
      - "14250:14250"     # gRPC
      - "9411:9411"       # Zipkin compatible
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - LOG_LEVEL=debug
```

```bash
# 启动 Jaeger
docker-compose up -d

# 访问 UI
# http://localhost:16686
```

---

### 3.3 追踪上下文传播

#### 3.3.1 HTTP 头传播

```rust
use axum::{
    extract::Request,
    http::HeaderMap,
    middleware::Next,
    response::Response,
};
use opentelemetry::global;
use opentelemetry::propagation::TextMapPropagator;
use opentelemetry_sdk::propagation::TraceContextPropagator;
use tracing::Span;

pub async fn trace_propagation_middleware(
    headers: HeaderMap,
    mut request: Request,
    next: Next,
) -> Response {
    let propagator = TraceContextPropagator::new();
    
    // 从 HTTP 头提取追踪上下文
    let parent_cx = propagator.extract(&HeaderExtractor(&headers));
    
    // 将上下文附加到当前 span
    let span = Span::current();
    span.set_parent(parent_cx);
    
    // 继续处理请求
    let response = next.run(request).await;
    
    response
}

// 辅助类：从 HTTP 头读取追踪信息
struct HeaderExtractor<'a>(&'a HeaderMap);

impl<'a> opentelemetry::propagation::Extractor for HeaderExtractor<'a> {
    fn get(&self, key: &str) -> Option<&str> {
        self.0.get(key).and_then(|v| v.to_str().ok())
    }
    
    fn keys(&self) -> Vec<&str> {
        self.0.keys().map(|k| k.as_str()).collect()
    }
}
```

#### 3.3.2 跨服务调用

```rust
use reqwest::Client;
use opentelemetry::global;
use opentelemetry::propagation::TextMapPropagator;
use opentelemetry_sdk::propagation::TraceContextPropagator;
use tracing::instrument;

#[instrument]
async fn call_downstream_service(url: &str) -> Result<String, reqwest::Error> {
    let client = Client::new();
    let propagator = TraceContextPropagator::new();
    
    // 创建 HTTP 头注入器
    let mut headers = reqwest::header::HeaderMap::new();
    propagator.inject_context(
        &tracing::Span::current().context(),
        &mut HeaderInjector(&mut headers)
    );
    
    // 发送请求（携带追踪上下文）
    let response = client
        .get(url)
        .headers(headers)
        .send()
        .await?
        .text()
        .await?;
    
    Ok(response)
}

struct HeaderInjector<'a>(&'a mut reqwest::header::HeaderMap);

impl<'a> opentelemetry::propagation::Injector for HeaderInjector<'a> {
    fn set(&mut self, key: &str, value: String) {
        if let Ok(name) = reqwest::header::HeaderName::from_bytes(key.as_bytes()) {
            if let Ok(val) = reqwest::header::HeaderValue::from_str(&value) {
                self.0.insert(name, val);
            }
        }
    }
}
```

---

### 3.4 性能分析

#### 3.4.1 识别慢请求

```promql
# Jaeger Query (通过 UI 或 API)
# 查找 P99 > 500ms 的 trace
duration > 500ms AND service = "order-service"

# 聚合统计
SELECT 
  operation_name,
  percentile(duration, 0.99) AS p99_ms
FROM traces
WHERE service = "order-service"
GROUP BY operation_name
HAVING p99_ms > 500
```

#### 3.4.2 分析 Span 依赖

```rust
use tracing::{info, warn};

#[instrument]
async fn analyze_trace_performance(trace_id: &str) {
    // 伪代码：从 Jaeger 获取 trace
    let trace = fetch_trace(trace_id).await;
    
    for span in trace.spans {
        let duration_ms = span.duration / 1_000_000;
        
        if duration_ms > 100 {
            warn!(
                span_name = %span.operation_name,
                duration_ms,
                "Slow span detected"
            );
        }
        
        // 分析子 span 占比
        let children_duration: u64 = span.child_spans.iter()
            .map(|s| s.duration)
            .sum();
        
        let self_time = span.duration - children_duration;
        let self_time_percent = (self_time as f64 / span.duration as f64) * 100.0;
        
        info!(
            span_name = %span.operation_name,
            self_time_ms = self_time / 1_000_000,
            self_time_percent,
            "Span self-time analysis"
        );
    }
}

async fn fetch_trace(trace_id: &str) -> Trace {
    // 调用 Jaeger API
    todo!()
}

struct Trace {
    spans: Vec<SpanData>,
}

struct SpanData {
    operation_name: String,
    duration: u64,
    child_spans: Vec<SpanData>,
}
```

---

## 4. 日志聚合

### 4.1 结构化日志

#### 4.1.1 使用 `tracing` + `tracing-subscriber`

```rust
use tracing::{info, warn, error};
use tracing_subscriber::{fmt, EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};

pub fn init_logging() {
    tracing_subscriber::registry()
        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| EnvFilter::new("info")))
        .with(fmt::layer().json()) // JSON 格式输出
        .init();
}

// 结构化日志示例
fn process_user_request(user_id: u64, action: &str) {
    info!(
        user_id = %user_id,
        action = %action,
        timestamp = %chrono::Utc::now().to_rfc3339(),
        "Processing user request"
    );
    
    // 业务逻辑...
    
    if action == "delete_account" {
        warn!(
            user_id = %user_id,
            action = %action,
            "Sensitive operation detected"
        );
    }
}
```

**输出示例**：

```json
{
  "timestamp": "2025-10-23T10:30:45.123Z",
  "level": "INFO",
  "message": "Processing user request",
  "fields": {
    "user_id": 12345,
    "action": "delete_account"
  },
  "target": "my_app::handlers",
  "span": {
    "name": "process_user_request"
  }
}
```

---

### 4.2 ELK Stack 集成

#### 4.2.1 Filebeat 配置

```yaml
# filebeat.yml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/myapp/*.log
    json.keys_under_root: true
    json.add_error_key: true

output.elasticsearch:
  hosts: ["http://localhost:9200"]
  index: "myapp-logs-%{+yyyy.MM.dd}"

setup.kibana:
  host: "http://localhost:5601"
```

#### 4.2.2 Rust 直接发送到 Elasticsearch

```toml
[dependencies]
elasticsearch = "8.5"
serde_json = "1.0"
```

```rust
use elasticsearch::{Elasticsearch, http::transport::Transport, IndexParts};
use serde_json::json;

pub async fn send_log_to_es(message: &str, level: &str) -> Result<(), Box<dyn std::error::Error>> {
    let transport = Transport::single_node("http://localhost:9200")?;
    let client = Elasticsearch::new(transport);
    
    let body = json!({
        "timestamp": chrono::Utc::now().to_rfc3339(),
        "level": level,
        "message": message,
        "service": "my-service"
    });
    
    client
        .index(IndexParts::Index("myapp-logs"))
        .body(body)
        .send()
        .await?;
    
    Ok(())
}
```

---

### 4.3 Loki 轻量级方案

#### 4.3.1 Promtail 配置

```yaml
# promtail-config.yml
server:
  http_listen_port: 9080

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: myapp
          __path__: /var/log/myapp/*.log
```

#### 4.3.2 Docker Compose

```yaml
version: '3.8'

services:
  loki:
    image: grafana/loki:2.9.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml

  promtail:
    image: grafana/promtail:2.9.0
    volumes:
      - /var/log/myapp:/var/log/myapp:ro
      - ./promtail-config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml

  grafana:
    image: grafana/grafana:10.1.0
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
```

---

### 4.4 日志采样

#### 4.4.1 采样策略

```rust
use tracing::{info, Level};
use tracing_subscriber::filter::{FilterFn, LevelFilter};
use std::sync::atomic::{AtomicU64, Ordering};

static SAMPLE_COUNTER: AtomicU64 = AtomicU64::new(0);

// 1% 采样
fn sample_filter() -> FilterFn<impl Fn(&tracing::Metadata<'_>) -> bool> {
    FilterFn::new(|metadata| {
        if metadata.level() == &Level::DEBUG {
            // DEBUG 日志 1% 采样
            SAMPLE_COUNTER.fetch_add(1, Ordering::Relaxed) % 100 == 0
        } else {
            // 其他级别全部记录
            true
        }
    })
}

pub fn init_sampled_logging() {
    tracing_subscriber::registry()
        .with(LevelFilter::DEBUG)
        .with(sample_filter())
        .with(tracing_subscriber::fmt::layer().json())
        .init();
}
```

#### 4.4.2 成本优化策略

| 策略 | 成本降低 | 信息损失 | 适用场景 |
|------|---------|---------|---------|
| 采样 (1%-10%) | 90%-99% | 中等 | 高频 DEBUG 日志 |
| 限流 (100/s) | 高 | 低（溢出时） | 异常日志 |
| 聚合 (去重) | 中等 | 低 | 重复日志 |
| 压缩存储 | 50%-70% | 无 | 所有日志 |

---

## 5. 告警策略

### 5.1 基于阈值

#### 5.1.1 Prometheus Alerting Rules

```yaml
# alerts.yml
groups:
  - name: latency_alerts
    interval: 30s
    rules:
      - alert: HighP99Latency
        expr: histogram_quantile(0.99, rate(http_duration_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High P99 latency detected"
          description: "P99 latency is {{ $value }}s (threshold: 0.5s)"
      
      - alert: HighErrorRate
        expr: sum(rate(http_errors_total[5m])) / sum(rate(http_requests_total[5m])) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
```

---

### 5.2 基于趋势

#### 5.2.1 预测告警

```promql
# 预测未来 1 小时的 CPU 使用率
predict_linear(node_cpu_seconds_total[1h], 3600) > 0.9

# 预测磁盘在 4 小时内耗尽
predict_linear(node_filesystem_free_bytes[1h], 4 * 3600) < 0
```

---

### 5.3 异常检测

#### 5.3.1 使用统计方法

```promql
# Z-score 异常检测 (超过 3 个标准差)
abs(
  rate(http_requests_total[5m]) 
  - 
  avg_over_time(rate(http_requests_total[5m])[1h:5m])
) 
> 
3 * stddev_over_time(rate(http_requests_total[5m])[1h:5m])
```

---

### 5.4 告警降噪

#### 5.4.1 告警分组与抑制

```yaml
# alertmanager.yml
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'slack-default'
  
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true
    
    - match:
        severity: warning
      receiver: 'slack-warnings'

inhibit_rules:
  # 高优先级告警抑制低优先级
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
```

---

## 6. 可观测性即代码

### 6.1 Terraform 基础设施

```hcl
# prometheus.tf
resource "kubernetes_config_map" "prometheus_config" {
  metadata {
    name      = "prometheus-config"
    namespace = "monitoring"
  }

  data = {
    "prometheus.yml" = file("${path.module}/prometheus.yml")
    "alerts.yml"     = file("${path.module}/alerts.yml")
  }
}

resource "kubernetes_deployment" "prometheus" {
  metadata {
    name      = "prometheus"
    namespace = "monitoring"
  }

  spec {
    replicas = 2

    selector {
      match_labels = {
        app = "prometheus"
      }
    }

    template {
      metadata {
        labels = {
          app = "prometheus"
        }
      }

      spec {
        container {
          name  = "prometheus"
          image = "prom/prometheus:v2.47.0"
          
          volume_mount {
            name       = "config"
            mount_path = "/etc/prometheus"
          }
        }

        volume {
          name = "config"
          config_map {
            name = kubernetes_config_map.prometheus_config.metadata[0].name
          }
        }
      }
    }
  }
}
```

---

### 6.2 Grafana as Code

```hcl
# grafana_dashboard.tf
resource "grafana_dashboard" "red_metrics" {
  config_json = jsonencode({
    title = "RED Metrics Dashboard"
    panels = [
      {
        title = "Request Rate"
        targets = [{
          expr = "sum(rate(http_requests_total[5m])) by (endpoint)"
        }]
      }
    ]
  })
}
```

---

### 6.3 配置版本化

```bash
# Git 仓库结构
observability-config/
├── prometheus/
│   ├── prometheus.yml
│   └── alerts.yml
├── grafana/
│   ├── dashboards/
│   │   ├── red-metrics.json
│   │   └── use-metrics.json
│   └── datasources/
│       └── prometheus.yml
└── terraform/
    ├── main.tf
    └── variables.tf
```

---

## 7. 实战案例

### 7.1 微服务全链路追踪

**场景**：电商系统订单流程（5 个微服务）

```rust
// Service A: API Gateway
#[instrument]
async fn handle_order_request(order_id: u64) {
    info!("Received order request");
    
    // 调用 Service B: Order Service
    call_service("order-service", &format!("/orders/{}", order_id)).await;
}

// Service B: Order Service
#[instrument]
async fn process_order(order_id: u64) {
    // 并行调用 C 和 D
    tokio::join!(
        call_service("inventory-service", "/check"),
        call_service("payment-service", "/charge")
    );
    
    // 调用 Service E: Notification
    call_service("notification-service", "/send").await;
}
```

**Jaeger 分析结果**：

- 总时长: 450ms
- Service B 等待: 200ms (最慢瓶颈)
- 优化建议: 缓存库存查询结果

---

### 7.2 高基数指标优化

**问题**：user_id 标签导致 10M+ 时间序列

```promql
# ❌ 高基数：每个用户一个序列
http_requests_total{user_id="12345", endpoint="/api/users"}

# ✅ 优化：移除 user_id，使用直方图聚合
http_requests_total{endpoint="/api/users"}

# 需要用户级分析时，使用日志/追踪而非指标
```

---

### 7.3 日志成本控制

**优化前**：1TB/天，成本 $150/天

**优化措施**：

1. DEBUG 日志采样 1% → 节省 60%
2. 压缩存储（gzip）→ 节省 70%
3. 7天热数据 + 90天冷存储 → 节省 50%

**优化后**：120GB/天，成本 $18/天

---

## 8. 相关资源

### 8.1 工具与框架

- **OpenTelemetry**: 统一可观测性标准
- **Prometheus**: 指标采集与存储
- **Grafana**: 可视化平台
- **Jaeger**: 分布式追踪
- **Loki**: 轻量级日志聚合
- **Alertmanager**: 告警管理

### 8.2 学习资源

- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/)
- [OpenTelemetry Rust SDK](https://docs.rs/opentelemetry/)

---

**下一步**：

- → 参考 [C13 主索引](../../02_主索引导航.md) 探索其他主题
- → 实践 [Tier 2 监控指南](../../tier_02_guides/04_监控可观测性指南.md)
- → 查看 [Tier 1 核心概念](../../tier_01_core/README.md) 巩固基础

---

*最后更新: 2025-10-23*
*版本: 1.0.0*
*作者: Rust 学习系统团队*
