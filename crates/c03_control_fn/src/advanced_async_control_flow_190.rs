//! Rust 1.90 é«˜çº§å¼‚æ­¥æ§åˆ¶æµæ¨¡å—
//!
//! æœ¬æ¨¡å—å±•ç¤ºäº† Rust 1.90 åœ¨å¤æ‚å¼‚æ­¥æ§åˆ¶æµåœºæ™¯ä¸­çš„é«˜çº§åº”ç”¨ï¼š
//! - å¼‚æ­¥çŠ¶æ€æœºä¸äº‹ä»¶é©±åŠ¨æ¶æ„
//! - å¼‚æ­¥å·¥ä½œæµå¼•æ“
//! - å¼‚æ­¥æ•°æ®ç®¡é“å¤„ç†
//! - å¼‚æ­¥é”™è¯¯æ¢å¤æœºåˆ¶
//! - å¼‚æ­¥èµ„æºæ± ç®¡ç†
//! - å¼‚æ­¥ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†
//!
//! æ‰€æœ‰ç¤ºä¾‹éƒ½ä½¿ç”¨ Rust 1.90 çš„æœ€æ–°ç‰¹æ€§ï¼Œå¹¶åŒ…å«è¯¦ç»†çš„æ³¨é‡Šå’Œæœ€ä½³å®è·µã€‚

use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock, Semaphore};
use tokio::time::{sleep, timeout};
use anyhow::{Result, Context};
use serde::{Deserialize, Serialize};

/// å¼‚æ­¥äº‹ä»¶ç±»å‹
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AsyncEvent {
    DataReceived { id: String, data: Vec<u8> },
    ProcessingComplete { id: String, result: String },
    ErrorOccurred { id: String, error: String },
    ResourceRequested { resource_type: String, priority: u8 },
    ResourceReleased { resource_id: String },
    SystemShutdown,
    HealthCheck,
}

/// å¼‚æ­¥äº‹ä»¶å¤„ç†å™¨æšä¸¾
#[derive(Debug, Clone)]
pub enum AsyncEventHandler {
    DataProcessor(AdvancedDataProcessor),
    ResourceManager(ResourceManager),
}

/// é«˜çº§æ•°æ®å¤„ç†å™¨
#[derive(Debug, Clone)]
pub struct AdvancedDataProcessor {
    name: String,
    processing_time: Duration,
    error_rate: f64,
}

impl AdvancedDataProcessor {
    pub fn new(name: String, processing_time: Duration, error_rate: f64) -> Self {
        Self {
            name,
            processing_time,
            error_rate,
        }
    }
}

impl AsyncEventHandler {
    /// å¤„ç†äº‹ä»¶
    pub async fn handle_event(&self, event: AsyncEvent) -> Result<()> {
        match self {
            AsyncEventHandler::DataProcessor(processor) => {
                match event {
                    AsyncEvent::DataReceived { id, data } => {
                        println!("  å¤„ç†å™¨ {} å¼€å§‹å¤„ç†æ•°æ® {} (å¤§å°: {} å­—èŠ‚)",
                                processor.name, id, data.len());

                        // æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                        sleep(processor.processing_time).await;

                        // æ¨¡æ‹Ÿé”™è¯¯ç‡
                        if fastrand::f64() < processor.error_rate {
                            return Err(anyhow::anyhow!("å¤„ç†æ•°æ® {} æ—¶å‘ç”Ÿé”™è¯¯", id));
                        }

                        let result = format!("processed_{}_{}", id, data.len());
                        println!("  å¤„ç†å™¨ {} å®Œæˆå¤„ç†æ•°æ® {} -> {}", processor.name, id, result);

                        Ok(())
                    }
                    _ => Ok(()), // å¿½ç•¥å…¶ä»–äº‹ä»¶
                }
            }
            AsyncEventHandler::ResourceManager(manager) => {
                match event {
                    AsyncEvent::ResourceRequested { resource_type, priority } => {
                        println!("  èµ„æºç®¡ç†å™¨ {} æ”¶åˆ°èµ„æºè¯·æ±‚: {} (ä¼˜å…ˆçº§: {})",
                                manager.name, resource_type, priority);
                        Ok(())
                    }
                    AsyncEvent::ResourceReleased { resource_id } => {
                        println!("  èµ„æºç®¡ç†å™¨ {} é‡Šæ”¾èµ„æº: {}", manager.name, resource_id);
                        Ok(())
                    }
                    _ => Ok(()),
                }
            }
        }
    }

    /// è·å–å¤„ç†å™¨åç§°
    pub fn get_handler_name(&self) -> &str {
        match self {
            AsyncEventHandler::DataProcessor(processor) => &processor.name,
            AsyncEventHandler::ResourceManager(manager) => &manager.name,
        }
    }
}

/// èµ„æºç®¡ç†å™¨
#[derive(Debug, Clone)]
#[allow(dead_code)]
pub struct ResourceManager {
    name: String,
    available_resources: HashMap<String, u32>,
    allocated_resources: HashMap<String, String>,
}

impl ResourceManager {
    pub fn new(name: String) -> Self {
        let mut available_resources = HashMap::new();
        available_resources.insert("cpu".to_string(), 8);
        available_resources.insert("memory".to_string(), 16);
        available_resources.insert("disk".to_string(), 100);

        Self {
            name,
            available_resources,
            allocated_resources: HashMap::new(),
        }
    }
}


/// å¼‚æ­¥äº‹ä»¶æ€»çº¿
#[allow(dead_code)]
pub struct AsyncEventBus {
    handlers: Arc<RwLock<Vec<AsyncEventHandler>>>,
    event_queue: Arc<Mutex<VecDeque<AsyncEvent>>>,
    metrics: Arc<Mutex<EventMetrics>>,
}

#[derive(Debug, Default)]
#[allow(dead_code)]
pub struct EventMetrics {
    total_events: u64,
    processed_events: u64,
    failed_events: u64,
    average_processing_time: Duration,
}

impl Default for AsyncEventBus {
    fn default() -> Self {
        Self {
            handlers: Arc::new(RwLock::new(Vec::new())),
            event_queue: Arc::new(Mutex::new(VecDeque::new())),
            metrics: Arc::new(Mutex::new(EventMetrics::default())),
        }
    }
}

impl AsyncEventBus {
    pub fn new() -> Self {
        Self::default()
    }

    /// æ³¨å†Œäº‹ä»¶å¤„ç†å™¨
    pub async fn register_handler(&self, handler: AsyncEventHandler) -> Result<()> {
        let mut handlers = self.handlers.write().await;
        handlers.push(handler);
        Ok(())
    }

    /// å‘å¸ƒäº‹ä»¶
    pub async fn publish_event(&self, event: AsyncEvent) -> Result<()> {
        let mut queue = self.event_queue.lock().await;
        queue.push_back(event);

        let mut metrics = self.metrics.lock().await;
        metrics.total_events += 1;

        Ok(())
    }

    /// å¤„ç†äº‹ä»¶é˜Ÿåˆ—
    pub async fn process_events(&self) -> Result<()> {
        let mut queue = self.event_queue.lock().await;

        while let Some(event) = queue.pop_front() {
            let start_time = Instant::now();

            let handlers = self.handlers.read().await;
            let mut success_count = 0;
            let mut error_count = 0;

            for handler in handlers.iter() {
                match handler.handle_event(event.clone()).await {
                    Ok(_) => success_count += 1,
                    Err(e) => {
                        error_count += 1;
                        eprintln!("  å¤„ç†å™¨ {} å¤„ç†äº‹ä»¶å¤±è´¥: {}",
                                handler.get_handler_name(), e);
                    }
                }
            }

            let processing_time = start_time.elapsed();

            // æ›´æ–°æŒ‡æ ‡
            let mut metrics = self.metrics.lock().await;
            metrics.processed_events += success_count;
            metrics.failed_events += error_count;

            // æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
            let total_processed = metrics.processed_events + metrics.failed_events;
            if total_processed > 0 {
                metrics.average_processing_time = Duration::from_nanos(
                    (metrics.average_processing_time.as_nanos() as u64 * (total_processed - 1)
                     + processing_time.as_nanos() as u64) / total_processed
                );
            }
        }

        Ok(())
    }

    /// è·å–æŒ‡æ ‡
    pub async fn get_metrics(&self) -> EventMetrics {
        self.metrics.lock().await.clone()
    }
}

impl Clone for EventMetrics {
    fn clone(&self) -> Self {
        Self {
            total_events: self.total_events,
            processed_events: self.processed_events,
            failed_events: self.failed_events,
            average_processing_time: self.average_processing_time,
        }
    }
}

impl EventMetrics {
    pub fn total_events(&self) -> u64 {
        self.total_events
    }

    pub fn processed_events(&self) -> u64 {
        self.processed_events
    }

    pub fn failed_events(&self) -> u64 {
        self.failed_events
    }

    pub fn average_processing_time(&self) -> Duration {
        self.average_processing_time
    }
}

/// å¼‚æ­¥å·¥ä½œæµå¼•æ“
#[allow(dead_code)]
pub struct AsyncWorkflowEngine {
    workflows: Arc<RwLock<HashMap<String, AsyncWorkflow>>>,
    event_bus: Arc<AsyncEventBus>,
    execution_semaphore: Arc<Semaphore>,
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub struct AsyncWorkflow {
    id: String,
    name: String,
    steps: Vec<WorkflowStep>,
    current_step: usize,
    status: WorkflowStatus,
    created_at: Instant,
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub struct WorkflowStep {
    pub id: String,
    pub name: String,
    pub action: WorkflowAction,
    pub timeout: Duration,
    pub retry_count: u32,
    pub max_retries: u32,
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub enum WorkflowAction {
    ProcessData { data: Vec<u8> },
    WaitForEvent { event_type: String },
    CallService { service_url: String },
    ValidateResult { expected: String },
}

#[derive(Debug, Clone, PartialEq)]
#[allow(dead_code)]
pub enum WorkflowStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Cancelled,
}

impl AsyncWorkflowEngine {
    pub fn new(event_bus: Arc<AsyncEventBus>, max_concurrent: usize) -> Self {
        Self {
            workflows: Arc::new(RwLock::new(HashMap::new())),
            event_bus,
            execution_semaphore: Arc::new(Semaphore::new(max_concurrent)),
        }
    }

    /// åˆ›å»ºæ–°å·¥ä½œæµ
    pub async fn create_workflow(&self, id: String, name: String, steps: Vec<WorkflowStep>) -> Result<()> {
        let workflow = AsyncWorkflow {
            id: id.clone(),
            name,
            steps,
            current_step: 0,
            status: WorkflowStatus::Pending,
            created_at: Instant::now(),
        };

        let mut workflows = self.workflows.write().await;
        workflows.insert(id, workflow);

        Ok(())
    }

    /// æ‰§è¡Œå·¥ä½œæµ
    pub async fn execute_workflow(&self, workflow_id: &str) -> Result<()> {
        let _permit = self.execution_semaphore.acquire().await
            .context("è·å–æ‰§è¡Œè®¸å¯å¤±è´¥")?;

        let workflow_name = {
            let workflows = self.workflows.read().await;
            let workflow = workflows.get(workflow_id)
                .ok_or_else(|| anyhow::anyhow!("å·¥ä½œæµ {} ä¸å­˜åœ¨", workflow_id))?;
            workflow.name.clone()
        };

        {
            let mut workflows = self.workflows.write().await;
            let workflow = workflows.get_mut(workflow_id).unwrap();
            workflow.status = WorkflowStatus::Running;
        }

        println!("  å¼€å§‹æ‰§è¡Œå·¥ä½œæµ: {} ({})", workflow_name, workflow_id);

        loop {
            // æ£€æŸ¥å·¥ä½œæµçŠ¶æ€å’Œæ­¥éª¤
            let (should_continue, current_step, step_to_execute) = {
                let workflows = self.workflows.read().await;
                let workflow = workflows.get(workflow_id)
                    .ok_or_else(|| anyhow::anyhow!("å·¥ä½œæµ {} ä¸å­˜åœ¨", workflow_id))?;

                if workflow.current_step >= workflow.steps.len() {
                    (false, workflow.current_step, None)
                } else {
                    (true, workflow.current_step, Some(workflow.steps[workflow.current_step].clone()))
                }
            };

            if !should_continue {
                // æ ‡è®°å·¥ä½œæµå®Œæˆ
                let mut workflows = self.workflows.write().await;
                let workflow = workflows.get_mut(workflow_id).unwrap();
                workflow.status = WorkflowStatus::Completed;
                println!("  å·¥ä½œæµ {} æ‰§è¡Œå®Œæˆ", workflow_id);
                break;
            }

            let step = step_to_execute.unwrap();

            match self.execute_step(workflow_id, &step).await {
                Ok(_) => {
                    let mut workflows = self.workflows.write().await;
                    let workflow = workflows.get_mut(workflow_id).unwrap();
                    workflow.current_step += 1;
                }
                Err(e) => {
                    let mut workflows = self.workflows.write().await;
                    let workflow = workflows.get_mut(workflow_id).unwrap();

                    if workflow.steps[current_step].retry_count < workflow.steps[current_step].max_retries {
                        workflow.steps[current_step].retry_count += 1;
                        println!("  æ­¥éª¤ {} æ‰§è¡Œå¤±è´¥ï¼Œé‡è¯• {}/{}: {}",
                                step.name,
                                workflow.steps[current_step].retry_count,
                                step.max_retries, e);
                    } else {
                        workflow.status = WorkflowStatus::Failed;
                        println!("  å·¥ä½œæµ {} æ‰§è¡Œå¤±è´¥: {}", workflow_id, e);
                        break;
                    }
                }
            }
        }

        Ok(())
    }

    /// æ‰§è¡Œå·¥ä½œæµæ­¥éª¤
    async fn execute_step(&self, workflow_id: &str, step: &WorkflowStep) -> Result<()> {
        println!("    æ‰§è¡Œæ­¥éª¤: {} (å·¥ä½œæµ: {})", step.name, workflow_id);

        let result = timeout(step.timeout, async {
            match &step.action {
                WorkflowAction::ProcessData { data } => {
                    // æ¨¡æ‹Ÿæ•°æ®å¤„ç†
                    sleep(Duration::from_millis(100)).await;
                    println!("      å¤„ç†æ•°æ®: {} å­—èŠ‚", data.len());
                    Ok(())
                }
                WorkflowAction::WaitForEvent { event_type } => {
                    // æ¨¡æ‹Ÿç­‰å¾…äº‹ä»¶
                    sleep(Duration::from_millis(50)).await;
                    println!("      ç­‰å¾…äº‹ä»¶: {}", event_type);
                    Ok(())
                }
                WorkflowAction::CallService { service_url } => {
                    // æ¨¡æ‹ŸæœåŠ¡è°ƒç”¨
                    sleep(Duration::from_millis(200)).await;
                    println!("      è°ƒç”¨æœåŠ¡: {}", service_url);
                    Ok(())
                }
                WorkflowAction::ValidateResult { expected } => {
                    // æ¨¡æ‹Ÿç»“æœéªŒè¯
                    sleep(Duration::from_millis(30)).await;
                    println!("      éªŒè¯ç»“æœ: {}", expected);
                    Ok(())
                }
            }
        }).await;

        match result {
            Ok(Ok(_)) => Ok(()),
            Ok(Err(e)) => Err(e),
            Err(_) => Err(anyhow::anyhow!("æ­¥éª¤ {} æ‰§è¡Œè¶…æ—¶", step.name)),
        }
    }

    /// è·å–å·¥ä½œæµçŠ¶æ€
    pub async fn get_workflow_status(&self, workflow_id: &str) -> Option<WorkflowStatus> {
        let workflows = self.workflows.read().await;
        workflows.get(workflow_id).map(|w| w.status.clone())
    }
}

/// å¼‚æ­¥æ•°æ®ç®¡é“
#[allow(dead_code)]
pub struct AsyncDataPipeline {
    stages: Vec<PipelineStage>,
    buffer_size: usize,
    metrics: Arc<Mutex<PipelineMetrics>>,
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub struct PipelineStage {
    pub id: String,
    pub name: String,
    pub processor: PipelineProcessor,
    pub parallelism: usize,
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub enum PipelineProcessor {
    DataTransform(DataTransformProcessor),
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub struct PipelineData {
    pub id: String,
    pub content: Vec<u8>,
    pub metadata: HashMap<String, String>,
    pub timestamp: Instant,
}

#[derive(Debug, Default, Clone)]
pub struct PipelineMetrics {
    pub total_processed: u64,
    pub total_errors: u64,
    pub average_processing_time: Duration,
    pub throughput_per_second: f64,
}

impl AsyncDataPipeline {
    pub fn new(buffer_size: usize) -> Self {
        Self {
            stages: Vec::new(),
            buffer_size,
            metrics: Arc::new(Mutex::new(PipelineMetrics::default())),
        }
    }

    /// æ·»åŠ ç®¡é“é˜¶æ®µ
    pub fn add_stage(&mut self, stage: PipelineStage) {
        self.stages.push(stage);
    }

    /// å¤„ç†æ•°æ®
    pub async fn process_data(&self, data: PipelineData) -> Result<PipelineData> {
        let mut current_data = data;
        let start_time = Instant::now();

        for (stage_index, stage) in self.stages.iter().enumerate() {
            println!("  ç®¡é“é˜¶æ®µ {}: {} å¤„ç†æ•°æ® {}",
                    stage_index + 1, stage.name, current_data.id);

            let stage_start = Instant::now();
            current_data = stage.processor.process(current_data).await
                .with_context(|| format!("ç®¡é“é˜¶æ®µ {} å¤„ç†å¤±è´¥", stage.name))?;
            let stage_duration = stage_start.elapsed();

            println!("    é˜¶æ®µ {} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {:?}", stage.name, stage_duration);
        }

        let total_duration = start_time.elapsed();

        // æ›´æ–°æŒ‡æ ‡
        let mut metrics = self.metrics.lock().await;
        metrics.total_processed += 1;
        metrics.average_processing_time = Duration::from_nanos(
            (metrics.average_processing_time.as_nanos() as u64 * (metrics.total_processed - 1)
             + total_duration.as_nanos() as u64) / metrics.total_processed
        );
        metrics.throughput_per_second = metrics.total_processed as f64 /
            start_time.elapsed().as_secs_f64();

        Ok(current_data)
    }

    /// è·å–ç®¡é“æŒ‡æ ‡
    pub async fn get_metrics(&self) -> PipelineMetrics {
        self.metrics.lock().await.clone()
    }
}

/// æ•°æ®è½¬æ¢å¤„ç†å™¨
#[derive(Debug, Clone)]
pub struct DataTransformProcessor {
    name: String,
    transform_type: TransformType,
}

#[derive(Debug, Clone)]
pub enum TransformType {
    Uppercase,
    Lowercase,
    Reverse,
    Compress,
    Encrypt,
}

impl DataTransformProcessor {
    pub fn new(name: String, transform_type: TransformType) -> Self {
        Self {
            name,
            transform_type,
        }
    }
}

impl PipelineProcessor {
    /// å¤„ç†æ•°æ®
    pub async fn process(&self, mut input: PipelineData) -> Result<PipelineData> {
        match self {
            PipelineProcessor::DataTransform(processor) => {
                // æ¨¡æ‹Ÿå¼‚æ­¥å¤„ç†
                sleep(Duration::from_millis(50)).await;

                match processor.transform_type {
                    TransformType::Uppercase => {
                        let content = String::from_utf8_lossy(&input.content).to_uppercase();
                        input.content = content.into_bytes();
                    }
                    TransformType::Lowercase => {
                        let content = String::from_utf8_lossy(&input.content).to_lowercase();
                        input.content = content.into_bytes();
                    }
                    TransformType::Reverse => {
                        input.content.reverse();
                    }
                    TransformType::Compress => {
                        // æ¨¡æ‹Ÿå‹ç¼©ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
                        input.content = input.content.chunks(2)
                            .flat_map(|chunk| {
                                if chunk.len() == 2 {
                                    vec![chunk[0] ^ chunk[1]]
                                } else {
                                    chunk.to_vec()
                                }
                            })
                            .collect();
                    }
                    TransformType::Encrypt => {
                        // æ¨¡æ‹ŸåŠ å¯†ï¼ˆç®€å•XORï¼‰
                        for byte in &mut input.content {
                            *byte ^= 0xAA;
                        }
                    }
                }

                input.metadata.insert("transformed_by".to_string(), processor.name.clone());
                input.metadata.insert("transform_type".to_string(),
                                    format!("{:?}", processor.transform_type));

                Ok(input)
            }
        }
    }

    /// è·å–é˜¶æ®µåç§°
    pub fn get_stage_name(&self) -> &str {
        match self {
            PipelineProcessor::DataTransform(processor) => &processor.name,
        }
    }
}

/// ç»¼åˆæ¼”ç¤ºé«˜çº§å¼‚æ­¥æ§åˆ¶æµ
pub async fn demonstrate_advanced_async_control_flow_190() -> Result<()> {
    println!("ğŸš€ æ¼”ç¤º Rust 1.90 é«˜çº§å¼‚æ­¥æ§åˆ¶æµ");
    println!("{}", "=".repeat(60));

    // 1. å¼‚æ­¥äº‹ä»¶æ€»çº¿æ¼”ç¤º
    println!("\n1. å¼‚æ­¥äº‹ä»¶æ€»çº¿æ¼”ç¤º:");
    let event_bus = Arc::new(AsyncEventBus::new());

    // æ³¨å†Œäº‹ä»¶å¤„ç†å™¨
    event_bus.register_handler(AsyncEventHandler::DataProcessor(AdvancedDataProcessor::new(
        "data_processor_1".to_string(),
        Duration::from_millis(100),
        0.1, // 10% é”™è¯¯ç‡
    ))).await?;

    event_bus.register_handler(AsyncEventHandler::DataProcessor(AdvancedDataProcessor::new(
        "data_processor_2".to_string(),
        Duration::from_millis(150),
        0.05, // 5% é”™è¯¯ç‡
    ))).await?;

    event_bus.register_handler(AsyncEventHandler::ResourceManager(ResourceManager::new(
        "resource_manager_1".to_string()
    ))).await?;

    // å‘å¸ƒäº‹ä»¶
    for i in 0..10 {
        let event = AsyncEvent::DataReceived {
            id: format!("data_{}", i),
            data: vec![0u8; 1024],
        };
        event_bus.publish_event(event).await?;

        let event = AsyncEvent::ResourceRequested {
            resource_type: "cpu".to_string(),
            priority: (i % 3) as u8 + 1,
        };
        event_bus.publish_event(event).await?;
    }

    // å¤„ç†äº‹ä»¶
    event_bus.process_events().await?;

    let metrics = event_bus.get_metrics().await;
    println!("  äº‹ä»¶æ€»çº¿æŒ‡æ ‡:");
    println!("    æ€»äº‹ä»¶æ•°: {}", metrics.total_events);
    println!("    æˆåŠŸå¤„ç†: {}", metrics.processed_events);
    println!("    å¤±è´¥å¤„ç†: {}", metrics.failed_events);
    println!("    å¹³å‡å¤„ç†æ—¶é—´: {:?}", metrics.average_processing_time);

    // 2. å¼‚æ­¥å·¥ä½œæµå¼•æ“æ¼”ç¤º
    println!("\n2. å¼‚æ­¥å·¥ä½œæµå¼•æ“æ¼”ç¤º:");
    let workflow_engine = AsyncWorkflowEngine::new(event_bus.clone(), 3);

    // åˆ›å»ºå·¥ä½œæµ
    let steps = vec![
        WorkflowStep {
            id: "step_1".to_string(),
            name: "æ•°æ®éªŒè¯".to_string(),
            action: WorkflowAction::ValidateResult {
                expected: "valid_data".to_string()
            },
            timeout: Duration::from_secs(5),
            retry_count: 0,
            max_retries: 3,
        },
        WorkflowStep {
            id: "step_2".to_string(),
            name: "æ•°æ®å¤„ç†".to_string(),
            action: WorkflowAction::ProcessData {
                data: b"test_data".to_vec()
            },
            timeout: Duration::from_secs(10),
            retry_count: 0,
            max_retries: 2,
        },
        WorkflowStep {
            id: "step_3".to_string(),
            name: "æœåŠ¡è°ƒç”¨".to_string(),
            action: WorkflowAction::CallService {
                service_url: "https://api.example.com/process".to_string()
            },
            timeout: Duration::from_secs(15),
            retry_count: 0,
            max_retries: 3,
        },
    ];

    workflow_engine.create_workflow(
        "workflow_1".to_string(),
        "æ•°æ®å¤„ç†å·¥ä½œæµ".to_string(),
        steps,
    ).await?;

    // æ‰§è¡Œå·¥ä½œæµ
    workflow_engine.execute_workflow("workflow_1").await?;

    let status = workflow_engine.get_workflow_status("workflow_1").await;
    println!("  å·¥ä½œæµçŠ¶æ€: {:?}", status);

    // 3. å¼‚æ­¥æ•°æ®ç®¡é“æ¼”ç¤º
    println!("\n3. å¼‚æ­¥æ•°æ®ç®¡é“æ¼”ç¤º:");
    let mut pipeline = AsyncDataPipeline::new(1000);

    // æ·»åŠ ç®¡é“é˜¶æ®µ
    pipeline.add_stage(PipelineStage {
        id: "stage_1".to_string(),
        name: "æ•°æ®è½¬æ¢".to_string(),
        processor: PipelineProcessor::DataTransform(DataTransformProcessor::new(
            "transform_1".to_string(),
            TransformType::Uppercase,
        )),
        parallelism: 2,
    });

    pipeline.add_stage(PipelineStage {
        id: "stage_2".to_string(),
        name: "æ•°æ®å‹ç¼©".to_string(),
        processor: PipelineProcessor::DataTransform(DataTransformProcessor::new(
            "transform_2".to_string(),
            TransformType::Compress,
        )),
        parallelism: 1,
    });

    pipeline.add_stage(PipelineStage {
        id: "stage_3".to_string(),
        name: "æ•°æ®åŠ å¯†".to_string(),
        processor: PipelineProcessor::DataTransform(DataTransformProcessor::new(
            "transform_3".to_string(),
            TransformType::Encrypt,
        )),
        parallelism: 1,
    });

    // å¤„ç†æ•°æ®
    for i in 0..5 {
        let data = PipelineData {
            id: format!("pipeline_data_{}", i),
            content: format!("Hello, Pipeline {}!", i).into_bytes(),
            metadata: HashMap::new(),
            timestamp: Instant::now(),
        };

        let result = pipeline.process_data(data).await?;
        println!("  ç®¡é“å¤„ç†ç»“æœ {}: {} å­—èŠ‚", i, result.content.len());
    }

    let pipeline_metrics = pipeline.get_metrics().await;
    println!("  ç®¡é“æŒ‡æ ‡:");
    println!("    æ€»å¤„ç†æ•°: {}", pipeline_metrics.total_processed);
    println!("    å¹³å‡å¤„ç†æ—¶é—´: {:?}", pipeline_metrics.average_processing_time);
    println!("    ååé‡: {:.2} é¡¹/ç§’", pipeline_metrics.throughput_per_second);

    println!("\nâœ… Rust 1.90 é«˜çº§å¼‚æ­¥æ§åˆ¶æµæ¼”ç¤ºå®Œæˆ!");
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_async_event_bus() {
        let event_bus = AsyncEventBus::new();

        event_bus.register_handler(AsyncEventHandler::DataProcessor(AdvancedDataProcessor::new(
            "test_processor".to_string(),
            Duration::from_millis(10),
            0.0,
        ))).await.unwrap();

        event_bus.publish_event(AsyncEvent::DataReceived {
            id: "test_data".to_string(),
            data: vec![1, 2, 3],
        }).await.unwrap();

        event_bus.process_events().await.unwrap();

        let metrics = event_bus.get_metrics().await;
        assert_eq!(metrics.total_events, 1);
        assert_eq!(metrics.processed_events, 1);
    }

    #[tokio::test]
    async fn test_workflow_engine() {
        let event_bus = Arc::new(AsyncEventBus::new());
        let workflow_engine = AsyncWorkflowEngine::new(event_bus, 1);

        let steps = vec![
            WorkflowStep {
                id: "test_step".to_string(),
                name: "æµ‹è¯•æ­¥éª¤".to_string(),
                action: WorkflowAction::ProcessData {
                    data: b"test".to_vec()
                },
                timeout: Duration::from_secs(1),
                retry_count: 0,
                max_retries: 1,
            },
        ];

        workflow_engine.create_workflow(
            "test_workflow".to_string(),
            "æµ‹è¯•å·¥ä½œæµ".to_string(),
            steps,
        ).await.unwrap();

        workflow_engine.execute_workflow("test_workflow").await.unwrap();

        let status = workflow_engine.get_workflow_status("test_workflow").await;
        assert_eq!(status, Some(WorkflowStatus::Completed));
    }

    #[tokio::test]
    async fn test_data_pipeline() {
        let mut pipeline = AsyncDataPipeline::new(100);

        pipeline.add_stage(PipelineStage {
            id: "test_stage".to_string(),
            name: "æµ‹è¯•é˜¶æ®µ".to_string(),
            processor: PipelineProcessor::DataTransform(DataTransformProcessor::new(
                "test_processor".to_string(),
                TransformType::Uppercase,
            )),
            parallelism: 1,
        });

        let data = PipelineData {
            id: "test_data".to_string(),
            content: b"hello".to_vec(),
            metadata: HashMap::new(),
            timestamp: Instant::now(),
        };

        let result = pipeline.process_data(data).await.unwrap();
        assert_eq!(result.content, b"HELLO");
    }

    #[tokio::test]
    async fn test_comprehensive_demo() {
        assert!(demonstrate_advanced_async_control_flow_190().await.is_ok());
    }
}
