# Machine Learning - æœºå™¨å­¦ä¹ 

## ğŸ“‹ ç›®å½•

- [Machine Learning - æœºå™¨å­¦ä¹ ](#machine-learning---æœºå™¨å­¦ä¹ )
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
    - [æ ¸å¿ƒä¾èµ–](#æ ¸å¿ƒä¾èµ–)
  - [æ•°å€¼è®¡ç®—](#æ•°å€¼è®¡ç®—)
    - [ndarray](#ndarray)
    - [çº¿æ€§ä»£æ•°](#çº¿æ€§ä»£æ•°)
  - [æ·±åº¦å­¦ä¹ ](#æ·±åº¦å­¦ä¹ )
    - [PyTorch (tch-rs)](#pytorch-tch-rs)
    - [Burn æ¡†æ¶](#burn-æ¡†æ¶)
  - [æ•°æ®å¤„ç†](#æ•°æ®å¤„ç†)
    - [Polars](#polars)
  - [å®æˆ˜ç¤ºä¾‹](#å®æˆ˜ç¤ºä¾‹)
    - [çº¿æ€§å›å½’](#çº¿æ€§å›å½’)
    - [K-means èšç±»](#k-means-èšç±»)
  - [å‚è€ƒèµ„æº](#å‚è€ƒèµ„æº)

---

## æ¦‚è¿°

Rust åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸé€æ¸å´›èµ·ï¼Œæä¾›é«˜æ€§èƒ½çš„æ•°å€¼è®¡ç®—å’Œæ•°æ®å¤„ç†èƒ½åŠ›ã€‚

### æ ¸å¿ƒä¾èµ–

```toml
[dependencies]
# æ•°å€¼è®¡ç®—
ndarray = "0.15"
nalgebra = "0.32"

# æ·±åº¦å­¦ä¹ 
tch = "0.15"  # PyTorch bindings
burn = "0.11"  # çº¯ Rust ML æ¡†æ¶

# æ•°æ®å¤„ç†
polars = "0.36"
linfa = "0.7"  # ML ç®—æ³•
```

---

## æ•°å€¼è®¡ç®—

### ndarray

```rust
use ndarray::{Array1, Array2, arr2};

fn ndarray_basics() {
    // 1D æ•°ç»„
    let a = Array1::from_vec(vec![1.0, 2.0, 3.0, 4.0, 5.0]);
    println!("æ•°ç»„: {:?}", a);
    
    // 2D æ•°ç»„
    let b = arr2(&[[1.0, 2.0],
                   [3.0, 4.0],
                   [5.0, 6.0]]);
    println!("çŸ©é˜µ:\n{:?}", b);
    
    // çŸ©é˜µè¿ç®—
    let c = arr2(&[[1.0, 2.0],
                   [3.0, 4.0]]);
    let d = arr2(&[[5.0, 6.0],
                   [7.0, 8.0]]);
    let result = c + d;
    println!("ç›¸åŠ :\n{:?}", result);
}
```

### çº¿æ€§ä»£æ•°

```rust
use nalgebra::{Matrix2, Vector2};

fn linear_algebra() {
    // çŸ©é˜µ
    let m = Matrix2::new(1.0, 2.0,
                         3.0, 4.0);
    
    // å‘é‡
    let v = Vector2::new(1.0, 2.0);
    
    // çŸ©é˜µ-å‘é‡ä¹˜æ³•
    let result = m * v;
    println!("ç»“æœ: {:?}", result);
    
    // çŸ©é˜µæ±‚é€†
    if let Some(inv) = m.try_inverse() {
        println!("é€†çŸ©é˜µ:\n{}", inv);
    }
}
```

---

## æ·±åº¦å­¦ä¹ 

### PyTorch (tch-rs)

```rust
use tch::{nn, nn::Module, nn::OptimizerConfig, Device, Tensor};

fn neural_network() {
    let vs = nn::VarStore::new(Device::Cpu);
    let net = nn::seq()
        .add(nn::linear(&vs.root(), 784, 128, Default::default()))
        .add_fn(|x| x.relu())
        .add(nn::linear(&vs.root(), 128, 10, Default::default()));
    
    let mut opt = nn::Adam::default().build(&vs, 1e-3).unwrap();
    
    for epoch in 1..=10 {
        let input = Tensor::randn(&[64, 784], (tch::Kind::Float, Device::Cpu));
        let target = Tensor::randint(10, &[64], (tch::Kind::Int64, Device::Cpu));
        
        let output = net.forward(&input);
        let loss = output.cross_entropy_for_logits(&target);
        
        opt.backward_step(&loss);
        
        println!("Epoch {}: Loss = {:?}", epoch, loss.double_value(&[]));
    }
}
```

### Burn æ¡†æ¶

```rust
use burn::prelude::*;

#[derive(Module, Debug)]
struct Model<B: Backend> {
    linear1: Linear<B>,
    linear2: Linear<B>,
}

impl<B: Backend> Model<B> {
    fn forward(&self, x: Tensor<B, 2>) -> Tensor<B, 2> {
        let x = self.linear1.forward(x);
        let x = x.relu();
        self.linear2.forward(x)
    }
}

fn burn_example() {
    type MyBackend = burn::backend::NdArray;
    let device = Default::default();
    
    let model: Model<MyBackend> = Model {
        linear1: LinearConfig::new(784, 128).init(&device),
        linear2: LinearConfig::new(128, 10).init(&device),
    };
    
    let input = Tensor::<MyBackend, 2>::random(
        [32, 784],
        Distribution::Default,
        &device,
    );
    
    let output = model.forward(input);
    println!("è¾“å‡ºå½¢çŠ¶: {:?}", output.shape());
}
```

---

## æ•°æ®å¤„ç†

### Polars

```rust
use polars::prelude::*;

fn data_processing() -> Result<(), PolarsError> {
    // åˆ›å»º DataFrame
    let df = df! {
        "A" => &[1, 2, 3, 4, 5],
        "B" => &[5, 4, 3, 2, 1],
        "C" => &["a", "b", "c", "d", "e"]
    }?;
    
    println!("{:?}", df);
    
    // è¿‡æ»¤
    let filtered = df.clone().lazy()
        .filter(col("A").gt(lit(2)))
        .collect()?;
    
    println!("è¿‡æ»¤å:\n{:?}", filtered);
    
    // èšåˆ
    let grouped = df.clone().lazy()
        .group_by([col("C")])
        .agg([col("A").sum()])
        .collect()?;
    
    println!("èšåˆå:\n{:?}", grouped);
    
    Ok(())
}
```

---

## å®æˆ˜ç¤ºä¾‹

### çº¿æ€§å›å½’

```rust
use ndarray::{Array1, Array2};
use ndarray_linalg::Solve;

fn linear_regression(x: &Array2<f64>, y: &Array1<f64>) -> Array1<f64> {
    // y = XÎ²
    // Î² = (X^T X)^(-1) X^T y
    
    let xt = x.t();
    let xtx = xt.dot(x);
    let xty = xt.dot(y);
    
    xtx.solve_into(xty).unwrap()
}

fn main() {
    // ç¤ºä¾‹æ•°æ®
    let x = Array2::from_shape_vec(
        (5, 2),
        vec![1.0, 1.0, 1.0, 2.0, 1.0, 3.0, 1.0, 4.0, 1.0, 5.0]
    ).unwrap();
    
    let y = Array1::from_vec(vec![2.0, 4.0, 5.0, 4.0, 5.0]);
    
    let beta = linear_regression(&x, &y);
    println!("ç³»æ•°: {:?}", beta);
}
```

### K-means èšç±»

```rust
use linfa::prelude::*;
use linfa_clustering::KMeans;
use ndarray::{Array2, Axis};

fn kmeans_example() {
    // ç”Ÿæˆæ•°æ®
    let mut data = Array2::zeros((100, 2));
    for i in 0..50 {
        data[[i, 0]] = rand::random::<f64>();
        data[[i, 1]] = rand::random::<f64>();
    }
    for i in 50..100 {
        data[[i, 0]] = rand::random::<f64>() + 5.0;
        data[[i, 1]] = rand::random::<f64>() + 5.0;
    }
    
    // K-means
    let dataset = DatasetBase::from(data);
    let model = KMeans::params(2)
        .max_n_iterations(100)
        .fit(&dataset)
        .expect("K-means å¤±è´¥");
    
    let clusters = model.predict(&dataset);
    println!("èšç±»: {:?}", clusters);
}
```

---

## å‚è€ƒèµ„æº

- [ndarray æ–‡æ¡£](https://docs.rs/ndarray/)
- [tch-rs GitHub](https://github.com/LaurentMazare/tch-rs)
- [Burn GitHub](https://github.com/tracel-ai/burn)
- [Polars æ–‡æ¡£](https://docs.rs/polars/)
- [linfa GitHub](https://github.com/rust-ml/linfa)
