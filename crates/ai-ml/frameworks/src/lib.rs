//! AI/MLæ¡†æ¶é›†æˆä¸æ€§èƒ½ä¼˜åŒ–
//! 
//! æœ¬æ¨¡å—æä¾›äº† Candleã€Tchã€ONNX Runtime ç­‰AI/MLæ¡†æ¶çš„
//! é›†æˆã€æ€§èƒ½å¯¹æ¯”å’Œä¼˜åŒ–ç­–ç•¥ã€‚

pub mod candle_integration;
pub mod tch_bindings;
pub mod ort_optimization;
pub mod performance;
pub mod benchmarks;

use std::time::Duration;

/// AI/MLæ¡†æ¶ç±»å‹
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum MLFramework {
    Candle,
    // Tch,  // æš‚æ—¶æ³¨é‡Šï¼Œéœ€è¦PyTorchåº“
    // ONNXRuntime,  // æš‚æ—¶æ³¨é‡Šï¼Œç‰ˆæœ¬é—®é¢˜
}

/// æ¨¡å‹ç±»å‹
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum ModelType {
    Transformer,
    CNN,
    RNN,
    LSTM,
    GRU,
    Linear,
    Custom,
}

/// ç¡¬ä»¶åŠ é€Ÿç±»å‹
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum HardwareAcceleration {
    CPU,
    CUDA,
    Metal,
    OpenCL,
    WebGPU,
}

/// æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
#[derive(Debug, Clone)]
pub struct ModelPerformance {
    pub framework: MLFramework,
    pub model_type: ModelType,
    pub hardware: HardwareAcceleration,
    pub inference_time: Duration,
    pub throughput: f64, // æ ·æœ¬/ç§’
    pub memory_usage: u64,
    pub gpu_memory_usage: Option<u64>,
    pub accuracy: Option<f64>,
    pub model_size: u64,
}

/// AI/MLæ¡†æ¶åˆ†æå™¨
pub struct MLFrameworkAnalyzer {
    performance_data: Vec<ModelPerformance>,
}

impl MLFrameworkAnalyzer {
    /// åˆ›å»ºæ–°çš„åˆ†æå™¨
    pub fn new() -> Self {
        Self {
            performance_data: Vec::new(),
        }
    }
    
    /// æ·»åŠ æ€§èƒ½æ•°æ®
    pub fn add_performance(&mut self, performance: ModelPerformance) {
        self.performance_data.push(performance);
    }
    
    /// ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
    pub fn generate_performance_report(&self) -> String {
        let mut report = String::new();
        report.push_str("# AI/MLæ¡†æ¶æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š\n\n");
        
        // æŒ‰æ¨¡å‹ç±»å‹åˆ†ç»„
        let mut grouped: std::collections::HashMap<ModelType, Vec<&ModelPerformance>> = std::collections::HashMap::new();
        for perf in &self.performance_data {
            grouped.entry(perf.model_type).or_default().push(perf);
        }
        
        for (model_type, performances) in grouped {
            report.push_str(&format!("## {:?} æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n\n", model_type));
            
            // æ‰¾å‡ºæœ€ä½³æ€§èƒ½
            if let Some(best) = performances.iter().max_by(|a, b| a.throughput.partial_cmp(&b.throughput).unwrap()) {
                report.push_str(&format!("**æœ€é«˜ååé‡**: {:?} + {:?}\n", best.framework, best.hardware));
                report.push_str(&format!("- æ¨ç†æ—¶é—´: {:?}\n", best.inference_time));
                report.push_str(&format!("- ååé‡: {:.2} æ ·æœ¬/ç§’\n", best.throughput));
                report.push_str(&format!("- å†…å­˜ä½¿ç”¨: {} MB\n", best.memory_usage / 1024 / 1024));
                if let Some(gpu_mem) = best.gpu_memory_usage {
                    report.push_str(&format!("- GPUå†…å­˜: {} MB\n", gpu_mem / 1024 / 1024));
                }
                report.push_str(&format!("- æ¨¡å‹å¤§å°: {} MB\n\n", best.model_size / 1024 / 1024));
            }
            
            // è¯¦ç»†å¯¹æ¯”è¡¨æ ¼
            report.push_str("### æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”\n\n");
            report.push_str("| æ¡†æ¶ | ç¡¬ä»¶ | æ¨ç†æ—¶é—´ | ååé‡ | å†…å­˜(MB) | GPUå†…å­˜(MB) | æ¨¡å‹å¤§å°(MB) |\n");
            report.push_str("|------|------|----------|--------|----------|-------------|-------------|\n");
            
            for perf in performances {
                let gpu_mem_str = perf.gpu_memory_usage
                    .map(|mem| format!("{}", mem / 1024 / 1024))
                    .unwrap_or_else(|| "N/A".to_string());
                
                report.push_str(&format!(
                    "| {:?} | {:?} | {:?} | {:.2} | {} | {} | {} |\n",
                    perf.framework,
                    perf.hardware,
                    perf.inference_time,
                    perf.throughput,
                    perf.memory_usage / 1024 / 1024,
                    gpu_mem_str,
                    perf.model_size / 1024 / 1024
                ));
            }
            report.push_str("\n");
        }
        
        // æ¡†æ¶ç‰¹æ€§å¯¹æ¯”
        report.push_str("## æ¡†æ¶ç‰¹æ€§å¯¹æ¯”\n\n");
        report.push_str("| ç‰¹æ€§ | Candle | Tch | ONNX Runtime |\n");
        report.push_str("|------|--------|-----|-------------|\n");
        report.push_str("| æ€§èƒ½ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |\n");
        report.push_str("| æ˜“ç”¨æ€§ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |\n");
        report.push_str("| æ¨¡å‹æ”¯æŒ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |\n");
        report.push_str("| ç¡¬ä»¶åŠ é€Ÿ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |\n");
        report.push_str("| å†…å­˜æ•ˆç‡ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |\n");
        report.push_str("| ç”Ÿæ€ç³»ç»Ÿ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |\n");
        report.push_str("| éƒ¨ç½²ä¾¿åˆ©æ€§ | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ |\n\n");
        
        // ä½¿ç”¨å»ºè®®
        report.push_str("## ä½¿ç”¨å»ºè®®\n\n");
        report.push_str("### ğŸ•¯ï¸ Candle\n");
        report.push_str("- **é€‚ç”¨åœºæ™¯**: æ–°é¡¹ç›®ã€åŸå‹å¼€å‘ã€è¾¹ç¼˜è®¾å¤‡\n");
        report.push_str("- **ä¼˜åŠ¿**: çº¯Rustå®ç°ã€å†…å­˜å®‰å…¨ã€éƒ¨ç½²ç®€å•\n");
        report.push_str("- **æ¨èæŒ‡æ•°**: â­â­â­â­\n\n");
        
        report.push_str("### ğŸ”¥ Tch (PyTorchç»‘å®š)\n");
        report.push_str("- **é€‚ç”¨åœºæ™¯**: ç ”ç©¶ã€å¤æ‚æ¨¡å‹ã€GPUåŠ é€Ÿ\n");
        report.push_str("- **ä¼˜åŠ¿**: æˆç†Ÿç”Ÿæ€ã€å¼ºå¤§åŠŸèƒ½ã€GPUæ”¯æŒ\n");
        report.push_str("- **æ¨èæŒ‡æ•°**: â­â­â­â­â­\n\n");
        
        report.push_str("### ğŸš€ ONNX Runtime\n");
        report.push_str("- **é€‚ç”¨åœºæ™¯**: ç”Ÿäº§ç¯å¢ƒã€å¤šæ¡†æ¶æ”¯æŒã€ä¼˜åŒ–æ¨ç†\n");
        report.push_str("- **ä¼˜åŠ¿**: é«˜æ€§èƒ½ã€å¤šç¡¬ä»¶æ”¯æŒã€æ¨¡å‹äº’æ“ä½œ\n");
        report.push_str("- **æ¨èæŒ‡æ•°**: â­â­â­â­â­\n\n");
        
        // ç¡¬ä»¶é€‰æ‹©å»ºè®®
        report.push_str("## ç¡¬ä»¶é€‰æ‹©å»ºè®®\n\n");
        report.push_str("### ğŸ–¥ï¸ CPU\n");
        report.push_str("- **é€‚ç”¨**: å°æ¨¡å‹ã€CPUæ¨ç†ã€è¾¹ç¼˜è®¾å¤‡\n");
        report.push_str("- **æ¨èæ¡†æ¶**: Candle, ONNX Runtime\n\n");
        
        report.push_str("### ğŸ® GPU (CUDA)\n");
        report.push_str("- **é€‚ç”¨**: å¤§æ¨¡å‹ã€è®­ç»ƒã€é«˜æ€§èƒ½æ¨ç†\n");
        report.push_str("- **æ¨èæ¡†æ¶**: Tch, ONNX Runtime\n\n");
        
        report.push_str("### ğŸ Metal (Apple Silicon)\n");
        report.push_str("- **é€‚ç”¨**: Macè®¾å¤‡ã€ç§»åŠ¨è®¾å¤‡\n");
        report.push_str("- **æ¨èæ¡†æ¶**: Tch, ONNX Runtime\n\n");
        
        report
    }
    
    /// è·å–æœ€ä½³æ¡†æ¶å»ºè®®
    pub fn get_framework_recommendation(&self, requirements: &str) -> MLFramework {
        match requirements.to_lowercase().as_str() {
            // "research" | "training" | "gpu" => MLFramework::Tch,  // æš‚æ—¶æ³¨é‡Š
            // "production" | "deployment" | "optimization" => MLFramework::ONNXRuntime,  // æš‚æ—¶æ³¨é‡Š
            "new-project" | "rust-native" | "safety" | "research" | "training" | "production" | "deployment" | "optimization" => MLFramework::Candle,
            _ => MLFramework::Candle, // é»˜è®¤æ¨è
        }
    }
    
    /// è·å–æœ€ä½³ç¡¬ä»¶å»ºè®®
    pub fn get_hardware_recommendation(&self, model_size: u64, budget: &str) -> HardwareAcceleration {
        match (model_size, budget.to_lowercase().as_str()) {
            (size, _) if size > 1_000_000_000 => HardwareAcceleration::CUDA, // > 1GBæ¨¡å‹
            (_, "high" | "unlimited") => HardwareAcceleration::CUDA,
            (_, "medium") => HardwareAcceleration::Metal,
            (_, "low" | "minimal") => HardwareAcceleration::CPU,
            _ => HardwareAcceleration::CPU, // é»˜è®¤
        }
    }
}

impl Default for MLFrameworkAnalyzer {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_analyzer_creation() {
        let analyzer = MLFrameworkAnalyzer::new();
        assert!(analyzer.performance_data.is_empty());
    }
    
    #[test]
    fn test_add_performance() {
        let mut analyzer = MLFrameworkAnalyzer::new();
        let performance = ModelPerformance {
            framework: MLFramework::Candle,
            model_type: ModelType::Transformer,
            hardware: HardwareAcceleration::CPU,
            inference_time: Duration::from_millis(100),
            throughput: 100.0,
            memory_usage: 1024 * 1024,
            gpu_memory_usage: None,
            accuracy: Some(0.95),
            model_size: 100 * 1024 * 1024,
        };
        
        analyzer.add_performance(performance);
        assert_eq!(analyzer.performance_data.len(), 1);
    }
    
    #[test]
    fn test_framework_recommendation() {
        let analyzer = MLFrameworkAnalyzer::new();
        assert_eq!(analyzer.get_framework_recommendation("research"), MLFramework::Tch);
        assert_eq!(analyzer.get_framework_recommendation("production"), MLFramework::ONNXRuntime);
        assert_eq!(analyzer.get_framework_recommendation("new-project"), MLFramework::Candle);
    }
    
    #[test]
    fn test_hardware_recommendation() {
        let analyzer = MLFrameworkAnalyzer::new();
        assert_eq!(analyzer.get_hardware_recommendation(100_000_000, "high"), HardwareAcceleration::CUDA);
        assert_eq!(analyzer.get_hardware_recommendation(10_000_000, "medium"), HardwareAcceleration::Metal);
        assert_eq!(analyzer.get_hardware_recommendation(1_000_000, "low"), HardwareAcceleration::CPU);
    }
}
