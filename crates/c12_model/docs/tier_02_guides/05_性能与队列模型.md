# C12 Model - Tier 2: 性能与队列模型

> **文档版本**: v1.0.0  
> **最后更新**: 2025-10-23  
> **Rust 版本**: 1.90+  
> **预计阅读**: 35 分钟

---


## 📊 目录

- [📋 目录](#目录)
- [1. 性能建模基础](#1-性能建模基础)
  - [1.1 性能指标](#11-性能指标)
  - [1.2 Little's Law](#12-littles-law)
- [2. 排队论](#2-排队论)
  - [2.1 M/M/1 队列模型](#21-mm1-队列模型)
  - [2.2 M/M/c 队列模型](#22-mmc-队列模型)
- [3. 负载模型](#3-负载模型)
  - [3.1 开放系统模型](#31-开放系统模型)
  - [3.2 闭合系统模型](#32-闭合系统模型)
- [4. 容量规划](#4-容量规划)
  - [4.1 扩展性分析](#41-扩展性分析)
  - [4.2 容量预测](#42-容量预测)
- [5. 性能优化](#5-性能优化)
  - [5.1 性能瓶颈分析](#51-性能瓶颈分析)
  - [5.2 负载均衡策略](#52-负载均衡策略)
- [6. 实战案例](#6-实战案例)
  - [6.1 Web 服务器性能模型](#61-web-服务器性能模型)
  - [6.2 数据库连接池优化](#62-数据库连接池优化)
- [7. 总结](#7-总结)
  - [核心要点](#核心要点)
  - [最佳实践](#最佳实践)
- [📚 参考资源](#参考资源)


## 📋 目录

1. [性能建模基础](#1-性能建模基础)
2. [排队论](#2-排队论)
3. [负载模型](#3-负载模型)
4. [容量规划](#4-容量规划)
5. [性能优化](#5-性能优化)
6. [实战案例](#6-实战案例)

---

## 1. 性能建模基础

### 1.1 性能指标

```rust
use std::time::{Duration, Instant};

struct PerformanceMetrics {
    latency: Duration,       // 延迟
    throughput: f64,         // 吞吐量（请求/秒）
    utilization: f64,        // 利用率（0.0-1.0）
    queue_length: usize,     // 队列长度
}

impl PerformanceMetrics {
    fn new() -> Self {
        Self {
            latency: Duration::from_secs(0),
            throughput: 0.0,
            utilization: 0.0,
            queue_length: 0,
        }
    }
    
    fn calculate_throughput(&mut self, requests: usize, duration: Duration) {
        self.throughput = requests as f64 / duration.as_secs_f64();
    }
    
    fn calculate_utilization(&mut self, busy_time: Duration, total_time: Duration) {
        self.utilization = busy_time.as_secs_f64() / total_time.as_secs_f64();
    }
    
    fn report(&self) {
        println!("=== 性能指标 ===");
        println!("延迟: {:?}", self.latency);
        println!("吞吐量: {:.2} 请求/秒", self.throughput);
        println!("利用率: {:.2}%", self.utilization * 100.0);
        println!("队列长度: {}", self.queue_length);
    }
}

fn main() {
    let mut metrics = PerformanceMetrics::new();
    metrics.calculate_throughput(1000, Duration::from_secs(10));
    metrics.calculate_utilization(Duration::from_secs(8), Duration::from_secs(10));
    metrics.latency = Duration::from_millis(50);
    metrics.queue_length = 10;
    
    metrics.report();
}
```

### 1.2 Little's Law

**公式**: `L = λ × W`

- `L`: 系统中的平均请求数
- `λ`: 请求到达率（请求/秒）
- `W`: 平均响应时间（秒）

```rust
fn littles_law(arrival_rate: f64, response_time: f64) -> f64 {
    arrival_rate * response_time
}

fn main() {
    let arrival_rate = 100.0; // 100 请求/秒
    let response_time = 0.05;  // 50ms
    
    let avg_requests = littles_law(arrival_rate, response_time);
    println!("系统平均请求数: {:.2}", avg_requests);
}
```

---

## 2. 排队论

### 2.1 M/M/1 队列模型

**M/M/1**: 马尔可夫到达、马尔可夫服务、1 个服务器

```rust
struct MM1Queue {
    arrival_rate: f64,   // λ (请求/秒)
    service_rate: f64,   // μ (请求/秒)
}

impl MM1Queue {
    fn new(arrival_rate: f64, service_rate: f64) -> Self {
        assert!(arrival_rate < service_rate, "系统不稳定：λ >= μ");
        Self { arrival_rate, service_rate }
    }
    
    // 利用率 ρ = λ/μ
    fn utilization(&self) -> f64 {
        self.arrival_rate / self.service_rate
    }
    
    // 平均队列长度 L = ρ/(1-ρ)
    fn avg_queue_length(&self) -> f64 {
        let rho = self.utilization();
        rho / (1.0 - rho)
    }
    
    // 平均等待时间 W = L/λ
    fn avg_waiting_time(&self) -> f64 {
        self.avg_queue_length() / self.arrival_rate
    }
    
    // 平均响应时间 R = W + 1/μ
    fn avg_response_time(&self) -> f64 {
        self.avg_waiting_time() + (1.0 / self.service_rate)
    }
    
    fn report(&self) {
        println!("=== M/M/1 队列分析 ===");
        println!("到达率: {:.2} 请求/秒", self.arrival_rate);
        println!("服务率: {:.2} 请求/秒", self.service_rate);
        println!("利用率: {:.2}%", self.utilization() * 100.0);
        println!("平均队列长度: {:.2}", self.avg_queue_length());
        println!("平均等待时间: {:.4} 秒", self.avg_waiting_time());
        println!("平均响应时间: {:.4} 秒", self.avg_response_time());
    }
}

fn main() {
    let queue = MM1Queue::new(80.0, 100.0);
    queue.report();
}
```

### 2.2 M/M/c 队列模型

**M/M/c**: 马尔可夫到达、马尔可夫服务、c 个服务器

```rust
struct MMcQueue {
    arrival_rate: f64,
    service_rate: f64,
    num_servers: usize,
}

impl MMcQueue {
    fn new(arrival_rate: f64, service_rate: f64, num_servers: usize) -> Self {
        assert!(
            arrival_rate < service_rate * num_servers as f64,
            "系统不稳定"
        );
        Self { arrival_rate, service_rate, num_servers }
    }
    
    fn utilization(&self) -> f64 {
        self.arrival_rate / (self.service_rate * self.num_servers as f64)
    }
    
    // 简化的平均响应时间估算
    fn avg_response_time_approx(&self) -> f64 {
        let rho = self.utilization();
        let base_service_time = 1.0 / self.service_rate;
        
        // 使用 Erlang C 公式的简化版本
        let waiting_time = rho * base_service_time / (self.num_servers as f64 * (1.0 - rho));
        
        waiting_time + base_service_time
    }
    
    fn report(&self) {
        println!("=== M/M/{} 队列分析 ===", self.num_servers);
        println!("到达率: {:.2} 请求/秒", self.arrival_rate);
        println!("服务率: {:.2} 请求/秒（每服务器）", self.service_rate);
        println!("服务器数: {}", self.num_servers);
        println!("利用率: {:.2}%", self.utilization() * 100.0);
        println!("近似响应时间: {:.4} 秒", self.avg_response_time_approx());
    }
}

fn main() {
    let queue = MMcQueue::new(400.0, 100.0, 5);
    queue.report();
}
```

---

## 3. 负载模型

### 3.1 开放系统模型

```rust
struct OpenSystemModel {
    arrival_rate: f64,
    service_time: f64,
}

impl OpenSystemModel {
    fn new(arrival_rate: f64, service_time: f64) -> Self {
        Self { arrival_rate, service_time }
    }
    
    fn avg_response_time(&self) -> f64 {
        // R = S / (1 - λS)，其中 S 是服务时间，λ 是到达率
        let utilization = self.arrival_rate * self.service_time;
        assert!(utilization < 1.0, "系统过载");
        
        self.service_time / (1.0 - utilization)
    }
    
    fn max_throughput(&self) -> f64 {
        1.0 / self.service_time
    }
}

fn main() {
    let model = OpenSystemModel::new(80.0, 0.01); // 80 req/s, 10ms 服务时间
    println!("平均响应时间: {:.4} 秒", model.avg_response_time());
    println!("最大吞吐量: {:.2} 请求/秒", model.max_throughput());
}
```

### 3.2 闭合系统模型

```rust
struct ClosedSystemModel {
    num_users: usize,
    think_time: f64,
    service_time: f64,
}

impl ClosedSystemModel {
    fn new(num_users: usize, think_time: f64, service_time: f64) -> Self {
        Self { num_users, think_time, service_time }
    }
    
    // 使用近似公式
    fn avg_response_time(&self) -> f64 {
        let n = self.num_users as f64;
        let z = self.think_time;
        let s = self.service_time;
        
        // R ≈ S × N / (1 + N × S / Z)
        s * n / (1.0 + n * s / z)
    }
    
    fn throughput(&self) -> f64 {
        let r = self.avg_response_time();
        self.num_users as f64 / (r + self.think_time)
    }
}

fn main() {
    let model = ClosedSystemModel::new(100, 5.0, 0.1); // 100用户, 5s思考, 0.1s服务
    println!("平均响应时间: {:.4} 秒", model.avg_response_time());
    println!("吞吐量: {:.2} 请求/秒", model.throughput());
}
```

---

## 4. 容量规划

### 4.1 扩展性分析

```rust
struct ScalabilityAnalysis {
    baseline_throughput: f64,
    baseline_servers: usize,
}

impl ScalabilityAnalysis {
    fn new(baseline_throughput: f64, baseline_servers: usize) -> Self {
        Self { baseline_throughput, baseline_servers }
    }
    
    // Amdahl's Law: 加速比 = 1 / (s + p/n)
    // s: 串行部分, p: 并行部分, n: 处理器数量
    fn amdahl_speedup(&self, serial_fraction: f64, num_servers: usize) -> f64 {
        let parallel_fraction = 1.0 - serial_fraction;
        1.0 / (serial_fraction + parallel_fraction / num_servers as f64)
    }
    
    // 线性扩展
    fn linear_scaling(&self, num_servers: usize) -> f64 {
        self.baseline_throughput * (num_servers as f64 / self.baseline_servers as f64)
    }
    
    // 次线性扩展（考虑开销）
    fn sublinear_scaling(&self, num_servers: usize, overhead: f64) -> f64 {
        let ideal = self.linear_scaling(num_servers);
        ideal * (1.0 - overhead * (num_servers as f64 - 1.0))
    }
    
    fn report(&self, target_servers: usize) {
        println!("=== 扩展性分析 ===");
        println!("基准吞吐量: {:.2} ({}台服务器)", self.baseline_throughput, self.baseline_servers);
        println!("目标服务器: {}", target_servers);
        println!("线性扩展: {:.2}", self.linear_scaling(target_servers));
        println!("次线性扩展 (10%开销): {:.2}", self.sublinear_scaling(target_servers, 0.1));
        println!("Amdahl 加速比 (10%串行): {:.2}x", self.amdahl_speedup(0.1, target_servers));
    }
}

fn main() {
    let analysis = ScalabilityAnalysis::new(1000.0, 2);
    analysis.report(10);
}
```

### 4.2 容量预测

```rust
struct CapacityPlanner {
    current_load: f64,
    current_capacity: f64,
    growth_rate: f64, // 月增长率
}

impl CapacityPlanner {
    fn new(current_load: f64, current_capacity: f64, growth_rate: f64) -> Self {
        Self { current_load, current_capacity, growth_rate }
    }
    
    fn predict_load(&self, months: usize) -> f64 {
        self.current_load * (1.0 + self.growth_rate).powi(months as i32)
    }
    
    fn months_until_capacity(&self) -> usize {
        let mut months = 0;
        let mut load = self.current_load;
        
        while load < self.current_capacity {
            load *= 1.0 + self.growth_rate;
            months += 1;
        }
        
        months
    }
    
    fn required_capacity(&self, months: usize, safety_margin: f64) -> f64 {
        self.predict_load(months) * (1.0 + safety_margin)
    }
    
    fn report(&self) {
        println!("=== 容量规划 ===");
        println!("当前负载: {:.2}", self.current_load);
        println!("当前容量: {:.2}", self.current_capacity);
        println!("月增长率: {:.2}%", self.growth_rate * 100.0);
        
        println!("\n预测负载:");
        for months in [3, 6, 12] {
            let load = self.predict_load(months);
            println!("  {}个月: {:.2}", months, load);
        }
        
        println!("\n容量不足预警: {}个月", self.months_until_capacity());
        println!("建议容量 (12个月, 20%裕量): {:.2}", self.required_capacity(12, 0.2));
    }
}

fn main() {
    let planner = CapacityPlanner::new(5000.0, 10000.0, 0.05);
    planner.report();
}
```

---

## 5. 性能优化

### 5.1 性能瓶颈分析

```rust
use std::collections::HashMap;

struct ResourceUtilization {
    cpu: f64,
    memory: f64,
    disk_io: f64,
    network_io: f64,
}

impl ResourceUtilization {
    fn identify_bottleneck(&self) -> &str {
        let mut max_util = self.cpu;
        let mut bottleneck = "CPU";
        
        if self.memory > max_util {
            max_util = self.memory;
            bottleneck = "内存";
        }
        if self.disk_io > max_util {
            max_util = self.disk_io;
            bottleneck = "磁盘I/O";
        }
        if self.network_io > max_util {
            bottleneck = "网络I/O";
        }
        
        bottleneck
    }
    
    fn report(&self) {
        println!("=== 资源利用率 ===");
        println!("CPU: {:.2}%", self.cpu * 100.0);
        println!("内存: {:.2}%", self.memory * 100.0);
        println!("磁盘I/O: {:.2}%", self.disk_io * 100.0);
        println!("网络I/O: {:.2}%", self.network_io * 100.0);
        println!("瓶颈: {}", self.identify_bottleneck());
    }
}

fn main() {
    let utilization = ResourceUtilization {
        cpu: 0.45,
        memory: 0.60,
        disk_io: 0.85,  // 瓶颈
        network_io: 0.30,
    };
    
    utilization.report();
}
```

### 5.2 负载均衡策略

```rust
struct LoadBalancer {
    servers: Vec<String>,
    current_index: usize,
}

impl LoadBalancer {
    fn new(servers: Vec<String>) -> Self {
        Self { servers, current_index: 0 }
    }
    
    // 轮询
    fn round_robin(&mut self) -> &str {
        let server = &self.servers[self.current_index];
        self.current_index = (self.current_index + 1) % self.servers.len();
        server
    }
    
    // 加权轮询（简化版）
    fn weighted_round_robin(&mut self, weights: &[usize]) -> &str {
        // 实际实现应考虑权重
        let total_weight: usize = weights.iter().sum();
        let index = self.current_index % total_weight;
        
        let mut cumulative = 0;
        for (i, &weight) in weights.iter().enumerate() {
            cumulative += weight;
            if index < cumulative {
                self.current_index += 1;
                return &self.servers[i];
            }
        }
        
        &self.servers[0]
    }
    
    // 最少连接
    fn least_connections(&self, connections: &[usize]) -> &str {
        let min_index = connections.iter()
            .enumerate()
            .min_by_key(|(_, &conn)| conn)
            .map(|(i, _)| i)
            .unwrap_or(0);
        
        &self.servers[min_index]
    }
}

fn main() {
    let mut lb = LoadBalancer::new(vec![
        "Server1".to_string(),
        "Server2".to_string(),
        "Server3".to_string(),
    ]);
    
    println!("=== 轮询 ===");
    for _ in 0..5 {
        println!("{}", lb.round_robin());
    }
    
    println!("\n=== 最少连接 ===");
    let connections = vec![5, 2, 8];
    println!("{}", lb.least_connections(&connections));
}
```

---

## 6. 实战案例

### 6.1 Web 服务器性能模型

```rust
struct WebServerModel {
    num_workers: usize,
    request_rate: f64,
    avg_processing_time: f64,
}

impl WebServerModel {
    fn new(num_workers: usize, request_rate: f64, avg_processing_time: f64) -> Self {
        Self { num_workers, request_rate, avg_processing_time }
    }
    
    fn analyze(&self) -> PerformanceMetrics {
        let queue = MMcQueue::new(
            self.request_rate,
            1.0 / self.avg_processing_time,
            self.num_workers,
        );
        
        let mut metrics = PerformanceMetrics::new();
        metrics.throughput = self.request_rate;
        metrics.latency = Duration::from_secs_f64(queue.avg_response_time_approx());
        metrics.utilization = queue.utilization();
        
        metrics
    }
    
    fn recommend_workers(&self, target_response_time: f64) -> usize {
        let mut workers = 1;
        loop {
            let model = Self::new(workers, self.request_rate, self.avg_processing_time);
            let queue = MMcQueue::new(
                model.request_rate,
                1.0 / model.avg_processing_time,
                workers,
            );
            
            if queue.avg_response_time_approx() <= target_response_time {
                return workers;
            }
            
            workers += 1;
            if workers > 100 {
                break; // 防止无限循环
            }
        }
        workers
    }
}

fn main() {
    let model = WebServerModel::new(4, 200.0, 0.015); // 4工作线程, 200req/s, 15ms处理
    let metrics = model.analyze();
    metrics.report();
    
    let recommended = model.recommend_workers(0.05); // 目标50ms响应时间
    println!("\n建议工作线程数: {}", recommended);
}
```

### 6.2 数据库连接池优化

```rust
struct ConnectionPoolModel {
    pool_size: usize,
    query_rate: f64,
    avg_query_time: f64,
    connection_timeout: f64,
}

impl ConnectionPoolModel {
    fn new(pool_size: usize, query_rate: f64, avg_query_time: f64) -> Self {
        Self {
            pool_size,
            query_rate,
            avg_query_time,
            connection_timeout: 1.0, // 1秒超时
        }
    }
    
    fn analyze(&self) -> (f64, f64) {
        let queue = MMcQueue::new(
            self.query_rate,
            1.0 / self.avg_query_time,
            self.pool_size,
        );
        
        let avg_wait_time = queue.avg_response_time_approx() - self.avg_query_time;
        let timeout_probability = if avg_wait_time > self.connection_timeout {
            (avg_wait_time - self.connection_timeout) / avg_wait_time
        } else {
            0.0
        };
        
        (avg_wait_time, timeout_probability)
    }
    
    fn optimal_pool_size(&self, max_timeout_prob: f64) -> usize {
        let mut size = 1;
        loop {
            let model = Self::new(size, self.query_rate, self.avg_query_time);
            let (_, timeout_prob) = model.analyze();
            
            if timeout_prob <= max_timeout_prob {
                return size;
            }
            
            size += 1;
            if size > 1000 {
                break;
            }
        }
        size
    }
}

fn main() {
    let model = ConnectionPoolModel::new(10, 100.0, 0.05); // 10连接, 100查询/s, 50ms查询
    let (wait_time, timeout_prob) = model.analyze();
    
    println!("=== 连接池分析 ===");
    println!("连接池大小: {}", model.pool_size);
    println!("平均等待时间: {:.4} 秒", wait_time);
    println!("超时概率: {:.2}%", timeout_prob * 100.0);
    
    let optimal = model.optimal_pool_size(0.01); // 1%超时容忍度
    println!("最优连接池大小: {}", optimal);
}
```

---

## 7. 总结

### 核心要点

| 模型 | 用途 | 关键指标 |
|-----|------|---------|
| **M/M/1** | 单服务器队列 | 利用率、队列长度、响应时间 |
| **M/M/c** | 多服务器队列 | 并发处理能力 |
| **Little's Law** | 系统分析 | L = λ × W |
| **Amdahl's Law** | 扩展性 | 加速比 |

### 最佳实践

1. **监控关键指标**: 延迟、吞吐量、利用率
2. **容量规划**: 预留 20-30% 裕量
3. **负载测试**: 模拟峰值负载
4. **瓶颈优化**: 识别并优化最慢组件
5. **水平扩展**: 优先选择无状态服务

---

## 📚 参考资源

- [Queuing Theory](https://en.wikipedia.org/wiki/Queueing_theory)
- [Performance Modeling](https://www.brendangregg.com/sysperfbook.html)
- [Little's Law](https://en.wikipedia.org/wiki/Little%27s_law)

---

**🎉 恭喜！** 你已完成 C12 Model Tier 2 全部实践指南的学习。

**下一步建议**：

- 深入学习 [Tier 3: 技术参考](../tier_03_references/README.md)
- 探索 [Tier 4: 高级主题](../tier_04_advanced/README.md)
- 实践应用到实际项目中
