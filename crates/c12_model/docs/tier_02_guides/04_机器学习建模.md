# C12 Model - Tier 2: 机器学习建模

> **文档版本**: v1.0.0  
> **最后更新**: 2025-10-23  
> **Rust 版本**: 1.90+  
> **预计阅读**: 35 分钟

---

## 📋 目录

- [C12 Model - Tier 2: 机器学习建模](#c12-model---tier-2-机器学习建模)
  - [📋 目录](#-目录)
  - [1. 机器学习概述](#1-机器学习概述)
    - [1.1 机器学习类型](#11-机器学习类型)
    - [1.2 数据预处理](#12-数据预处理)
  - [2. 线性回归](#2-线性回归)
    - [2.1 简单线性回归](#21-简单线性回归)
    - [2.2 多项式回归](#22-多项式回归)
  - [3. 逻辑回归与分类](#3-逻辑回归与分类)
    - [3.1 Sigmoid 函数](#31-sigmoid-函数)
  - [4. 神经网络基础](#4-神经网络基础)
    - [4.1 简单神经网络](#41-简单神经网络)
  - [5. 模型评估](#5-模型评估)
    - [5.1 评估指标](#51-评估指标)
    - [5.2 交叉验证](#52-交叉验证)
  - [6. 实战案例](#6-实战案例)
    - [6.1 房价预测](#61-房价预测)
    - [6.2 垃圾邮件分类](#62-垃圾邮件分类)
  - [7. 总结](#7-总结)
    - [核心要点](#核心要点)
    - [最佳实践](#最佳实践)
  - [📚 参考资源](#-参考资源)

---

## 1. 机器学习概述

### 1.1 机器学习类型

```rust
enum LearningType {
    Supervised,    // 监督学习
    Unsupervised,  // 无监督学习
    Reinforcement, // 强化学习
}

struct MLModel {
    learning_type: LearningType,
    parameters: Vec<f64>,
}

impl MLModel {
    fn new(learning_type: LearningType) -> Self {
        Self {
            learning_type,
            parameters: Vec::new(),
        }
    }
}

fn main() {
    let model = MLModel::new(LearningType::Supervised);
    println!("创建监督学习模型");
}
```

### 1.2 数据预处理

```rust
struct Dataset {
    features: Vec<Vec<f64>>,
    labels: Vec<f64>,
}

impl Dataset {
    // 标准化
    fn normalize(&mut self) {
        let n_features = self.features[0].len();
        
        for j in 0..n_features {
            // 计算均值和标准差
            let values: Vec<f64> = self.features.iter().map(|x| x[j]).collect();
            let mean = values.iter().sum::<f64>() / values.len() as f64;
            let variance = values.iter().map(|x| (x - mean).powi(2)).sum::<f64>() / values.len() as f64;
            let std_dev = variance.sqrt();
            
            // 标准化
            for i in 0..self.features.len() {
                self.features[i][j] = (self.features[i][j] - mean) / std_dev;
            }
        }
    }
    
    // 划分训练集和测试集
    fn split(&self, train_ratio: f64) -> (Dataset, Dataset) {
        let train_size = (self.features.len() as f64 * train_ratio) as usize;
        
        let train = Dataset {
            features: self.features[..train_size].to_vec(),
            labels: self.labels[..train_size].to_vec(),
        };
        
        let test = Dataset {
            features: self.features[train_size..].to_vec(),
            labels: self.labels[train_size..].to_vec(),
        };
        
        (train, test)
    }
}

fn main() {
    let mut dataset = Dataset {
        features: vec![
            vec![1.0, 2.0],
            vec![2.0, 3.0],
            vec![3.0, 4.0],
            vec![4.0, 5.0],
        ],
        labels: vec![1.0, 2.0, 3.0, 4.0],
    };
    
    dataset.normalize();
    println!("标准化后: {:?}", dataset.features);
    
    let (train, test) = dataset.split(0.8);
    println!("训练集: {}, 测试集: {}", train.features.len(), test.features.len());
}
```

---

## 2. 线性回归

### 2.1 简单线性回归

```rust
struct LinearRegression {
    weights: Vec<f64>,
    bias: f64,
    learning_rate: f64,
}

impl LinearRegression {
    fn new(n_features: usize, learning_rate: f64) -> Self {
        Self {
            weights: vec![0.0; n_features],
            bias: 0.0,
            learning_rate,
        }
    }
    
    // 预测
    fn predict(&self, features: &[f64]) -> f64 {
        let mut prediction = self.bias;
        for (w, x) in self.weights.iter().zip(features.iter()) {
            prediction += w * x;
        }
        prediction
    }
    
    // 训练（梯度下降）
    fn train(&mut self, dataset: &Dataset, epochs: usize) {
        let n = dataset.features.len() as f64;
        
        for epoch in 0..epochs {
            let mut total_loss = 0.0;
            
            for (features, &label) in dataset.features.iter().zip(dataset.labels.iter()) {
                // 前向传播
                let prediction = self.predict(features);
                let error = prediction - label;
                total_loss += error * error;
                
                // 反向传播
                for (w, &x) in self.weights.iter_mut().zip(features.iter()) {
                    *w -= self.learning_rate * (2.0 / n) * error * x;
                }
                self.bias -= self.learning_rate * (2.0 / n) * error;
            }
            
            if epoch % 100 == 0 {
                println!("Epoch {}: Loss = {}", epoch, total_loss / n);
            }
        }
    }
}

fn main() {
    let dataset = Dataset {
        features: vec![
            vec![1.0],
            vec![2.0],
            vec![3.0],
            vec![4.0],
        ],
        labels: vec![2.0, 4.0, 6.0, 8.0],
    };
    
    let mut model = LinearRegression::new(1, 0.01);
    model.train(&dataset, 1000);
    
    // 预测
    let prediction = model.predict(&[5.0]);
    println!("预测 x=5: {}", prediction);
    println!("权重: {:?}, 偏置: {}", model.weights, model.bias);
}
```

### 2.2 多项式回归

```rust
fn polynomial_features(x: f64, degree: usize) -> Vec<f64> {
    (0..=degree).map(|d| x.powi(d as i32)).collect()
}

fn polynomial_regression_demo() {
    let x_data = vec![1.0, 2.0, 3.0, 4.0, 5.0];
    let y_data = vec![2.1, 4.2, 5.9, 8.1, 10.2];
    
    // 转换为多项式特征（degree=2）
    let features: Vec<Vec<f64>> = x_data.iter()
        .map(|&x| polynomial_features(x, 2))
        .collect();
    
    let dataset = Dataset { features, labels: y_data };
    
    let mut model = LinearRegression::new(3, 0.001);
    model.train(&dataset, 1000);
    
    println!("多项式回归完成");
}

fn main() {
    polynomial_regression_demo();
}
```

---

## 3. 逻辑回归与分类

### 3.1 Sigmoid 函数

```rust
fn sigmoid(x: f64) -> f64 {
    1.0 / (1.0 + (-x).exp())
}

struct LogisticRegression {
    weights: Vec<f64>,
    bias: f64,
    learning_rate: f64,
}

impl LogisticRegression {
    fn new(n_features: usize, learning_rate: f64) -> Self {
        Self {
            weights: vec![0.0; n_features],
            bias: 0.0,
            learning_rate,
        }
    }
    
    fn predict_proba(&self, features: &[f64]) -> f64 {
        let mut z = self.bias;
        for (w, x) in self.weights.iter().zip(features.iter()) {
            z += w * x;
        }
        sigmoid(z)
    }
    
    fn predict(&self, features: &[f64]) -> u8 {
        if self.predict_proba(features) >= 0.5 { 1 } else { 0 }
    }
    
    fn train(&mut self, dataset: &Dataset, epochs: usize) {
        let n = dataset.features.len() as f64;
        
        for epoch in 0..epochs {
            for (features, &label) in dataset.features.iter().zip(dataset.labels.iter()) {
                let prediction = self.predict_proba(features);
                let error = prediction - label;
                
                // 更新权重
                for (w, &x) in self.weights.iter_mut().zip(features.iter()) {
                    *w -= self.learning_rate * error * x / n;
                }
                self.bias -= self.learning_rate * error / n;
            }
            
            if epoch % 1000 == 0 {
                let accuracy = self.evaluate(dataset);
                println!("Epoch {}: Accuracy = {:.2}%", epoch, accuracy * 100.0);
            }
        }
    }
    
    fn evaluate(&self, dataset: &Dataset) -> f64 {
        let correct = dataset.features.iter()
            .zip(dataset.labels.iter())
            .filter(|(features, &label)| {
                self.predict(features) == label as u8
            })
            .count();
        
        correct as f64 / dataset.features.len() as f64
    }
}

fn main() {
    // 二分类数据
    let dataset = Dataset {
        features: vec![
            vec![1.0, 2.0],
            vec![2.0, 3.0],
            vec![3.0, 1.0],
            vec![6.0, 5.0],
            vec![7.0, 8.0],
            vec![8.0, 7.0],
        ],
        labels: vec![0.0, 0.0, 0.0, 1.0, 1.0, 1.0],
    };
    
    let mut model = LogisticRegression::new(2, 0.1);
    model.train(&dataset, 5000);
    
    // 预测
    let test_sample = vec![5.0, 4.0];
    println!("预测类别: {}", model.predict(&test_sample));
}
```

---

## 4. 神经网络基础

### 4.1 简单神经网络

```rust
fn relu(x: f64) -> f64 {
    x.max(0.0)
}

fn relu_derivative(x: f64) -> f64 {
    if x > 0.0 { 1.0 } else { 0.0 }
}

struct NeuralNetwork {
    // 权重矩阵
    w1: Vec<Vec<f64>>, // 输入层到隐藏层
    b1: Vec<f64>,
    w2: Vec<Vec<f64>>, // 隐藏层到输出层
    b2: Vec<f64>,
    learning_rate: f64,
}

impl NeuralNetwork {
    fn new(input_size: usize, hidden_size: usize, output_size: usize, learning_rate: f64) -> Self {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        
        let w1 = (0..hidden_size)
            .map(|_| (0..input_size).map(|_| rng.gen_range(-0.5..0.5)).collect())
            .collect();
        
        let w2 = (0..output_size)
            .map(|_| (0..hidden_size).map(|_| rng.gen_range(-0.5..0.5)).collect())
            .collect();
        
        Self {
            w1,
            b1: vec![0.0; hidden_size],
            w2,
            b2: vec![0.0; output_size],
            learning_rate,
        }
    }
    
    fn forward(&self, input: &[f64]) -> (Vec<f64>, Vec<f64>) {
        // 隐藏层
        let mut hidden = vec![0.0; self.b1.len()];
        for i in 0..self.b1.len() {
            let mut sum = self.b1[i];
            for j in 0..input.len() {
                sum += self.w1[i][j] * input[j];
            }
            hidden[i] = relu(sum);
        }
        
        // 输出层
        let mut output = vec![0.0; self.b2.len()];
        for i in 0..self.b2.len() {
            let mut sum = self.b2[i];
            for j in 0..hidden.len() {
                sum += self.w2[i][j] * hidden[j];
            }
            output[i] = sum; // 回归任务不需要激活函数
        }
        
        (hidden, output)
    }
    
    fn train(&mut self, dataset: &Dataset, epochs: usize) {
        for epoch in 0..epochs {
            let mut total_loss = 0.0;
            
            for (features, &label) in dataset.features.iter().zip(dataset.labels.iter()) {
                let (hidden, output) = self.forward(features);
                let error = output[0] - label;
                total_loss += error * error;
                
                // 反向传播（简化版）
                // 输出层梯度
                let output_grad = 2.0 * error;
                
                // 更新 w2 和 b2
                for i in 0..self.w2.len() {
                    for j in 0..self.w2[i].len() {
                        self.w2[i][j] -= self.learning_rate * output_grad * hidden[j];
                    }
                    self.b2[i] -= self.learning_rate * output_grad;
                }
            }
            
            if epoch % 100 == 0 {
                println!("Epoch {}: Loss = {}", epoch, total_loss / dataset.features.len() as f64);
            }
        }
    }
}

fn main() {
    let dataset = Dataset {
        features: vec![
            vec![0.0, 0.0],
            vec![0.0, 1.0],
            vec![1.0, 0.0],
            vec![1.0, 1.0],
        ],
        labels: vec![0.0, 1.0, 1.0, 0.0], // XOR
    };
    
    let mut nn = NeuralNetwork::new(2, 4, 1, 0.1);
    nn.train(&dataset, 1000);
    
    println!("神经网络训练完成");
}
```

---

## 5. 模型评估

### 5.1 评估指标

```rust
struct ClassificationMetrics {
    accuracy: f64,
    precision: f64,
    recall: f64,
    f1_score: f64,
}

fn calculate_metrics(y_true: &[u8], y_pred: &[u8]) -> ClassificationMetrics {
    let mut tp = 0;
    let mut fp = 0;
    let mut fn_count = 0;
    let mut tn = 0;
    
    for (&true_label, &pred_label) in y_true.iter().zip(y_pred.iter()) {
        match (true_label, pred_label) {
            (1, 1) => tp += 1,
            (0, 1) => fp += 1,
            (1, 0) => fn_count += 1,
            (0, 0) => tn += 1,
            _ => {}
        }
    }
    
    let accuracy = (tp + tn) as f64 / y_true.len() as f64;
    let precision = tp as f64 / (tp + fp) as f64;
    let recall = tp as f64 / (tp + fn_count) as f64;
    let f1_score = 2.0 * (precision * recall) / (precision + recall);
    
    ClassificationMetrics { accuracy, precision, recall, f1_score }
}

fn main() {
    let y_true = vec![1, 0, 1, 1, 0, 1, 0, 0];
    let y_pred = vec![1, 0, 1, 0, 0, 1, 0, 1];
    
    let metrics = calculate_metrics(&y_true, &y_pred);
    println!("准确率: {:.2}", metrics.accuracy);
    println!("精确率: {:.2}", metrics.precision);
    println!("召回率: {:.2}", metrics.recall);
    println!("F1 分数: {:.2}", metrics.f1_score);
}
```

### 5.2 交叉验证

```rust
fn k_fold_cross_validation(dataset: &Dataset, k: usize) -> Vec<f64> {
    let fold_size = dataset.features.len() / k;
    let mut scores = Vec::new();
    
    for i in 0..k {
        // 划分训练集和验证集
        let val_start = i * fold_size;
        let val_end = (i + 1) * fold_size;
        
        let train_features = [
            &dataset.features[..val_start],
            &dataset.features[val_end..],
        ].concat();
        let train_labels = [
            &dataset.labels[..val_start],
            &dataset.labels[val_end..],
        ].concat();
        
        let val_features = dataset.features[val_start..val_end].to_vec();
        let val_labels = dataset.labels[val_start..val_end].to_vec();
        
        // 训练模型
        let train_set = Dataset { features: train_features, labels: train_labels };
        let mut model = LinearRegression::new(dataset.features[0].len(), 0.01);
        model.train(&train_set, 500);
        
        // 评估
        let mut mse = 0.0;
        for (features, &label) in val_features.iter().zip(val_labels.iter()) {
            let prediction = model.predict(features);
            mse += (prediction - label).powi(2);
        }
        mse /= val_features.len() as f64;
        
        scores.push(mse);
        println!("Fold {}: MSE = {}", i + 1, mse);
    }
    
    scores
}

fn main() {
    let dataset = Dataset {
        features: vec![
            vec![1.0], vec![2.0], vec![3.0], vec![4.0],
            vec![5.0], vec![6.0], vec![7.0], vec![8.0],
        ],
        labels: vec![2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0],
    };
    
    let scores = k_fold_cross_validation(&dataset, 4);
    let avg_score = scores.iter().sum::<f64>() / scores.len() as f64;
    println!("平均 MSE: {}", avg_score);
}
```

---

## 6. 实战案例

### 6.1 房价预测

```rust
fn house_price_prediction() {
    // 特征: [面积, 卧室数, 楼层]
    let dataset = Dataset {
        features: vec![
            vec![50.0, 1.0, 1.0],
            vec![80.0, 2.0, 3.0],
            vec![120.0, 3.0, 5.0],
            vec![150.0, 4.0, 7.0],
            vec![200.0, 5.0, 10.0],
        ],
        labels: vec![150.0, 250.0, 400.0, 520.0, 750.0], // 价格（万元）
    };
    
    let mut model = LinearRegression::new(3, 0.0001);
    model.train(&dataset, 2000);
    
    // 预测新房价格
    let new_house = vec![100.0, 3.0, 4.0];
    let predicted_price = model.predict(&new_house);
    println!("预测房价: {:.2} 万元", predicted_price);
}

fn main() {
    house_price_prediction();
}
```

### 6.2 垃圾邮件分类

```rust
fn spam_classification() {
    // 简化特征: [关键词数量, 链接数量]
    let dataset = Dataset {
        features: vec![
            vec![0.0, 0.0],
            vec![1.0, 0.0],
            vec![5.0, 2.0],
            vec![10.0, 5.0],
            vec![15.0, 8.0],
            vec![20.0, 10.0],
        ],
        labels: vec![0.0, 0.0, 0.0, 1.0, 1.0, 1.0], // 0=正常, 1=垃圾
    };
    
    let mut model = LogisticRegression::new(2, 0.1);
    model.train(&dataset, 3000);
    
    // 预测
    let test_email = vec![8.0, 3.0];
    let is_spam = model.predict(&test_email);
    println!("邮件分类: {}", if is_spam == 1 { "垃圾邮件" } else { "正常邮件" });
}

fn main() {
    spam_classification();
}
```

---

## 7. 总结

### 核心要点

| 模型 | 任务类型 | 适用场景 |
|-----|---------|---------|
| **线性回归** | 回归 | 连续值预测 |
| **逻辑回归** | 分类 | 二分类问题 |
| **神经网络** | 回归/分类 | 复杂非线性关系 |

### 最佳实践

1. **数据预处理**: 标准化/归一化
2. **特征工程**: 多项式特征、交叉特征
3. **超参数调优**: 学习率、批次大小
4. **正则化**: 防止过拟合
5. **交叉验证**: 评估模型泛化能力

---

## 📚 参考资源

- [Rust ML Ecosystem](https://www.arewelearningyet.com/)
- [Machine Learning Basics](https://developers.google.com/machine-learning/crash-course)
- [ndarray Crate](https://docs.rs/ndarray/)

---

**下一步**: 学习 [性能与队列模型](05_性能与队列模型.md)，掌握性能分析技术。
