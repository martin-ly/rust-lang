# C12 Model - Tier 2: æœºå™¨å­¦ä¹ å»ºæ¨¡

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **æœ€åæ›´æ–°**: 2025-10-23  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **é¢„è®¡é˜…è¯»**: 35 åˆ†é’Ÿ

---

## ğŸ“‹ ç›®å½•

- [C12 Model - Tier 2: æœºå™¨å­¦ä¹ å»ºæ¨¡](#c12-model---tier-2-æœºå™¨å­¦ä¹ å»ºæ¨¡)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. æœºå™¨å­¦ä¹ æ¦‚è¿°](#1-æœºå™¨å­¦ä¹ æ¦‚è¿°)
    - [1.1 æœºå™¨å­¦ä¹ ç±»å‹](#11-æœºå™¨å­¦ä¹ ç±»å‹)
    - [1.2 æ•°æ®é¢„å¤„ç†](#12-æ•°æ®é¢„å¤„ç†)
  - [2. çº¿æ€§å›å½’](#2-çº¿æ€§å›å½’)
    - [2.1 ç®€å•çº¿æ€§å›å½’](#21-ç®€å•çº¿æ€§å›å½’)
    - [2.2 å¤šé¡¹å¼å›å½’](#22-å¤šé¡¹å¼å›å½’)
  - [3. é€»è¾‘å›å½’ä¸åˆ†ç±»](#3-é€»è¾‘å›å½’ä¸åˆ†ç±»)
    - [3.1 Sigmoid å‡½æ•°](#31-sigmoid-å‡½æ•°)
  - [4. ç¥ç»ç½‘ç»œåŸºç¡€](#4-ç¥ç»ç½‘ç»œåŸºç¡€)
    - [4.1 ç®€å•ç¥ç»ç½‘ç»œ](#41-ç®€å•ç¥ç»ç½‘ç»œ)
  - [5. æ¨¡å‹è¯„ä¼°](#5-æ¨¡å‹è¯„ä¼°)
    - [5.1 è¯„ä¼°æŒ‡æ ‡](#51-è¯„ä¼°æŒ‡æ ‡)
    - [5.2 äº¤å‰éªŒè¯](#52-äº¤å‰éªŒè¯)
  - [6. å®æˆ˜æ¡ˆä¾‹](#6-å®æˆ˜æ¡ˆä¾‹)
    - [6.1 æˆ¿ä»·é¢„æµ‹](#61-æˆ¿ä»·é¢„æµ‹)
    - [6.2 åƒåœ¾é‚®ä»¶åˆ†ç±»](#62-åƒåœ¾é‚®ä»¶åˆ†ç±»)
  - [7. æ€»ç»“](#7-æ€»ç»“)
    - [æ ¸å¿ƒè¦ç‚¹](#æ ¸å¿ƒè¦ç‚¹)
    - [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## 1. æœºå™¨å­¦ä¹ æ¦‚è¿°

### 1.1 æœºå™¨å­¦ä¹ ç±»å‹

```rust
enum LearningType {
    Supervised,    // ç›‘ç£å­¦ä¹ 
    Unsupervised,  // æ— ç›‘ç£å­¦ä¹ 
    Reinforcement, // å¼ºåŒ–å­¦ä¹ 
}

struct MLModel {
    learning_type: LearningType,
    parameters: Vec<f64>,
}

impl MLModel {
    fn new(learning_type: LearningType) -> Self {
        Self {
            learning_type,
            parameters: Vec::new(),
        }
    }
}

fn main() {
    let model = MLModel::new(LearningType::Supervised);
    println!("åˆ›å»ºç›‘ç£å­¦ä¹ æ¨¡å‹");
}
```

### 1.2 æ•°æ®é¢„å¤„ç†

```rust
struct Dataset {
    features: Vec<Vec<f64>>,
    labels: Vec<f64>,
}

impl Dataset {
    // æ ‡å‡†åŒ–
    fn normalize(&mut self) {
        let n_features = self.features[0].len();
        
        for j in 0..n_features {
            // è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            let values: Vec<f64> = self.features.iter().map(|x| x[j]).collect();
            let mean = values.iter().sum::<f64>() / values.len() as f64;
            let variance = values.iter().map(|x| (x - mean).powi(2)).sum::<f64>() / values.len() as f64;
            let std_dev = variance.sqrt();
            
            // æ ‡å‡†åŒ–
            for i in 0..self.features.len() {
                self.features[i][j] = (self.features[i][j] - mean) / std_dev;
            }
        }
    }
    
    // åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    fn split(&self, train_ratio: f64) -> (Dataset, Dataset) {
        let train_size = (self.features.len() as f64 * train_ratio) as usize;
        
        let train = Dataset {
            features: self.features[..train_size].to_vec(),
            labels: self.labels[..train_size].to_vec(),
        };
        
        let test = Dataset {
            features: self.features[train_size..].to_vec(),
            labels: self.labels[train_size..].to_vec(),
        };
        
        (train, test)
    }
}

fn main() {
    let mut dataset = Dataset {
        features: vec![
            vec![1.0, 2.0],
            vec![2.0, 3.0],
            vec![3.0, 4.0],
            vec![4.0, 5.0],
        ],
        labels: vec![1.0, 2.0, 3.0, 4.0],
    };
    
    dataset.normalize();
    println!("æ ‡å‡†åŒ–å: {:?}", dataset.features);
    
    let (train, test) = dataset.split(0.8);
    println!("è®­ç»ƒé›†: {}, æµ‹è¯•é›†: {}", train.features.len(), test.features.len());
}
```

---

## 2. çº¿æ€§å›å½’

### 2.1 ç®€å•çº¿æ€§å›å½’

```rust
struct LinearRegression {
    weights: Vec<f64>,
    bias: f64,
    learning_rate: f64,
}

impl LinearRegression {
    fn new(n_features: usize, learning_rate: f64) -> Self {
        Self {
            weights: vec![0.0; n_features],
            bias: 0.0,
            learning_rate,
        }
    }
    
    // é¢„æµ‹
    fn predict(&self, features: &[f64]) -> f64 {
        let mut prediction = self.bias;
        for (w, x) in self.weights.iter().zip(features.iter()) {
            prediction += w * x;
        }
        prediction
    }
    
    // è®­ç»ƒï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰
    fn train(&mut self, dataset: &Dataset, epochs: usize) {
        let n = dataset.features.len() as f64;
        
        for epoch in 0..epochs {
            let mut total_loss = 0.0;
            
            for (features, &label) in dataset.features.iter().zip(dataset.labels.iter()) {
                // å‰å‘ä¼ æ’­
                let prediction = self.predict(features);
                let error = prediction - label;
                total_loss += error * error;
                
                // åå‘ä¼ æ’­
                for (w, &x) in self.weights.iter_mut().zip(features.iter()) {
                    *w -= self.learning_rate * (2.0 / n) * error * x;
                }
                self.bias -= self.learning_rate * (2.0 / n) * error;
            }
            
            if epoch % 100 == 0 {
                println!("Epoch {}: Loss = {}", epoch, total_loss / n);
            }
        }
    }
}

fn main() {
    let dataset = Dataset {
        features: vec![
            vec![1.0],
            vec![2.0],
            vec![3.0],
            vec![4.0],
        ],
        labels: vec![2.0, 4.0, 6.0, 8.0],
    };
    
    let mut model = LinearRegression::new(1, 0.01);
    model.train(&dataset, 1000);
    
    // é¢„æµ‹
    let prediction = model.predict(&[5.0]);
    println!("é¢„æµ‹ x=5: {}", prediction);
    println!("æƒé‡: {:?}, åç½®: {}", model.weights, model.bias);
}
```

### 2.2 å¤šé¡¹å¼å›å½’

```rust
fn polynomial_features(x: f64, degree: usize) -> Vec<f64> {
    (0..=degree).map(|d| x.powi(d as i32)).collect()
}

fn polynomial_regression_demo() {
    let x_data = vec![1.0, 2.0, 3.0, 4.0, 5.0];
    let y_data = vec![2.1, 4.2, 5.9, 8.1, 10.2];
    
    // è½¬æ¢ä¸ºå¤šé¡¹å¼ç‰¹å¾ï¼ˆdegree=2ï¼‰
    let features: Vec<Vec<f64>> = x_data.iter()
        .map(|&x| polynomial_features(x, 2))
        .collect();
    
    let dataset = Dataset { features, labels: y_data };
    
    let mut model = LinearRegression::new(3, 0.001);
    model.train(&dataset, 1000);
    
    println!("å¤šé¡¹å¼å›å½’å®Œæˆ");
}

fn main() {
    polynomial_regression_demo();
}
```

---

## 3. é€»è¾‘å›å½’ä¸åˆ†ç±»

### 3.1 Sigmoid å‡½æ•°

```rust
fn sigmoid(x: f64) -> f64 {
    1.0 / (1.0 + (-x).exp())
}

struct LogisticRegression {
    weights: Vec<f64>,
    bias: f64,
    learning_rate: f64,
}

impl LogisticRegression {
    fn new(n_features: usize, learning_rate: f64) -> Self {
        Self {
            weights: vec![0.0; n_features],
            bias: 0.0,
            learning_rate,
        }
    }
    
    fn predict_proba(&self, features: &[f64]) -> f64 {
        let mut z = self.bias;
        for (w, x) in self.weights.iter().zip(features.iter()) {
            z += w * x;
        }
        sigmoid(z)
    }
    
    fn predict(&self, features: &[f64]) -> u8 {
        if self.predict_proba(features) >= 0.5 { 1 } else { 0 }
    }
    
    fn train(&mut self, dataset: &Dataset, epochs: usize) {
        let n = dataset.features.len() as f64;
        
        for epoch in 0..epochs {
            for (features, &label) in dataset.features.iter().zip(dataset.labels.iter()) {
                let prediction = self.predict_proba(features);
                let error = prediction - label;
                
                // æ›´æ–°æƒé‡
                for (w, &x) in self.weights.iter_mut().zip(features.iter()) {
                    *w -= self.learning_rate * error * x / n;
                }
                self.bias -= self.learning_rate * error / n;
            }
            
            if epoch % 1000 == 0 {
                let accuracy = self.evaluate(dataset);
                println!("Epoch {}: Accuracy = {:.2}%", epoch, accuracy * 100.0);
            }
        }
    }
    
    fn evaluate(&self, dataset: &Dataset) -> f64 {
        let correct = dataset.features.iter()
            .zip(dataset.labels.iter())
            .filter(|(features, &label)| {
                self.predict(features) == label as u8
            })
            .count();
        
        correct as f64 / dataset.features.len() as f64
    }
}

fn main() {
    // äºŒåˆ†ç±»æ•°æ®
    let dataset = Dataset {
        features: vec![
            vec![1.0, 2.0],
            vec![2.0, 3.0],
            vec![3.0, 1.0],
            vec![6.0, 5.0],
            vec![7.0, 8.0],
            vec![8.0, 7.0],
        ],
        labels: vec![0.0, 0.0, 0.0, 1.0, 1.0, 1.0],
    };
    
    let mut model = LogisticRegression::new(2, 0.1);
    model.train(&dataset, 5000);
    
    // é¢„æµ‹
    let test_sample = vec![5.0, 4.0];
    println!("é¢„æµ‹ç±»åˆ«: {}", model.predict(&test_sample));
}
```

---

## 4. ç¥ç»ç½‘ç»œåŸºç¡€

### 4.1 ç®€å•ç¥ç»ç½‘ç»œ

```rust
fn relu(x: f64) -> f64 {
    x.max(0.0)
}

fn relu_derivative(x: f64) -> f64 {
    if x > 0.0 { 1.0 } else { 0.0 }
}

struct NeuralNetwork {
    // æƒé‡çŸ©é˜µ
    w1: Vec<Vec<f64>>, // è¾“å…¥å±‚åˆ°éšè—å±‚
    b1: Vec<f64>,
    w2: Vec<Vec<f64>>, // éšè—å±‚åˆ°è¾“å‡ºå±‚
    b2: Vec<f64>,
    learning_rate: f64,
}

impl NeuralNetwork {
    fn new(input_size: usize, hidden_size: usize, output_size: usize, learning_rate: f64) -> Self {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        
        let w1 = (0..hidden_size)
            .map(|_| (0..input_size).map(|_| rng.gen_range(-0.5..0.5)).collect())
            .collect();
        
        let w2 = (0..output_size)
            .map(|_| (0..hidden_size).map(|_| rng.gen_range(-0.5..0.5)).collect())
            .collect();
        
        Self {
            w1,
            b1: vec![0.0; hidden_size],
            w2,
            b2: vec![0.0; output_size],
            learning_rate,
        }
    }
    
    fn forward(&self, input: &[f64]) -> (Vec<f64>, Vec<f64>) {
        // éšè—å±‚
        let mut hidden = vec![0.0; self.b1.len()];
        for i in 0..self.b1.len() {
            let mut sum = self.b1[i];
            for j in 0..input.len() {
                sum += self.w1[i][j] * input[j];
            }
            hidden[i] = relu(sum);
        }
        
        // è¾“å‡ºå±‚
        let mut output = vec![0.0; self.b2.len()];
        for i in 0..self.b2.len() {
            let mut sum = self.b2[i];
            for j in 0..hidden.len() {
                sum += self.w2[i][j] * hidden[j];
            }
            output[i] = sum; // å›å½’ä»»åŠ¡ä¸éœ€è¦æ¿€æ´»å‡½æ•°
        }
        
        (hidden, output)
    }
    
    fn train(&mut self, dataset: &Dataset, epochs: usize) {
        for epoch in 0..epochs {
            let mut total_loss = 0.0;
            
            for (features, &label) in dataset.features.iter().zip(dataset.labels.iter()) {
                let (hidden, output) = self.forward(features);
                let error = output[0] - label;
                total_loss += error * error;
                
                // åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰
                // è¾“å‡ºå±‚æ¢¯åº¦
                let output_grad = 2.0 * error;
                
                // æ›´æ–° w2 å’Œ b2
                for i in 0..self.w2.len() {
                    for j in 0..self.w2[i].len() {
                        self.w2[i][j] -= self.learning_rate * output_grad * hidden[j];
                    }
                    self.b2[i] -= self.learning_rate * output_grad;
                }
            }
            
            if epoch % 100 == 0 {
                println!("Epoch {}: Loss = {}", epoch, total_loss / dataset.features.len() as f64);
            }
        }
    }
}

fn main() {
    let dataset = Dataset {
        features: vec![
            vec![0.0, 0.0],
            vec![0.0, 1.0],
            vec![1.0, 0.0],
            vec![1.0, 1.0],
        ],
        labels: vec![0.0, 1.0, 1.0, 0.0], // XOR
    };
    
    let mut nn = NeuralNetwork::new(2, 4, 1, 0.1);
    nn.train(&dataset, 1000);
    
    println!("ç¥ç»ç½‘ç»œè®­ç»ƒå®Œæˆ");
}
```

---

## 5. æ¨¡å‹è¯„ä¼°

### 5.1 è¯„ä¼°æŒ‡æ ‡

```rust
struct ClassificationMetrics {
    accuracy: f64,
    precision: f64,
    recall: f64,
    f1_score: f64,
}

fn calculate_metrics(y_true: &[u8], y_pred: &[u8]) -> ClassificationMetrics {
    let mut tp = 0;
    let mut fp = 0;
    let mut fn_count = 0;
    let mut tn = 0;
    
    for (&true_label, &pred_label) in y_true.iter().zip(y_pred.iter()) {
        match (true_label, pred_label) {
            (1, 1) => tp += 1,
            (0, 1) => fp += 1,
            (1, 0) => fn_count += 1,
            (0, 0) => tn += 1,
            _ => {}
        }
    }
    
    let accuracy = (tp + tn) as f64 / y_true.len() as f64;
    let precision = tp as f64 / (tp + fp) as f64;
    let recall = tp as f64 / (tp + fn_count) as f64;
    let f1_score = 2.0 * (precision * recall) / (precision + recall);
    
    ClassificationMetrics { accuracy, precision, recall, f1_score }
}

fn main() {
    let y_true = vec![1, 0, 1, 1, 0, 1, 0, 0];
    let y_pred = vec![1, 0, 1, 0, 0, 1, 0, 1];
    
    let metrics = calculate_metrics(&y_true, &y_pred);
    println!("å‡†ç¡®ç‡: {:.2}", metrics.accuracy);
    println!("ç²¾ç¡®ç‡: {:.2}", metrics.precision);
    println!("å¬å›ç‡: {:.2}", metrics.recall);
    println!("F1 åˆ†æ•°: {:.2}", metrics.f1_score);
}
```

### 5.2 äº¤å‰éªŒè¯

```rust
fn k_fold_cross_validation(dataset: &Dataset, k: usize) -> Vec<f64> {
    let fold_size = dataset.features.len() / k;
    let mut scores = Vec::new();
    
    for i in 0..k {
        // åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        let val_start = i * fold_size;
        let val_end = (i + 1) * fold_size;
        
        let train_features = [
            &dataset.features[..val_start],
            &dataset.features[val_end..],
        ].concat();
        let train_labels = [
            &dataset.labels[..val_start],
            &dataset.labels[val_end..],
        ].concat();
        
        let val_features = dataset.features[val_start..val_end].to_vec();
        let val_labels = dataset.labels[val_start..val_end].to_vec();
        
        // è®­ç»ƒæ¨¡å‹
        let train_set = Dataset { features: train_features, labels: train_labels };
        let mut model = LinearRegression::new(dataset.features[0].len(), 0.01);
        model.train(&train_set, 500);
        
        // è¯„ä¼°
        let mut mse = 0.0;
        for (features, &label) in val_features.iter().zip(val_labels.iter()) {
            let prediction = model.predict(features);
            mse += (prediction - label).powi(2);
        }
        mse /= val_features.len() as f64;
        
        scores.push(mse);
        println!("Fold {}: MSE = {}", i + 1, mse);
    }
    
    scores
}

fn main() {
    let dataset = Dataset {
        features: vec![
            vec![1.0], vec![2.0], vec![3.0], vec![4.0],
            vec![5.0], vec![6.0], vec![7.0], vec![8.0],
        ],
        labels: vec![2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0],
    };
    
    let scores = k_fold_cross_validation(&dataset, 4);
    let avg_score = scores.iter().sum::<f64>() / scores.len() as f64;
    println!("å¹³å‡ MSE: {}", avg_score);
}
```

---

## 6. å®æˆ˜æ¡ˆä¾‹

### 6.1 æˆ¿ä»·é¢„æµ‹

```rust
fn house_price_prediction() {
    // ç‰¹å¾: [é¢ç§¯, å§å®¤æ•°, æ¥¼å±‚]
    let dataset = Dataset {
        features: vec![
            vec![50.0, 1.0, 1.0],
            vec![80.0, 2.0, 3.0],
            vec![120.0, 3.0, 5.0],
            vec![150.0, 4.0, 7.0],
            vec![200.0, 5.0, 10.0],
        ],
        labels: vec![150.0, 250.0, 400.0, 520.0, 750.0], // ä»·æ ¼ï¼ˆä¸‡å…ƒï¼‰
    };
    
    let mut model = LinearRegression::new(3, 0.0001);
    model.train(&dataset, 2000);
    
    // é¢„æµ‹æ–°æˆ¿ä»·æ ¼
    let new_house = vec![100.0, 3.0, 4.0];
    let predicted_price = model.predict(&new_house);
    println!("é¢„æµ‹æˆ¿ä»·: {:.2} ä¸‡å…ƒ", predicted_price);
}

fn main() {
    house_price_prediction();
}
```

### 6.2 åƒåœ¾é‚®ä»¶åˆ†ç±»

```rust
fn spam_classification() {
    // ç®€åŒ–ç‰¹å¾: [å…³é”®è¯æ•°é‡, é“¾æ¥æ•°é‡]
    let dataset = Dataset {
        features: vec![
            vec![0.0, 0.0],
            vec![1.0, 0.0],
            vec![5.0, 2.0],
            vec![10.0, 5.0],
            vec![15.0, 8.0],
            vec![20.0, 10.0],
        ],
        labels: vec![0.0, 0.0, 0.0, 1.0, 1.0, 1.0], // 0=æ­£å¸¸, 1=åƒåœ¾
    };
    
    let mut model = LogisticRegression::new(2, 0.1);
    model.train(&dataset, 3000);
    
    // é¢„æµ‹
    let test_email = vec![8.0, 3.0];
    let is_spam = model.predict(&test_email);
    println!("é‚®ä»¶åˆ†ç±»: {}", if is_spam == 1 { "åƒåœ¾é‚®ä»¶" } else { "æ­£å¸¸é‚®ä»¶" });
}

fn main() {
    spam_classification();
}
```

---

## 7. æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

| æ¨¡å‹ | ä»»åŠ¡ç±»å‹ | é€‚ç”¨åœºæ™¯ |
|-----|---------|---------|
| **çº¿æ€§å›å½’** | å›å½’ | è¿ç»­å€¼é¢„æµ‹ |
| **é€»è¾‘å›å½’** | åˆ†ç±» | äºŒåˆ†ç±»é—®é¢˜ |
| **ç¥ç»ç½‘ç»œ** | å›å½’/åˆ†ç±» | å¤æ‚éçº¿æ€§å…³ç³» |

### æœ€ä½³å®è·µ

1. **æ•°æ®é¢„å¤„ç†**: æ ‡å‡†åŒ–/å½’ä¸€åŒ–
2. **ç‰¹å¾å·¥ç¨‹**: å¤šé¡¹å¼ç‰¹å¾ã€äº¤å‰ç‰¹å¾
3. **è¶…å‚æ•°è°ƒä¼˜**: å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°
4. **æ­£åˆ™åŒ–**: é˜²æ­¢è¿‡æ‹Ÿåˆ
5. **äº¤å‰éªŒè¯**: è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›

---

## ğŸ“š å‚è€ƒèµ„æº

- [Rust ML Ecosystem](https://www.arewelearningyet.com/)
- [Machine Learning Basics](https://developers.google.com/machine-learning/crash-course)
- [ndarray Crate](https://docs.rs/ndarray/)

---

**ä¸‹ä¸€æ­¥**: å­¦ä¹  [æ€§èƒ½ä¸é˜Ÿåˆ—æ¨¡å‹](05_æ€§èƒ½ä¸é˜Ÿåˆ—æ¨¡å‹.md)ï¼ŒæŒæ¡æ€§èƒ½åˆ†ææŠ€æœ¯ã€‚
