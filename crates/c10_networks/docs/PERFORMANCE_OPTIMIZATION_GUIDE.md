# C10 Networks æ€§èƒ½ä¼˜åŒ–æŒ‡å—

> é€‚ç”¨èŒƒå›´ï¼šRust 1.90+ï¼ŒTokio 1.35+ã€‚æ–‡æ¡£é£æ ¼éµå¾ª [`STYLE.md`](STYLE.md)ã€‚

## ğŸ“‹ ç›®å½•

- [C10 Networks æ€§èƒ½ä¼˜åŒ–æŒ‡å—](#c10-networks-æ€§èƒ½ä¼˜åŒ–æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
  - [âš¡ å¼‚æ­¥I/Oä¼˜åŒ–](#-å¼‚æ­¥ioä¼˜åŒ–)
    - [Tokioè¿è¡Œæ—¶ä¼˜åŒ–](#tokioè¿è¡Œæ—¶ä¼˜åŒ–)
    - [é›¶æ‹·è´ä¼˜åŒ–](#é›¶æ‹·è´ä¼˜åŒ–)
    - [å¼‚æ­¥æµä¼˜åŒ–](#å¼‚æ­¥æµä¼˜åŒ–)
  - [ğŸ’¾ å†…å­˜ç®¡ç†ä¼˜åŒ–](#-å†…å­˜ç®¡ç†ä¼˜åŒ–)
    - [å†…å­˜æ± ç®¡ç†](#å†…å­˜æ± ç®¡ç†)
    - [å†…å­˜æ˜ å°„ä¼˜åŒ–](#å†…å­˜æ˜ å°„ä¼˜åŒ–)
    - [ç¼“å­˜ä¼˜åŒ–](#ç¼“å­˜ä¼˜åŒ–)
  - [ğŸŒ ç½‘ç»œåè®®ä¼˜åŒ–](#-ç½‘ç»œåè®®ä¼˜åŒ–)
    - [TCPä¼˜åŒ–](#tcpä¼˜åŒ–)
    - [HTTP/2ä¼˜åŒ–](#http2ä¼˜åŒ–)
    - [WebSocketä¼˜åŒ–](#websocketä¼˜åŒ–)
  - [ğŸ”„ å¹¶å‘å¤„ç†ä¼˜åŒ–](#-å¹¶å‘å¤„ç†ä¼˜åŒ–)
    - [å·¥ä½œçªƒå–è°ƒåº¦](#å·¥ä½œçªƒå–è°ƒåº¦)
    - [æ— é”æ•°æ®ç»“æ„](#æ— é”æ•°æ®ç»“æ„)
  - [ğŸ“Š æ€§èƒ½ç›‘æ§](#-æ€§èƒ½ç›‘æ§)
    - [æ€§èƒ½æŒ‡æ ‡æ”¶é›†](#æ€§èƒ½æŒ‡æ ‡æ”¶é›†)
    - [å®æ—¶ç›‘æ§](#å®æ—¶ç›‘æ§)
  - [ğŸ§ª åŸºå‡†æµ‹è¯•](#-åŸºå‡†æµ‹è¯•)
    - [åŸºå‡†æµ‹è¯•æ¡†æ¶](#åŸºå‡†æµ‹è¯•æ¡†æ¶)
    - [å‹åŠ›æµ‹è¯•](#å‹åŠ›æµ‹è¯•)
  - [ğŸ”§ è°ƒä¼˜å·¥å…·](#-è°ƒä¼˜å·¥å…·)
    - [æ€§èƒ½åˆ†æå™¨](#æ€§èƒ½åˆ†æå™¨)
  - [ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡](#-æ€§èƒ½æŒ‡æ ‡)
    - [å…³é”®æ€§èƒ½æŒ‡æ ‡](#å…³é”®æ€§èƒ½æŒ‡æ ‡)

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†C10 Networksæ€§èƒ½ä¼˜åŒ–çš„å…¨é¢æ–¹æ³•ï¼ŒåŒ…æ‹¬å¼‚æ­¥I/Oã€å†…å­˜ç®¡ç†ã€ç½‘ç»œåè®®ã€å¹¶å‘å¤„ç†ç­‰æ–¹é¢çš„ä¼˜åŒ–ç­–ç•¥ã€‚

## âš¡ å¼‚æ­¥I/Oä¼˜åŒ–

### Tokioè¿è¡Œæ—¶ä¼˜åŒ–

```rust
// è‡ªå®šä¹‰Tokioè¿è¡Œæ—¶é…ç½®
pub fn create_optimized_runtime() -> tokio::runtime::Runtime {
    tokio::runtime::Builder::new_multi_thread()
        .worker_threads(num_cpus::get())
        .max_blocking_threads(512)
        .thread_stack_size(3 * 1024 * 1024) // 3MB
        .enable_all()
        .build()
        .unwrap()
}

// å¼‚æ­¥ä»»åŠ¡è°ƒåº¦ä¼˜åŒ–
pub struct OptimizedTaskScheduler {
    cpu_intensive_pool: ThreadPool,
    io_intensive_pool: ThreadPool,
    blocking_pool: ThreadPool,
}

impl OptimizedTaskScheduler {
    pub fn new() -> Self {
        Self {
            cpu_intensive_pool: ThreadPool::new(num_cpus::get()),
            io_intensive_pool: ThreadPool::new(num_cpus::get() * 2),
            blocking_pool: ThreadPool::new(512),
        }
    }
    
    pub async fn spawn_cpu_intensive<F, T>(&self, task: F) -> JoinHandle<T>
    where
        F: FnOnce() -> T + Send + 'static,
        T: Send + 'static,
    {
        self.cpu_intensive_pool.spawn(task)
    }
    
    pub async fn spawn_io_intensive<F, T>(&self, task: F) -> JoinHandle<T>
    where
        F: FnOnce() -> T + Send + 'static,
        T: Send + 'static,
    {
        self.io_intensive_pool.spawn(task)
    }
}
```

### é›¶æ‹·è´ä¼˜åŒ–

```rust
// é›¶æ‹·è´ç¼“å†²åŒº
pub struct ZeroCopyBuffer {
    data: Vec<u8>,
    read_pos: usize,
    write_pos: usize,
    capacity: usize,
}

impl ZeroCopyBuffer {
    pub fn new(capacity: usize) -> Self {
        Self {
            data: vec![0; capacity],
            read_pos: 0,
            write_pos: 0,
            capacity,
        }
    }
    
    // ä½¿ç”¨IoSliceè¿›è¡Œé›¶æ‹·è´å†™å…¥
    pub fn write_vectored(&mut self, slices: &[IoSlice]) -> NetworkResult<usize> {
        let mut total_written = 0;
        
        for slice in slices {
            let available = self.capacity - self.write_pos;
            let to_write = slice.len().min(available);
            
            if to_write > 0 {
                self.data[self.write_pos..self.write_pos + to_write]
                    .copy_from_slice(&slice[..to_write]);
                self.write_pos += to_write;
                total_written += to_write;
            }
            
            if to_write < slice.len() {
                break;
            }
        }
        
        Ok(total_written)
    }
    
    // ä½¿ç”¨Bytesè¿›è¡Œé›¶æ‹·è´è¯»å–
    pub fn read_bytes(&mut self, len: usize) -> NetworkResult<Bytes> {
        let available = self.write_pos - self.read_pos;
        let to_read = len.min(available);
        
        if to_read > 0 {
            let data = Bytes::copy_from_slice(&self.data[self.read_pos..self.read_pos + to_read]);
            self.read_pos += to_read;
            Ok(data)
        } else {
            Ok(Bytes::new())
        }
    }
}
```

### å¼‚æ­¥æµä¼˜åŒ–

```rust
// é«˜æ€§èƒ½å¼‚æ­¥æµ
pub struct OptimizedAsyncStream {
    buffer: Arc<Mutex<VecDeque<Bytes>>>,
    sender: mpsc::UnboundedSender<Bytes>,
    receiver: mpsc::UnboundedReceiver<Bytes>,
    batch_size: usize,
    flush_interval: Duration,
}

impl OptimizedAsyncStream {
    pub fn new(batch_size: usize, flush_interval: Duration) -> Self {
        let (sender, receiver) = mpsc::unbounded_channel();
        
        Self {
            buffer: Arc::new(Mutex::new(VecDeque::new())),
            sender,
            receiver,
            batch_size,
            flush_interval,
        }
    }
    
    // æ‰¹é‡å¤„ç†æ¶ˆæ¯
    pub async fn process_batch(&mut self) -> NetworkResult<Vec<Bytes>> {
        let mut batch = Vec::with_capacity(self.batch_size);
        let timeout = tokio::time::sleep(self.flush_interval);
        
        tokio::select! {
            _ = timeout => {
                // è¶…æ—¶ï¼Œå¤„ç†å½“å‰æ‰¹æ¬¡
            }
            message = self.receiver.recv() => {
                if let Some(msg) = message {
                    batch.push(msg);
                    
                    // ç»§ç»­æ”¶é›†ç›´åˆ°æ‰¹æ¬¡æ»¡
                    while batch.len() < self.batch_size {
                        match self.receiver.try_recv() {
                            Ok(msg) => batch.push(msg),
                            Err(_) => break,
                        }
                    }
                }
            }
        }
        
        Ok(batch)
    }
}
```

## ğŸ’¾ å†…å­˜ç®¡ç†ä¼˜åŒ–

### å†…å­˜æ± ç®¡ç†

```rust
// å¯¹è±¡æ± 
pub struct ObjectPool<T> {
    objects: Arc<Mutex<Vec<T>>>,
    factory: Arc<dyn Fn() -> T + Send + Sync>,
    max_size: usize,
}

impl<T> ObjectPool<T> {
    pub fn new<F>(factory: F, max_size: usize) -> Self
    where
        F: Fn() -> T + Send + Sync + 'static,
    {
        Self {
            objects: Arc::new(Mutex::new(Vec::new())),
            factory: Arc::new(factory),
            max_size,
        }
    }
    
    pub fn get(&self) -> PooledObject<T> {
        let mut objects = self.objects.lock().unwrap();
        
        if let Some(obj) = objects.pop() {
            PooledObject::new(obj, self.objects.clone())
        } else {
            PooledObject::new((self.factory)(), self.objects.clone())
        }
    }
}

// æ± åŒ–å¯¹è±¡
pub struct PooledObject<T> {
    object: Option<T>,
    pool: Arc<Mutex<Vec<T>>>,
}

impl<T> PooledObject<T> {
    fn new(object: T, pool: Arc<Mutex<Vec<T>>>) -> Self {
        Self {
            object: Some(object),
            pool,
        }
    }
    
    pub fn as_ref(&self) -> &T {
        self.object.as_ref().unwrap()
    }
    
    pub fn as_mut(&mut self) -> &mut T {
        self.object.as_mut().unwrap()
    }
}

impl<T> Drop for PooledObject<T> {
    fn drop(&mut self) {
        if let Some(obj) = self.object.take() {
            let mut pool = self.pool.lock().unwrap();
            if pool.len() < 1000 { // é™åˆ¶æ± å¤§å°
                pool.push(obj);
            }
        }
    }
}
```

### å†…å­˜æ˜ å°„ä¼˜åŒ–

```rust
// å†…å­˜æ˜ å°„æ–‡ä»¶
pub struct MemoryMappedFile {
    file: File,
    mapping: MmapMut,
    size: usize,
}

impl MemoryMappedFile {
    pub fn new(path: &Path, size: usize) -> NetworkResult<Self> {
        let file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .open(path)?;
        
        file.set_len(size as u64)?;
        
        let mapping = unsafe { MmapOptions::new().map_mut(&file)? };
        
        Ok(Self {
            file,
            mapping,
            size,
        })
    }
    
    pub fn as_slice(&self) -> &[u8] {
        &self.mapping[..]
    }
    
    pub fn as_mut_slice(&mut self) -> &mut [u8] {
        &mut self.mapping[..]
    }
    
    // å¼‚æ­¥åˆ·æ–°
    pub async fn flush_async(&self) -> NetworkResult<()> {
        let mapping = self.mapping.clone();
        tokio::task::spawn_blocking(move || {
            mapping.flush()
        }).await??;
        Ok(())
    }
}
```

### ç¼“å­˜ä¼˜åŒ–

```rust
// LRUç¼“å­˜
pub struct LRUCache<K, V> {
    cache: LinkedHashMap<K, V>,
    capacity: usize,
    hits: AtomicUsize,
    misses: AtomicUsize,
}

impl<K, V> LRUCache<K, V>
where
    K: Eq + Hash + Clone,
    V: Clone,
{
    pub fn new(capacity: usize) -> Self {
        Self {
            cache: LinkedHashMap::new(),
            capacity,
            hits: AtomicUsize::new(0),
            misses: AtomicUsize::new(0),
        }
    }
    
    pub fn get(&mut self, key: &K) -> Option<&V> {
        if let Some(value) = self.cache.remove(key) {
            self.cache.insert(key.clone(), value);
            self.hits.fetch_add(1, Ordering::Relaxed);
            self.cache.get(key)
        } else {
            self.misses.fetch_add(1, Ordering::Relaxed);
            None
        }
    }
    
    pub fn put(&mut self, key: K, value: V) {
        if self.cache.contains_key(&key) {
            self.cache.remove(&key);
        } else if self.cache.len() >= self.capacity {
            self.cache.pop_front();
        }
        
        self.cache.insert(key, value);
    }
    
    pub fn hit_rate(&self) -> f64 {
        let hits = self.hits.load(Ordering::Relaxed);
        let misses = self.misses.load(Ordering::Relaxed);
        let total = hits + misses;
        
        if total == 0 {
            0.0
        } else {
            hits as f64 / total as f64
        }
    }
}
```

## ğŸŒ ç½‘ç»œåè®®ä¼˜åŒ–

### TCPä¼˜åŒ–

```rust
// TCPä¼˜åŒ–é…ç½®
pub struct TcpOptimization {
    nagle_algorithm: bool,
    tcp_nodelay: bool,
    keep_alive: bool,
    window_scaling: bool,
    congestion_control: CongestionControl,
}

pub enum CongestionControl {
    Reno,
    Cubic,
    BBR,
}

impl TcpOptimization {
    pub fn apply_to_socket(&self, socket: &TcpStream) -> NetworkResult<()> {
        // ç¦ç”¨Nagleç®—æ³•
        if self.tcp_nodelay {
            socket.set_nodelay(true)?;
        }
        
        // å¯ç”¨keep-alive
        if self.keep_alive {
            socket.set_keepalive(true)?;
        }
        
        // è®¾ç½®ç¼“å†²åŒºå¤§å°
        socket.set_send_buffer_size(1024 * 1024)?; // 1MB
        socket.set_recv_buffer_size(1024 * 1024)?;  // 1MB
        
        Ok(())
    }
}
```

### HTTP/2ä¼˜åŒ–

```rust
// HTTP/2ä¼˜åŒ–
pub struct Http2Optimization {
    max_concurrent_streams: u32,
    initial_window_size: u32,
    max_frame_size: u32,
    header_table_size: u32,
    enable_push: bool,
}

impl Http2Optimization {
    pub fn new() -> Self {
        Self {
            max_concurrent_streams: 100,
            initial_window_size: 64 * 1024, // 64KB
            max_frame_size: 16 * 1024,      // 16KB
            header_table_size: 4 * 1024,    // 4KB
            enable_push: false,
        }
    }
    
    pub fn apply_to_client(&self, client: &mut HttpClient) {
        client.set_max_concurrent_streams(self.max_concurrent_streams);
        client.set_initial_window_size(self.initial_window_size);
        client.set_max_frame_size(self.max_frame_size);
        client.set_header_table_size(self.header_table_size);
        client.set_enable_push(self.enable_push);
    }
}
```

### WebSocketä¼˜åŒ–

```rust
// WebSocketä¼˜åŒ–
pub struct WebSocketOptimization {
    compression_enabled: bool,
    max_message_size: usize,
    ping_interval: Duration,
    pong_timeout: Duration,
    close_timeout: Duration,
}

impl WebSocketOptimization {
    pub fn new() -> Self {
        Self {
            compression_enabled: true,
            max_message_size: 16 * 1024 * 1024, // 16MB
            ping_interval: Duration::from_secs(30),
            pong_timeout: Duration::from_secs(10),
            close_timeout: Duration::from_secs(5),
        }
    }
    
    pub fn apply_to_server(&self, server: &mut WebSocketServer) {
        server.set_compression_enabled(self.compression_enabled);
        server.set_max_message_size(self.max_message_size);
        server.set_ping_interval(self.ping_interval);
        server.set_pong_timeout(self.pong_timeout);
        server.set_close_timeout(self.close_timeout);
    }
}
```

## ğŸ”„ å¹¶å‘å¤„ç†ä¼˜åŒ–

### å·¥ä½œçªƒå–è°ƒåº¦

```rust
// å·¥ä½œçªƒå–è°ƒåº¦å™¨
pub struct WorkStealingScheduler {
    workers: Vec<Worker>,
    global_queue: Arc<Mutex<VecDeque<Task>>>,
}

struct Worker {
    id: usize,
    local_queue: VecDeque<Task>,
    steal_count: AtomicUsize,
}

impl WorkStealingScheduler {
    pub fn new(num_workers: usize) -> Self {
        let workers = (0..num_workers)
            .map(|id| Worker {
                id,
                local_queue: VecDeque::new(),
                steal_count: AtomicUsize::new(0),
            })
            .collect();
        
        Self {
            workers,
            global_queue: Arc::new(Mutex::new(VecDeque::new())),
        }
    }
    
    pub fn schedule(&self, task: Task) {
        // å°è¯•æ”¾å…¥æœ¬åœ°é˜Ÿåˆ—
        let worker_id = thread_id::get() % self.workers.len();
        let worker = &self.workers[worker_id];
        
        if worker.local_queue.len() < 1000 {
            worker.local_queue.push_back(task);
        } else {
            // æ”¾å…¥å…¨å±€é˜Ÿåˆ—
            self.global_queue.lock().unwrap().push_back(task);
        }
    }
    
    pub fn steal(&self, worker_id: usize) -> Option<Task> {
        let worker = &self.workers[worker_id];
        
        // å°è¯•ä»å…¶ä»–workerçªƒå–
        for i in 0..self.workers.len() {
            if i != worker_id {
                let other_worker = &self.workers[i];
                if let Some(task) = other_worker.local_queue.pop_back() {
                    worker.steal_count.fetch_add(1, Ordering::Relaxed);
                    return Some(task);
                }
            }
        }
        
        // å°è¯•ä»å…¨å±€é˜Ÿåˆ—è·å–
        self.global_queue.lock().unwrap().pop_front()
    }
}
```

### æ— é”æ•°æ®ç»“æ„

```rust
// æ— é”é˜Ÿåˆ—
pub struct LockFreeQueue<T> {
    head: AtomicPtr<Node<T>>,
    tail: AtomicPtr<Node<T>>,
}

struct Node<T> {
    data: Option<T>,
    next: AtomicPtr<Node<T>>,
}

impl<T> LockFreeQueue<T> {
    pub fn new() -> Self {
        let dummy = Box::new(Node {
            data: None,
            next: AtomicPtr::new(ptr::null_mut()),
        });
        let dummy_ptr = Box::into_raw(dummy);
        
        Self {
            head: AtomicPtr::new(dummy_ptr),
            tail: AtomicPtr::new(dummy_ptr),
        }
    }
    
    pub fn enqueue(&self, value: T) {
        let new_node = Box::new(Node {
            data: Some(value),
            next: AtomicPtr::new(ptr::null_mut()),
        });
        let new_ptr = Box::into_raw(new_node);
        
        loop {
            let tail = self.tail.load(Ordering::Acquire);
            let next = unsafe { (*tail).next.load(Ordering::Acquire) };
            
            if next.is_null() {
                if unsafe { (*tail).next.compare_exchange_weak(
                    ptr::null_mut(),
                    new_ptr,
                    Ordering::Release,
                    Ordering::Relaxed,
                ).is_ok() } {
                    self.tail.compare_exchange_weak(
                        tail,
                        new_ptr,
                        Ordering::Release,
                        Ordering::Relaxed,
                    ).ok();
                    break;
                }
            } else {
                self.tail.compare_exchange_weak(
                    tail,
                    next,
                    Ordering::Release,
                    Ordering::Relaxed,
                ).ok();
            }
        }
    }
    
    pub fn dequeue(&self) -> Option<T> {
        loop {
            let head = self.head.load(Ordering::Acquire);
            let tail = self.tail.load(Ordering::Acquire);
            let next = unsafe { (*head).next.load(Ordering::Acquire) };
            
            if head == tail {
                if next.is_null() {
                    return None;
                }
                self.tail.compare_exchange_weak(
                    tail,
                    next,
                    Ordering::Release,
                    Ordering::Relaxed,
                ).ok();
            } else {
                if let Some(next) = NonNull::new(next) {
                    let data = unsafe { (*next.as_ptr()).data.take() };
                    self.head.compare_exchange_weak(
                        head,
                        next.as_ptr(),
                        Ordering::Release,
                        Ordering::Relaxed,
                    ).ok();
                    return data;
                }
            }
        }
    }
}
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```rust
// æ€§èƒ½æŒ‡æ ‡
pub struct PerformanceMetrics {
    request_count: AtomicUsize,
    response_time: AtomicU64,
    error_count: AtomicUsize,
    throughput: AtomicU64,
    memory_usage: AtomicUsize,
    cpu_usage: AtomicF64,
}

impl PerformanceMetrics {
    pub fn new() -> Self {
        Self {
            request_count: AtomicUsize::new(0),
            response_time: AtomicU64::new(0),
            error_count: AtomicUsize::new(0),
            throughput: AtomicU64::new(0),
            memory_usage: AtomicUsize::new(0),
            cpu_usage: AtomicF64::new(0.0),
        }
    }
    
    pub fn record_request(&self, response_time: Duration) {
        self.request_count.fetch_add(1, Ordering::Relaxed);
        self.response_time.fetch_add(response_time.as_nanos() as u64, Ordering::Relaxed);
    }
    
    pub fn record_error(&self) {
        self.error_count.fetch_add(1, Ordering::Relaxed);
    }
    
    pub fn update_throughput(&self, bytes: u64) {
        self.throughput.fetch_add(bytes, Ordering::Relaxed);
    }
    
    pub fn get_average_response_time(&self) -> Duration {
        let count = self.request_count.load(Ordering::Relaxed);
        if count > 0 {
            let total_nanos = self.response_time.load(Ordering::Relaxed);
            Duration::from_nanos(total_nanos / count as u64)
        } else {
            Duration::ZERO
        }
    }
    
    pub fn get_error_rate(&self) -> f64 {
        let errors = self.error_count.load(Ordering::Relaxed);
        let requests = self.request_count.load(Ordering::Relaxed);
        
        if requests > 0 {
            errors as f64 / requests as f64
        } else {
            0.0
        }
    }
}
```

### å®æ—¶ç›‘æ§

```rust
// å®æ—¶ç›‘æ§å™¨
pub struct RealTimeMonitor {
    metrics: Arc<PerformanceMetrics>,
    interval: Duration,
}

impl RealTimeMonitor {
    pub fn new(metrics: Arc<PerformanceMetrics>, interval: Duration) -> Self {
        Self { metrics, interval }
    }
    
    pub async fn start(&self) -> NetworkResult<()> {
        let mut interval = tokio::time::interval(self.interval);
        
        loop {
            interval.tick().await;
            
            // æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
            let memory_usage = self.get_memory_usage();
            let cpu_usage = self.get_cpu_usage();
            
            // æ›´æ–°æŒ‡æ ‡
            self.metrics.memory_usage.store(memory_usage, Ordering::Relaxed);
            self.metrics.cpu_usage.store(cpu_usage, Ordering::Relaxed);
            
            // è¾“å‡ºç›‘æ§ä¿¡æ¯
            self.print_metrics();
        }
    }
    
    fn print_metrics(&self) {
        println!("=== æ€§èƒ½ç›‘æ§æŠ¥å‘Š ===");
        println!("è¯·æ±‚æ•°: {}", self.metrics.request_count.load(Ordering::Relaxed));
        println!("å¹³å‡å“åº”æ—¶é—´: {:?}", self.metrics.get_average_response_time());
        println!("é”™è¯¯ç‡: {:.2}%", self.metrics.get_error_rate() * 100.0);
        println!("ååé‡: {} bytes/s", self.metrics.throughput.load(Ordering::Relaxed));
        println!("å†…å­˜ä½¿ç”¨: {} MB", self.metrics.memory_usage.load(Ordering::Relaxed) / 1024 / 1024);
        println!("CPUä½¿ç”¨ç‡: {:.2}%", self.metrics.cpu_usage.load(Ordering::Relaxed) * 100.0);
    }
}
```

## ğŸ§ª åŸºå‡†æµ‹è¯•

### åŸºå‡†æµ‹è¯•æ¡†æ¶

```rust
// åŸºå‡†æµ‹è¯•
pub struct Benchmark {
    name: String,
    iterations: usize,
    warmup_iterations: usize,
}

impl Benchmark {
    pub fn new(name: String) -> Self {
        Self {
            name,
            iterations: 1000,
            warmup_iterations: 100,
        }
    }
    
    pub async fn run<F, Fut>(&self, test_fn: F) -> BenchmarkResult
    where
        F: Fn() -> Fut,
        Fut: Future<Output = NetworkResult<()>>,
    {
        let mut times = Vec::new();
        
        // é¢„çƒ­
        for _ in 0..self.warmup_iterations {
            let _ = test_fn().await;
        }
        
        // æ­£å¼æµ‹è¯•
        for _ in 0..self.iterations {
            let start = Instant::now();
            let _ = test_fn().await;
            let duration = start.elapsed();
            times.push(duration);
        }
        
        // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        times.sort();
        let min = times[0];
        let max = times[times.len() - 1];
        let median = times[times.len() / 2];
        let mean = times.iter().sum::<Duration>() / times.len() as u32;
        
        BenchmarkResult {
            name: self.name.clone(),
            min,
            max,
            median,
            mean,
            iterations: self.iterations,
        }
    }
}

pub struct BenchmarkResult {
    name: String,
    min: Duration,
    max: Duration,
    median: Duration,
    mean: Duration,
    iterations: usize,
}
```

### å‹åŠ›æµ‹è¯•

```rust
// å‹åŠ›æµ‹è¯•
pub struct StressTest {
    concurrent_users: usize,
    duration: Duration,
    ramp_up_time: Duration,
}

impl StressTest {
    pub fn new(concurrent_users: usize, duration: Duration) -> Self {
        Self {
            concurrent_users,
            duration,
            ramp_up_time: Duration::from_secs(10),
        }
    }
    
    pub async fn run<F, Fut>(&self, test_fn: F) -> StressTestResult
    where
        F: Fn() -> Fut + Send + Sync + 'static,
        Fut: Future<Output = NetworkResult<()>> + Send,
    {
        let start_time = Instant::now();
        let mut handles = Vec::new();
        let metrics = Arc::new(PerformanceMetrics::new());
        
        // é€æ­¥å¢åŠ å¹¶å‘ç”¨æˆ·
        let ramp_up_interval = self.ramp_up_time / self.concurrent_users as u32;
        
        for user_id in 0..self.concurrent_users {
            let metrics = metrics.clone();
            let test_fn = &test_fn;
            
            let handle = tokio::spawn(async move {
                // ç­‰å¾…è½®åˆ°è¯¥ç”¨æˆ·å¯åŠ¨
                tokio::time::sleep(ramp_up_interval * user_id as u32).await;
                
                let mut request_count = 0;
                let mut error_count = 0;
                
                while start_time.elapsed() < self.duration {
                    let request_start = Instant::now();
                    
                    match test_fn().await {
                        Ok(_) => {
                            request_count += 1;
                            metrics.record_request(request_start.elapsed());
                        }
                        Err(_) => {
                            error_count += 1;
                            metrics.record_error();
                        }
                    }
                }
                
                (request_count, error_count)
            });
            
            handles.push(handle);
        }
        
        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        let mut total_requests = 0;
        let mut total_errors = 0;
        
        for handle in handles {
            let (requests, errors) = handle.await.unwrap();
            total_requests += requests;
            total_errors += errors;
        }
        
        StressTestResult {
            duration: self.duration,
            concurrent_users: self.concurrent_users,
            total_requests,
            total_errors,
            average_response_time: metrics.get_average_response_time(),
            error_rate: metrics.get_error_rate(),
            throughput: metrics.throughput.load(Ordering::Relaxed),
        }
    }
}
```

## ğŸ”§ è°ƒä¼˜å·¥å…·

### æ€§èƒ½åˆ†æå™¨

```rust
// æ€§èƒ½åˆ†æå™¨
pub struct Profiler {
    samples: Arc<Mutex<Vec<Sample>>>,
    enabled: AtomicBool,
}

struct Sample {
    timestamp: Instant,
    function: String,
    duration: Duration,
}

impl Profiler {
    pub fn new() -> Self {
        Self {
            samples: Arc::new(Mutex::new(Vec::new())),
            enabled: AtomicBool::new(false),
        }
    }
    
    pub fn enable(&self) {
        self.enabled.store(true, Ordering::Relaxed);
    }
    
    pub fn disable(&self) {
        self.enabled.store(false, Ordering::Relaxed);
    }
    
    pub fn record_sample(&self, function: String, duration: Duration) {
        if self.enabled.load(Ordering::Relaxed) {
            let sample = Sample {
                timestamp: Instant::now(),
                function,
                duration,
            };
            
            self.samples.lock().unwrap().push(sample);
        }
    }
    
    pub fn generate_report(&self) -> String {
        let samples = self.samples.lock().unwrap();
        let mut function_stats: HashMap<String, Vec<Duration>> = HashMap::new();
        
        for sample in samples.iter() {
            function_stats
                .entry(sample.function.clone())
                .or_insert_with(Vec::new)
                .push(sample.duration);
        }
        
        let mut report = String::new();
        report.push_str("=== æ€§èƒ½åˆ†ææŠ¥å‘Š ===\n");
        
        for (function, durations) in function_stats {
            let total_time: Duration = durations.iter().sum();
            let average_time = total_time / durations.len() as u32;
            let min_time = durations.iter().min().unwrap();
            let max_time = durations.iter().max().unwrap();
            
            report.push_str(&format!(
                "å‡½æ•°: {}\n  è°ƒç”¨æ¬¡æ•°: {}\n  æ€»æ—¶é—´: {:?}\n  å¹³å‡æ—¶é—´: {:?}\n  æœ€å°æ—¶é—´: {:?}\n  æœ€å¤§æ—¶é—´: {:?}\n\n",
                function, durations.len(), total_time, average_time, min_time, max_time
            ));
        }
        
        report
    }
}
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æŒ‡æ ‡

```rust
// å…³é”®æ€§èƒ½æŒ‡æ ‡
pub struct KeyPerformanceIndicators {
    // å»¶è¿ŸæŒ‡æ ‡
    p50_latency: Duration,
    p95_latency: Duration,
    p99_latency: Duration,
    
    // ååé‡æŒ‡æ ‡
    requests_per_second: f64,
    bytes_per_second: u64,
    
    // é”™è¯¯æŒ‡æ ‡
    error_rate: f64,
    timeout_rate: f64,
    
    // èµ„æºæŒ‡æ ‡
    cpu_usage: f64,
    memory_usage: usize,
    connection_count: usize,
}

impl KeyPerformanceIndicators {
    pub fn calculate(&self, samples: &[Duration]) -> Self {
        let mut sorted_samples = samples.to_vec();
        sorted_samples.sort();
        
        let len = sorted_samples.len();
        let p50_index = (len as f64 * 0.5) as usize;
        let p95_index = (len as f64 * 0.95) as usize;
        let p99_index = (len as f64 * 0.99) as usize;
        
        Self {
            p50_latency: sorted_samples[p50_index.min(len - 1)],
            p95_latency: sorted_samples[p95_index.min(len - 1)],
            p99_latency: sorted_samples[p99_index.min(len - 1)],
            requests_per_second: 0.0, // éœ€è¦æ ¹æ®å®é™…æµ‹è¯•è®¡ç®—
            bytes_per_second: 0,     // éœ€è¦æ ¹æ®å®é™…æµ‹è¯•è®¡ç®—
            error_rate: 0.0,         // éœ€è¦æ ¹æ®å®é™…æµ‹è¯•è®¡ç®—
            timeout_rate: 0.0,        // éœ€è¦æ ¹æ®å®é™…æµ‹è¯•è®¡ç®—
            cpu_usage: 0.0,          // éœ€è¦æ ¹æ®å®é™…æµ‹è¯•è®¡ç®—
            memory_usage: 0,         // éœ€è¦æ ¹æ®å®é™…æµ‹è¯•è®¡ç®—
            connection_count: 0,     // éœ€è¦æ ¹æ®å®é™…æµ‹è¯•è®¡ç®—
        }
    }
}
```

---

**æ€§èƒ½ä¼˜åŒ–æŒ‡å—ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ  
**ç»´æŠ¤è€…**: C10 Networks æ€§èƒ½å›¢é˜Ÿ
