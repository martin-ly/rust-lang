# C07-07. è¿›ç¨‹ç®¡ç†æ€§èƒ½ä¼˜åŒ–

> **æ–‡æ¡£å®šä½**: Tier 4 é«˜çº§ä¸»é¢˜
> **æœ€åæ›´æ–°**: 2025-12-25
> **Rustç‰ˆæœ¬**: 1.92.0+ (Edition 2024)
> **ç›¸å…³æ–‡æ¡£**: [ä¸»ç´¢å¼•](./00_MASTER_INDEX.md) | [FAQ](./FAQ.md) | [Glossary](./Glossary.md)

## ğŸ“‹ ç›®å½•

- [C07-07. è¿›ç¨‹ç®¡ç†æ€§èƒ½ä¼˜åŒ–](#c07-07-è¿›ç¨‹ç®¡ç†æ€§èƒ½ä¼˜åŒ–)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“ çŸ¥è¯†ç»“æ„](#-çŸ¥è¯†ç»“æ„)
    - [æ¦‚å¿µå®šä¹‰](#æ¦‚å¿µå®šä¹‰)
    - [å±æ€§ç‰¹å¾](#å±æ€§ç‰¹å¾)
    - [å…³ç³»è¿æ¥](#å…³ç³»è¿æ¥)
    - [æ€ç»´å¯¼å›¾](#æ€ç»´å¯¼å›¾)
  - [1. å†…å­˜ç®¡ç†ä¼˜åŒ–](#1-å†…å­˜ç®¡ç†ä¼˜åŒ–)
    - [1.1 é›¶æ‹·è´æ•°æ®ä¼ è¾“](#11-é›¶æ‹·è´æ•°æ®ä¼ è¾“)
    - [1.2 å†…å­˜æ± ç®¡ç†](#12-å†…å­˜æ± ç®¡ç†)
  - [2. CPU ä¼˜åŒ–](#2-cpu-ä¼˜åŒ–)
    - [2.1 CPU äº²å’Œæ€§è®¾ç½®](#21-cpu-äº²å’Œæ€§è®¾ç½®)
    - [2.2 è¿›ç¨‹ä¼˜å…ˆçº§ç®¡ç†](#22-è¿›ç¨‹ä¼˜å…ˆçº§ç®¡ç†)
  - [3. I/O ä¼˜åŒ–](#3-io-ä¼˜åŒ–)
    - [3.1 å¼‚æ­¥ I/O ä¼˜åŒ–](#31-å¼‚æ­¥-io-ä¼˜åŒ–)
    - [3.2 æ–‡ä»¶æè¿°ç¬¦ä¼˜åŒ–](#32-æ–‡ä»¶æè¿°ç¬¦ä¼˜åŒ–)
  - [4. å¹¶å‘ä¼˜åŒ–](#4-å¹¶å‘ä¼˜åŒ–)
    - [4.1 æ— é”æ•°æ®ç»“æ„](#41-æ— é”æ•°æ®ç»“æ„)
    - [4.2 å·¥ä½œçªƒå–è°ƒåº¦å™¨](#42-å·¥ä½œçªƒå–è°ƒåº¦å™¨)
  - [5. æ€§èƒ½ç›‘æ§](#5-æ€§èƒ½ç›‘æ§)
    - [5.1 æ€§èƒ½æŒ‡æ ‡æ”¶é›†](#51-æ€§èƒ½æŒ‡æ ‡æ”¶é›†)
    - [5.2 æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®](#52-æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®)
  - [6. æ€»ç»“](#6-æ€»ç»“)

æœ¬ç« æ·±å…¥æ¢è®¨ Rust è¿›ç¨‹ç®¡ç†çš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬å†…å­˜ç®¡ç†ã€CPU ä¼˜åŒ–ã€I/O ä¼˜åŒ–ã€å¹¶å‘ä¼˜åŒ–å’Œæ€§èƒ½ç›‘æ§ã€‚

---

## ğŸ“ çŸ¥è¯†ç»“æ„

### æ¦‚å¿µå®šä¹‰

**è¿›ç¨‹ç®¡ç†æ€§èƒ½ä¼˜åŒ– (Process Management Performance Optimization)**:

- **å®šä¹‰**: Rust è¿›ç¨‹ç®¡ç†çš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬å†…å­˜ç®¡ç†ã€CPU ä¼˜åŒ–ã€I/O ä¼˜åŒ–ã€å¹¶å‘ä¼˜åŒ–å’Œæ€§èƒ½ç›‘æ§
- **ç±»å‹**: æ€§èƒ½ä¼˜åŒ–æ–‡æ¡£
- **èŒƒç•´**: è¿›ç¨‹ç®¡ç†ã€æ€§èƒ½ä¼˜åŒ–
- **ç‰ˆæœ¬**: Rust 1.92.0+
- **ç›¸å…³æ¦‚å¿µ**: æ€§èƒ½ä¼˜åŒ–ã€å†…å­˜ç®¡ç†ã€CPU ä¼˜åŒ–ã€I/O ä¼˜åŒ–ã€å¹¶å‘ä¼˜åŒ–ã€æ€§èƒ½ç›‘æ§

### å±æ€§ç‰¹å¾

**æ ¸å¿ƒå±æ€§**:

- **å†…å­˜ç®¡ç†ä¼˜åŒ–**: é›¶æ‹·è´æ•°æ®ä¼ è¾“ã€å†…å­˜æ± ç®¡ç†
- **CPU ä¼˜åŒ–**: CPU äº²å’Œæ€§è®¾ç½®ã€è¿›ç¨‹ä¼˜å…ˆçº§ç®¡ç†
- **I/O ä¼˜åŒ–**: å¼‚æ­¥ I/O ä¼˜åŒ–ã€æ–‡ä»¶æè¿°ç¬¦ä¼˜åŒ–
- **å¹¶å‘ä¼˜åŒ–**: æ— é”æ•°æ®ç»“æ„ã€å·¥ä½œçªƒå–è°ƒåº¦å™¨

**æ€§èƒ½ç‰¹å¾**:

- **æ€§èƒ½æå‡**: é€šè¿‡ä¼˜åŒ–æå‡æ€§èƒ½
- **èµ„æºåˆ©ç”¨**: ä¼˜åŒ–èµ„æºåˆ©ç”¨
- **é€‚ç”¨åœºæ™¯**: é«˜æ€§èƒ½è¿›ç¨‹ç®¡ç†ã€æ€§èƒ½å…³é”®åº”ç”¨

### å…³ç³»è¿æ¥

**ç»„åˆå…³ç³»**:

- è¿›ç¨‹ç®¡ç†æ€§èƒ½ä¼˜åŒ– --[covers]--> æ€§èƒ½ä¼˜åŒ–å®Œæ•´å†…å®¹
- é«˜æ€§èƒ½è¿›ç¨‹ç®¡ç† --[uses]--> æ€§èƒ½ä¼˜åŒ–

**ä¾èµ–å…³ç³»**:

- è¿›ç¨‹ç®¡ç†æ€§èƒ½ä¼˜åŒ– --[depends-on]--> è¿›ç¨‹ç®¡ç†
- æ€§èƒ½æå‡ --[depends-on]--> æ€§èƒ½ä¼˜åŒ–

### æ€ç»´å¯¼å›¾

```text
è¿›ç¨‹ç®¡ç†æ€§èƒ½ä¼˜åŒ–
â”‚
â”œâ”€â”€ å†…å­˜ç®¡ç†ä¼˜åŒ–
â”‚   â””â”€â”€ é›¶æ‹·è´ä¼ è¾“
â”œâ”€â”€ CPU ä¼˜åŒ–
â”‚   â””â”€â”€ CPU äº²å’Œæ€§
â”œâ”€â”€ I/O ä¼˜åŒ–
â”‚   â””â”€â”€ å¼‚æ­¥ I/O
â”œâ”€â”€ å¹¶å‘ä¼˜åŒ–
â”‚   â””â”€â”€ æ— é”æ•°æ®ç»“æ„
â””â”€â”€ æ€§èƒ½ç›‘æ§
    â””â”€â”€ æ€§èƒ½æŒ‡æ ‡æ”¶é›†
```

---

## 1. å†…å­˜ç®¡ç†ä¼˜åŒ–

### 1.1 é›¶æ‹·è´æ•°æ®ä¼ è¾“

```rust
use std::io::{BufReader, BufWriter};
use tokio::io::{AsyncBufReadExt, AsyncWriteExt};
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct ZeroCopyProcessManager {
    buffer_pool: Arc<Mutex<BufferPool>>,
    memory_mappings: Arc<Mutex<Vec<MemoryMapping>>>,
}

#[derive(Debug)]
pub struct BufferPool {
    buffers: Vec<Vec<u8>>,
    buffer_size: usize,
    max_buffers: usize,
}

#[derive(Debug)]
pub struct MemoryMapping {
    pub id: String,
    pub data: Arc<Vec<u8>>,
    pub offset: usize,
    pub length: usize,
}

impl ZeroCopyProcessManager {
    pub fn new(buffer_size: usize, max_buffers: usize) -> Self {
        Self {
            buffer_pool: Arc::new(Mutex::new(BufferPool {
                buffers: Vec::new(),
                buffer_size,
                max_buffers,
            })),
            memory_mappings: Arc::new(Mutex::new(Vec::new())),
        }
    }

    pub async fn create_memory_mapping(
        &self,
        data: Vec<u8>,
    ) -> Result<MemoryMapping, Box<dyn std::error::Error>> {
        let mapping_id = uuid::Uuid::new_v4().to_string();
        let data_arc = Arc::new(data);

        let mapping = MemoryMapping {
            id: mapping_id,
            data: data_arc,
            offset: 0,
            length: data.len(),
        };

        let mut mappings = self.memory_mappings.lock().await;
        mappings.push(mapping.clone());

        Ok(mapping)
    }

    pub async fn get_buffer(&self) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
        let mut pool = self.buffer_pool.lock().await;

        if let Some(buffer) = pool.buffers.pop() {
            Ok(buffer)
        } else {
            Ok(vec![0u8; pool.buffer_size])
        }
    }

    pub async fn return_buffer(&self, mut buffer: Vec<u8>) {
        let mut pool = self.buffer_pool.lock().await;

        if pool.buffers.len() < pool.max_buffers {
            buffer.clear();
            pool.buffers.push(buffer);
        }
    }

    pub async fn transfer_data_zero_copy(
        &self,
        source_mapping: &MemoryMapping,
        target_process: &mut tokio::process::Child,
    ) -> Result<(), Box<dyn std::error::Error>> {
        if let Some(stdin) = target_process.stdin.as_mut() {
            let mut writer = BufWriter::new(stdin);

            // ç›´æ¥å†™å…¥å†…å­˜æ˜ å°„çš„æ•°æ®ï¼Œé¿å…é¢å¤–æ‹·è´
            writer.write_all(&source_mapping.data[source_mapping.offset..source_mapping.offset + source_mapping.length]).await?;
            writer.flush().await?;
        }

        Ok(())
    }

    pub async fn read_data_zero_copy(
        &self,
        source_process: &mut tokio::process::Child,
        target_mapping: &mut MemoryMapping,
    ) -> Result<(), Box<dyn std::error::Error>> {
        if let Some(stdout) = source_process.stdout.as_mut() {
            let mut reader = BufReader::new(stdout);
            let mut buffer = self.get_buffer().await?;

            let bytes_read = reader.read(&mut buffer).await?;

            if bytes_read > 0 {
                // ç›´æ¥å†™å…¥ç›®æ ‡å†…å­˜æ˜ å°„
                let target_data = Arc::get_mut(&mut target_mapping.data).unwrap();
                target_data.resize(target_mapping.offset + bytes_read, 0);
                target_data[target_mapping.offset..target_mapping.offset + bytes_read].copy_from_slice(&buffer[..bytes_read]);
                target_mapping.length = bytes_read;
            }

            self.return_buffer(buffer).await;
        }

        Ok(())
    }
}
```

### 1.2 å†…å­˜æ± ç®¡ç†

```rust
use std::sync::Arc;
use tokio::sync::{Mutex, RwLock};
use std::collections::VecDeque;
use std::time::{Duration, Instant};

pub struct MemoryPoolManager {
    pools: Arc<RwLock<Vec<MemoryPool>>>,
    allocation_stats: Arc<Mutex<AllocationStats>>,
}

#[derive(Debug)]
pub struct MemoryPool {
    pub id: String,
    pub buffer_size: usize,
    pub max_buffers: usize,
    pub available_buffers: VecDeque<Vec<u8>>,
    pub allocated_buffers: usize,
    pub created_at: Instant,
    pub last_used: Instant,
}

#[derive(Debug, Default)]
pub struct AllocationStats {
    pub total_allocations: u64,
    pub total_deallocations: u64,
    pub peak_usage: usize,
    pub current_usage: usize,
    pub allocation_failures: u64,
    pub average_allocation_time: Duration,
}

impl MemoryPoolManager {
    pub fn new() -> Self {
        Self {
            pools: Arc::new(RwLock::new(Vec::new())),
            allocation_stats: Arc::new(Mutex::new(AllocationStats::default())),
        }
    }

    pub async fn create_pool(
        &self,
        buffer_size: usize,
        max_buffers: usize,
    ) -> Result<String, Box<dyn std::error::Error>> {
        let pool_id = uuid::Uuid::new_v4().to_string();

        let pool = MemoryPool {
            id: pool_id.clone(),
            buffer_size,
            max_buffers,
            available_buffers: VecDeque::new(),
            allocated_buffers: 0,
            created_at: Instant::now(),
            last_used: Instant::now(),
        };

        let mut pools = self.pools.write().await;
        pools.push(pool);

        Ok(pool_id)
    }

    pub async fn allocate_buffer(
        &self,
        pool_id: &str,
        size: usize,
    ) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
        let start_time = Instant::now();

        let mut pools = self.pools.write().await;
        let pool = pools.iter_mut()
            .find(|p| p.id == pool_id)
            .ok_or("å†…å­˜æ± æœªæ‰¾åˆ°")?;

        // æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ç¼“å†²åŒº
        if let Some(mut buffer) = pool.available_buffers.pop_front() {
            buffer.resize(size, 0);
            pool.allocated_buffers += 1;
            pool.last_used = Instant::now();

            // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            let mut stats = self.allocation_stats.lock().await;
            stats.total_allocations += 1;
            stats.current_usage += 1;
            stats.peak_usage = stats.peak_usage.max(stats.current_usage);
            stats.average_allocation_time = Duration::from_millis(
                (stats.average_allocation_time.as_millis() * (stats.total_allocations - 1) as u128
                 + start_time.elapsed().as_millis()) / stats.total_allocations as u128
            );

            Ok(buffer)
        } else {
            // åˆ›å»ºæ–°ç¼“å†²åŒº
            if pool.allocated_buffers < pool.max_buffers {
                let buffer = vec![0u8; size];
                pool.allocated_buffers += 1;
                pool.last_used = Instant::now();

                // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                let mut stats = self.allocation_stats.lock().await;
                stats.total_allocations += 1;
                stats.current_usage += 1;
                stats.peak_usage = stats.peak_usage.max(stats.current_usage);

                Ok(buffer)
            } else {
                // æ›´æ–°å¤±è´¥ç»Ÿè®¡
                let mut stats = self.allocation_stats.lock().await;
                stats.allocation_failures += 1;

                Err("å†…å­˜æ± å·²æ»¡".into())
            }
        }
    }

    pub async fn deallocate_buffer(
        &self,
        pool_id: &str,
        mut buffer: Vec<u8>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let mut pools = self.pools.write().await;
        let pool = pools.iter_mut()
            .find(|p| p.id == pool_id)
            .ok_or("å†…å­˜æ± æœªæ‰¾åˆ°")?;

        if pool.available_buffers.len() < pool.max_buffers {
            buffer.clear();
            pool.available_buffers.push_back(buffer);
            pool.allocated_buffers -= 1;
            pool.last_used = Instant::now();

            // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            let mut stats = self.allocation_stats.lock().await;
            stats.total_deallocations += 1;
            stats.current_usage -= 1;
        }

        Ok(())
    }

    pub async fn get_pool_stats(&self, pool_id: &str) -> Result<PoolStats, Box<dyn std::error::Error>> {
        let pools = self.pools.read().await;
        let pool = pools.iter()
            .find(|p| p.id == pool_id)
            .ok_or("å†…å­˜æ± æœªæ‰¾åˆ°")?;

        Ok(PoolStats {
            id: pool.id.clone(),
            buffer_size: pool.buffer_size,
            max_buffers: pool.max_buffers,
            available_buffers: pool.available_buffers.len(),
            allocated_buffers: pool.allocated_buffers,
            utilization: pool.allocated_buffers as f64 / pool.max_buffers as f64,
            created_at: pool.created_at,
            last_used: pool.last_used,
        })
    }

    pub async fn cleanup_unused_pools(&self, max_idle_time: Duration) -> Result<(), Box<dyn std::error::Error>> {
        let mut pools = self.pools.write().await;
        let now = Instant::now();

        pools.retain(|pool| {
            now.duration_since(pool.last_used) < max_idle_time
        });

        Ok(())
    }
}

#[derive(Debug)]
pub struct PoolStats {
    pub id: String,
    pub buffer_size: usize,
    pub max_buffers: usize,
    pub available_buffers: usize,
    pub allocated_buffers: usize,
    pub utilization: f64,
    pub created_at: Instant,
    pub last_used: Instant,
}
```

## 2. CPU ä¼˜åŒ–

### 2.1 CPU äº²å’Œæ€§è®¾ç½®

```rust
use std::sync::Arc;
use tokio::sync::Mutex;
use std::collections::HashMap;

pub struct CpuAffinityManager {
    cpu_cores: usize,
    process_affinities: Arc<Mutex<HashMap<String, CpuAffinity>>>,
    load_balancer: Arc<Mutex<LoadBalancer>>,
}

#[derive(Debug, Clone)]
pub struct CpuAffinity {
    pub process_id: String,
    pub cpu_mask: u64,
    pub priority: ProcessPriority,
    pub last_assigned: std::time::Instant,
}

#[derive(Debug, Clone)]
pub enum ProcessPriority {
    Low,
    Normal,
    High,
    Critical,
}

#[derive(Debug)]
pub struct LoadBalancer {
    cpu_usage: Vec<f64>,
    process_count: Vec<usize>,
    last_update: std::time::Instant,
}

impl CpuAffinityManager {
    pub fn new() -> Self {
        let cpu_cores = num_cpus::get();

        Self {
            cpu_cores,
            process_affinities: Arc::new(Mutex::new(HashMap::new())),
            load_balancer: Arc::new(Mutex::new(LoadBalancer {
                cpu_usage: vec![0.0; cpu_cores],
                process_count: vec![0; cpu_cores],
                last_update: std::time::Instant::now(),
            })),
        }
    }

    pub async fn set_process_affinity(
        &self,
        process_id: &str,
        cpu_mask: u64,
        priority: ProcessPriority,
    ) -> Result<(), Box<dyn std::error::Error>> {
        #[cfg(unix)]
        {
            use nix::sched::{sched_setaffinity, CpuSet};
            use nix::unistd::Pid;

            let mut cpu_set = CpuSet::new();
            for i in 0..self.cpu_cores {
                if (cpu_mask >> i) & 1 == 1 {
                    cpu_set.set(i);
                }
            }

            // è¿™é‡Œéœ€è¦å®é™…çš„è¿›ç¨‹ PIDï¼Œç¤ºä¾‹ä¸­ä½¿ç”¨ 0
            sched_setaffinity(Pid::from_raw(0), &cpu_set)?;
        }

        let affinity = CpuAffinity {
            process_id: process_id.to_string(),
            cpu_mask,
            priority,
            last_assigned: std::time::Instant::now(),
        };

        let mut affinities = self.process_affinities.lock().await;
        affinities.insert(process_id.to_string(), affinity);

        Ok(())
    }

    pub async fn get_optimal_cpu_mask(
        &self,
        priority: ProcessPriority,
    ) -> Result<u64, Box<dyn std::error::Error>> {
        let mut load_balancer = self.load_balancer.lock().await;

        // æ›´æ–° CPU ä½¿ç”¨æƒ…å†µ
        self.update_cpu_usage(&mut load_balancer).await?;

        // æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹© CPU
        let cpu_mask = match priority {
            ProcessPriority::Critical => {
                // å…³é”®è¿›ç¨‹ä½¿ç”¨æ‰€æœ‰ CPU
                (1u64 << self.cpu_cores) - 1
            }
            ProcessPriority::High => {
                // é«˜ä¼˜å…ˆçº§è¿›ç¨‹ä½¿ç”¨å‰ä¸€åŠ CPU
                (1u64 << (self.cpu_cores / 2)) - 1
            }
            ProcessPriority::Normal => {
                // æ™®é€šè¿›ç¨‹ä½¿ç”¨è´Ÿè½½æœ€ä½çš„ CPU
                self.find_least_loaded_cpu(&load_balancer)
            }
            ProcessPriority::Low => {
                // ä½ä¼˜å…ˆçº§è¿›ç¨‹ä½¿ç”¨åä¸€åŠ CPU
                let start = self.cpu_cores / 2;
                ((1u64 << (self.cpu_cores - start)) - 1) << start
            }
        };

        Ok(cpu_mask)
    }

    async fn update_cpu_usage(&self, load_balancer: &mut LoadBalancer) -> Result<(), Box<dyn std::error::Error>> {
        // å®é™…å®ç°ä¸­åº”è¯¥è¯»å–ç³»ç»Ÿ CPU ä½¿ç”¨æƒ…å†µ
        // è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        for i in 0..self.cpu_cores {
            load_balancer.cpu_usage[i] = rand::random::<f64>() * 100.0;
        }

        load_balancer.last_update = std::time::Instant::now();
        Ok(())
    }

    fn find_least_loaded_cpu(&self, load_balancer: &LoadBalancer) -> u64 {
        let mut min_usage = f64::MAX;
        let mut best_cpu = 0;

        for i in 0..self.cpu_cores {
            if load_balancer.cpu_usage[i] < min_usage {
                min_usage = load_balancer.cpu_usage[i];
                best_cpu = i;
            }
        }

        1u64 << best_cpu
    }

    pub async fn optimize_process_distribution(&self) -> Result<(), Box<dyn std::error::Error>> {
        let affinities = self.process_affinities.lock().await;
        let mut load_balancer = self.load_balancer.lock().await;

        // é‡æ–°å¹³è¡¡è¿›ç¨‹åˆ†å¸ƒ
        for (process_id, affinity) in affinities.iter() {
            let optimal_mask = self.get_optimal_cpu_mask(affinity.priority.clone()).await?;

            if optimal_mask != affinity.cpu_mask {
                println!("å»ºè®®é‡æ–°åˆ†é…è¿›ç¨‹ {} çš„ CPU äº²å’Œæ€§", process_id);
            }
        }

        Ok(())
    }

    pub async fn get_cpu_stats(&self) -> CpuStats {
        let load_balancer = self.load_balancer.lock().await;

        CpuStats {
            total_cores: self.cpu_cores,
            cpu_usage: load_balancer.cpu_usage.clone(),
            process_count: load_balancer.process_count.clone(),
            last_update: load_balancer.last_update,
        }
    }
}

#[derive(Debug)]
pub struct CpuStats {
    pub total_cores: usize,
    pub cpu_usage: Vec<f64>,
    pub process_count: Vec<usize>,
    pub last_update: std::time::Instant,
}
```

### 2.2 è¿›ç¨‹ä¼˜å…ˆçº§ç®¡ç†

```rust
use std::sync::Arc;
use tokio::sync::Mutex;
use std::collections::HashMap;

pub struct ProcessPriorityManager {
    process_priorities: Arc<Mutex<HashMap<String, ProcessPriorityInfo>>>,
    scheduler: Arc<Mutex<PriorityScheduler>>,
}

#[derive(Debug, Clone)]
pub struct ProcessPriorityInfo {
    pub process_id: String,
    pub priority: ProcessPriority,
    pub nice_value: i32,
    pub real_time_priority: Option<i32>,
    pub cpu_limit: f64,
    pub memory_limit: u64,
}

#[derive(Debug)]
pub struct PriorityScheduler {
    pub high_priority_queue: Vec<String>,
    pub normal_priority_queue: Vec<String>,
    pub low_priority_queue: Vec<String>,
    pub current_process: Option<String>,
    pub time_slice: std::time::Duration,
}

impl ProcessPriorityManager {
    pub fn new() -> Self {
        Self {
            process_priorities: Arc::new(Mutex::new(HashMap::new())),
            scheduler: Arc::new(Mutex::new(PriorityScheduler {
                high_priority_queue: Vec::new(),
                normal_priority_queue: Vec::new(),
                low_priority_queue: Vec::new(),
                current_process: None,
                time_slice: std::time::Duration::from_millis(10),
            })),
        }
    }

    pub async fn set_process_priority(
        &self,
        process_id: &str,
        priority: ProcessPriority,
        nice_value: Option<i32>,
        real_time_priority: Option<i32>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        #[cfg(unix)]
        {
            use nix::unistd::{nice, setpriority, Priority};
            use nix::sched::{sched_setscheduler, Scheduler, SchedParam};

            if let Some(nice_val) = nice_value {
                nice(nice_val)?;
            }

            if let Some(rt_priority) = real_time_priority {
                let param = SchedParam { sched_priority: rt_priority };
                sched_setscheduler(nix::unistd::Pid::from_raw(0), Scheduler::FIFO, &param)?;
            }
        }

        let priority_info = ProcessPriorityInfo {
            process_id: process_id.to_string(),
            priority: priority.clone(),
            nice_value: nice_value.unwrap_or(0),
            real_time_priority,
            cpu_limit: 1.0,
            memory_limit: 0,
        };

        let mut priorities = self.process_priorities.lock().await;
        priorities.insert(process_id.to_string(), priority_info);

        // æ›´æ–°è°ƒåº¦å™¨é˜Ÿåˆ—
        self.update_scheduler_queue(process_id, priority).await;

        Ok(())
    }

    async fn update_scheduler_queue(&self, process_id: &str, priority: ProcessPriority) {
        let mut scheduler = self.scheduler.lock().await;

        // ä»æ‰€æœ‰é˜Ÿåˆ—ä¸­ç§»é™¤è¿›ç¨‹
        scheduler.high_priority_queue.retain(|id| id != process_id);
        scheduler.normal_priority_queue.retain(|id| id != process_id);
        scheduler.low_priority_queue.retain(|id| id != process_id);

        // æ ¹æ®ä¼˜å…ˆçº§æ·»åŠ åˆ°ç›¸åº”é˜Ÿåˆ—
        match priority {
            ProcessPriority::Critical | ProcessPriority::High => {
                scheduler.high_priority_queue.push(process_id.to_string());
            }
            ProcessPriority::Normal => {
                scheduler.normal_priority_queue.push(process_id.to_string());
            }
            ProcessPriority::Low => {
                scheduler.low_priority_queue.push(process_id.to_string());
            }
        }
    }

    pub async fn schedule_next_process(&self) -> Option<String> {
        let mut scheduler = self.scheduler.lock().await;

        // æŒ‰ä¼˜å…ˆçº§é¡ºåºè°ƒåº¦è¿›ç¨‹
        if let Some(process_id) = scheduler.high_priority_queue.pop() {
            scheduler.current_process = Some(process_id.clone());
            return Some(process_id);
        }

        if let Some(process_id) = scheduler.normal_priority_queue.pop() {
            scheduler.current_process = Some(process_id.clone());
            return Some(process_id);
        }

        if let Some(process_id) = scheduler.low_priority_queue.pop() {
            scheduler.current_process = Some(process_id.clone());
            return Some(process_id);
        }

        None
    }

    pub async fn get_process_priority(&self, process_id: &str) -> Option<ProcessPriorityInfo> {
        let priorities = self.process_priorities.lock().await;
        priorities.get(process_id).cloned()
    }

    pub async fn get_scheduler_stats(&self) -> SchedulerStats {
        let scheduler = self.scheduler.lock().await;

        SchedulerStats {
            high_priority_queue_size: scheduler.high_priority_queue.len(),
            normal_priority_queue_size: scheduler.normal_priority_queue.len(),
            low_priority_queue_size: scheduler.low_priority_queue.len(),
            current_process: scheduler.current_process.clone(),
            time_slice: scheduler.time_slice,
        }
    }
}

#[derive(Debug)]
pub struct SchedulerStats {
    pub high_priority_queue_size: usize,
    pub normal_priority_queue_size: usize,
    pub low_priority_queue_size: usize,
    pub current_process: Option<String>,
    pub time_slice: std::time::Duration,
}
```

## 3. I/O ä¼˜åŒ–

### 3.1 å¼‚æ­¥ I/O ä¼˜åŒ–

```rust
use tokio::io::{AsyncBufReadExt, AsyncWriteExt, BufReader, BufWriter};
use std::sync::Arc;
use tokio::sync::Mutex;
use std::collections::VecDeque;

pub struct AsyncIOOptimizer {
    io_pools: Arc<Mutex<HashMap<String, IOPool>>>,
    buffer_sizes: Arc<Mutex<BufferSizeConfig>>,
}

#[derive(Debug)]
pub struct IOPool {
    pub id: String,
    pub read_buffers: VecDeque<Vec<u8>>,
    pub write_buffers: VecDeque<Vec<u8>>,
    pub buffer_size: usize,
    pub max_buffers: usize,
}

#[derive(Debug)]
pub struct BufferSizeConfig {
    pub stdin_buffer_size: usize,
    pub stdout_buffer_size: usize,
    pub stderr_buffer_size: usize,
    pub file_buffer_size: usize,
}

impl AsyncIOOptimizer {
    pub fn new() -> Self {
        Self {
            io_pools: Arc::new(Mutex::new(HashMap::new())),
            buffer_sizes: Arc::new(Mutex::new(BufferSizeConfig {
                stdin_buffer_size: 8192,
                stdout_buffer_size: 8192,
                stderr_buffer_size: 4096,
                file_buffer_size: 65536,
            })),
        }
    }

    pub async fn create_io_pool(
        &self,
        pool_id: &str,
        buffer_size: usize,
        max_buffers: usize,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let pool = IOPool {
            id: pool_id.to_string(),
            read_buffers: VecDeque::new(),
            write_buffers: VecDeque::new(),
            buffer_size,
            max_buffers,
        };

        let mut pools = self.io_pools.lock().await;
        pools.insert(pool_id.to_string(), pool);

        Ok(())
    }

    pub async fn get_read_buffer(&self, pool_id: &str) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
        let mut pools = self.io_pools.lock().await;
        let pool = pools.get_mut(pool_id).ok_or("IO æ± æœªæ‰¾åˆ°")?;

        if let Some(buffer) = pool.read_buffers.pop_front() {
            Ok(buffer)
        } else {
            Ok(vec![0u8; pool.buffer_size])
        }
    }

    pub async fn return_read_buffer(&self, pool_id: &str, mut buffer: Vec<u8>) {
        let mut pools = self.io_pools.lock().await;
        if let Some(pool) = pools.get_mut(pool_id) {
            if pool.read_buffers.len() < pool.max_buffers {
                buffer.clear();
                pool.read_buffers.push_back(buffer);
            }
        }
    }

    pub async fn get_write_buffer(&self, pool_id: &str) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
        let mut pools = self.io_pools.lock().await;
        let pool = pools.get_mut(pool_id).ok_or("IO æ± æœªæ‰¾åˆ°")?;

        if let Some(buffer) = pool.write_buffers.pop_front() {
            Ok(buffer)
        } else {
            Ok(vec![0u8; pool.buffer_size])
        }
    }

    pub async fn return_write_buffer(&self, pool_id: &str, mut buffer: Vec<u8>) {
        let mut pools = self.io_pools.lock().await;
        if let Some(pool) = pools.get_mut(pool_id) {
            if pool.write_buffers.len() < pool.max_buffers {
                buffer.clear();
                pool.write_buffers.push_back(buffer);
            }
        }
    }

    pub async fn optimize_process_io(
        &self,
        child: &mut tokio::process::Child,
        pool_id: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let buffer_sizes = self.buffer_sizes.lock().await;

        // ä¼˜åŒ–æ ‡å‡†è¾“å…¥
        if let Some(stdin) = child.stdin.take() {
            let writer = BufWriter::with_capacity(buffer_sizes.stdin_buffer_size, stdin);
            child.stdin = Some(Box::pin(writer));
        }

        // ä¼˜åŒ–æ ‡å‡†è¾“å‡º
        if let Some(stdout) = child.stdout.take() {
            let reader = BufReader::with_capacity(buffer_sizes.stdout_buffer_size, stdout);
            child.stdout = Some(Box::pin(reader));
        }

        // ä¼˜åŒ–æ ‡å‡†é”™è¯¯
        if let Some(stderr) = child.stderr.take() {
            let reader = BufReader::with_capacity(buffer_sizes.stderr_buffer_size, stderr);
            child.stderr = Some(Box::pin(reader));
        }

        Ok(())
    }

    pub async fn batch_read(
        &self,
        reader: &mut tokio::io::BufReader<tokio::process::ChildStdout>,
        pool_id: &str,
        batch_size: usize,
    ) -> Result<Vec<Vec<u8>>, Box<dyn std::error::Error>> {
        let mut results = Vec::new();

        for _ in 0..batch_size {
            let mut buffer = self.get_read_buffer(pool_id).await?;
            let bytes_read = reader.read(&mut buffer).await?;

            if bytes_read > 0 {
                buffer.truncate(bytes_read);
                results.push(buffer);
            } else {
                self.return_read_buffer(pool_id, buffer).await;
                break;
            }
        }

        Ok(results)
    }

    pub async fn batch_write(
        &self,
        writer: &mut tokio::io::BufWriter<tokio::process::ChildStdin>,
        pool_id: &str,
        data_batches: Vec<Vec<u8>>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        for mut data in data_batches {
            writer.write_all(&data).await?;
            self.return_write_buffer(pool_id, data).await;
        }

        writer.flush().await?;
        Ok(())
    }
}
```

### 3.2 æ–‡ä»¶æè¿°ç¬¦ä¼˜åŒ–

```rust
use std::sync::Arc;
use tokio::sync::Mutex;
use std::collections::HashMap;
use std::time::{Duration, Instant};

pub struct FileDescriptorManager {
    fd_pools: Arc<Mutex<HashMap<String, FDPool>>>,
    fd_stats: Arc<Mutex<FDStats>>,
    max_fds_per_process: usize,
}

#[derive(Debug)]
pub struct FDPool {
    pub id: String,
    pub available_fds: Vec<i32>,
    pub used_fds: Vec<i32>,
    pub max_fds: usize,
    pub created_at: Instant,
}

#[derive(Debug, Default)]
pub struct FDStats {
    pub total_fds: usize,
    pub available_fds: usize,
    pub used_fds: usize,
    pub peak_usage: usize,
    pub allocation_failures: u64,
    pub last_cleanup: Instant,
}

impl FileDescriptorManager {
    pub fn new(max_fds_per_process: usize) -> Self {
        Self {
            fd_pools: Arc::new(Mutex::new(HashMap::new())),
            fd_stats: Arc::new(Mutex::new(FDStats::default())),
            max_fds_per_process,
        }
    }

    pub async fn create_fd_pool(
        &self,
        pool_id: &str,
        max_fds: usize,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let pool = FDPool {
            id: pool_id.to_string(),
            available_fds: Vec::new(),
            used_fds: Vec::new(),
            max_fds,
            created_at: Instant::now(),
        };

        let mut pools = self.fd_pools.lock().await;
        pools.insert(pool_id.to_string(), pool);

        Ok(())
    }

    pub async fn allocate_fd(
        &self,
        pool_id: &str,
    ) -> Result<i32, Box<dyn std::error::Error>> {
        let mut pools = self.fd_pools.lock().await;
        let pool = pools.get_mut(pool_id).ok_or("FD æ± æœªæ‰¾åˆ°")?;

        if let Some(fd) = pool.available_fds.pop() {
            pool.used_fds.push(fd);

            let mut stats = self.fd_stats.lock().await;
            stats.used_fds += 1;
            stats.total_fds += 1;
            stats.peak_usage = stats.peak_usage.max(stats.used_fds);

            Ok(fd)
        } else {
            // åˆ›å»ºæ–°çš„æ–‡ä»¶æè¿°ç¬¦
            if pool.used_fds.len() < pool.max_fds {
                let fd = self.create_new_fd().await?;
                pool.used_fds.push(fd);

                let mut stats = self.fd_stats.lock().await;
                stats.used_fds += 1;
                stats.total_fds += 1;
                stats.peak_usage = stats.peak_usage.max(stats.used_fds);

                Ok(fd)
            } else {
                let mut stats = self.fd_stats.lock().await;
                stats.allocation_failures += 1;

                Err("FD æ± å·²æ»¡".into())
            }
        }
    }

    pub async fn deallocate_fd(
        &self,
        pool_id: &str,
        fd: i32,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let mut pools = self.fd_pools.lock().await;
        let pool = pools.get_mut(pool_id).ok_or("FD æ± æœªæ‰¾åˆ°")?;

        if let Some(pos) = pool.used_fds.iter().position(|&x| x == fd) {
            pool.used_fds.remove(pos);
            pool.available_fds.push(fd);

            let mut stats = self.fd_stats.lock().await;
            stats.used_fds -= 1;
        }

        Ok(())
    }

    async fn create_new_fd(&self) -> Result<i32, Box<dyn std::error::Error>> {
        // å®é™…å®ç°ä¸­åº”è¯¥åˆ›å»ºçœŸå®çš„æ–‡ä»¶æè¿°ç¬¦
        // è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        Ok(rand::random::<i32>())
    }

    pub async fn cleanup_unused_fds(&self, max_idle_time: Duration) -> Result<(), Box<dyn std::error::Error>> {
        let mut pools = self.fd_pools.lock().await;
        let now = Instant::now();

        for pool in pools.values_mut() {
            if now.duration_since(pool.created_at) > max_idle_time {
                // æ¸…ç†æœªä½¿ç”¨çš„æ–‡ä»¶æè¿°ç¬¦
                pool.available_fds.clear();
            }
        }

        let mut stats = self.fd_stats.lock().await;
        stats.last_cleanup = now;

        Ok(())
    }

    pub async fn get_fd_stats(&self) -> FDStats {
        let stats = self.fd_stats.lock().await;
        stats.clone()
    }

    pub async fn optimize_fd_usage(&self) -> Result<(), Box<dyn std::error::Error>> {
        let pools = self.fd_pools.lock().await;
        let mut stats = self.fd_stats.lock().await;

        // è®¡ç®—ä¼˜åŒ–å»ºè®®
        let total_available = pools.values().map(|p| p.available_fds.len()).sum::<usize>();
        let total_used = pools.values().map(|p| p.used_fds.len()).sum::<usize>();

        if total_used as f64 / (total_available + total_used) as f64 > 0.8 {
            println!("è­¦å‘Š: æ–‡ä»¶æè¿°ç¬¦ä½¿ç”¨ç‡è¿‡é«˜ ({:.2}%)",
                total_used as f64 / (total_available + total_used) as f64 * 100.0);
        }

        stats.total_fds = total_available + total_used;
        stats.available_fds = total_available;
        stats.used_fds = total_used;

        Ok(())
    }
}
```

## 4. å¹¶å‘ä¼˜åŒ–

### 4.1 æ— é”æ•°æ®ç»“æ„

```rust
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, AtomicBool, Ordering};
use std::collections::VecDeque;
use std::time::{Duration, Instant};

pub struct LockFreeProcessQueue {
    queue: Arc<crossbeam::queue::SegQueue<ProcessTask>>,
    size: AtomicUsize,
    max_size: usize,
    stats: Arc<LockFreeStats>,
}

#[derive(Debug, Clone)]
pub struct ProcessTask {
    pub id: String,
    pub command: String,
    pub args: Vec<String>,
    pub priority: u8,
    pub created_at: Instant,
}

#[derive(Debug, Default)]
pub struct LockFreeStats {
    pub total_enqueued: AtomicUsize,
    pub total_dequeued: AtomicUsize,
    pub total_failed: AtomicUsize,
    pub average_wait_time: AtomicUsize,
}

impl LockFreeProcessQueue {
    pub fn new(max_size: usize) -> Self {
        Self {
            queue: Arc::new(crossbeam::queue::SegQueue::new()),
            size: AtomicUsize::new(0),
            max_size,
            stats: Arc::new(LockFreeStats::default()),
        }
    }

    pub fn enqueue(&self, task: ProcessTask) -> Result<(), QueueError> {
        if self.size.load(Ordering::Relaxed) >= self.max_size {
            self.stats.total_failed.fetch_add(1, Ordering::Relaxed);
            return Err(QueueError::QueueFull);
        }

        self.queue.push(task);
        self.size.fetch_add(1, Ordering::Relaxed);
        self.stats.total_enqueued.fetch_add(1, Ordering::Relaxed);

        Ok(())
    }

    pub fn dequeue(&self) -> Option<ProcessTask> {
        if let Some(task) = self.queue.pop() {
            self.size.fetch_sub(1, Ordering::Relaxed);
            self.stats.total_dequeued.fetch_add(1, Ordering::Relaxed);
            Some(task)
        } else {
            None
        }
    }

    pub fn size(&self) -> usize {
        self.size.load(Ordering::Relaxed)
    }

    pub fn is_empty(&self) -> bool {
        self.size.load(Ordering::Relaxed) == 0
    }

    pub fn get_stats(&self) -> QueueStats {
        QueueStats {
            total_enqueued: self.stats.total_enqueued.load(Ordering::Relaxed),
            total_dequeued: self.stats.total_dequeued.load(Ordering::Relaxed),
            total_failed: self.stats.total_failed.load(Ordering::Relaxed),
            current_size: self.size.load(Ordering::Relaxed),
            max_size: self.max_size,
        }
    }
}

#[derive(Debug, thiserror::Error)]
pub enum QueueError {
    #[error("é˜Ÿåˆ—å·²æ»¡")]
    QueueFull,
}

#[derive(Debug)]
pub struct QueueStats {
    pub total_enqueued: usize,
    pub total_dequeued: usize,
    pub total_failed: usize,
    pub current_size: usize,
    pub max_size: usize,
}

pub struct LockFreeProcessPool {
    queues: Vec<Arc<LockFreeProcessQueue>>,
    current_queue: AtomicUsize,
    pool_size: usize,
}

impl LockFreeProcessPool {
    pub fn new(pool_size: usize, queue_size: usize) -> Self {
        let mut queues = Vec::new();
        for _ in 0..pool_size {
            queues.push(Arc::new(LockFreeProcessQueue::new(queue_size)));
        }

        Self {
            queues,
            current_queue: AtomicUsize::new(0),
            pool_size,
        }
    }

    pub fn enqueue_task(&self, task: ProcessTask) -> Result<(), QueueError> {
        // ä½¿ç”¨è½®è¯¢ç­–ç•¥é€‰æ‹©é˜Ÿåˆ—
        let queue_index = self.current_queue.fetch_add(1, Ordering::Relaxed) % self.pool_size;
        let queue = &self.queues[queue_index];

        queue.enqueue(task)
    }

    pub fn dequeue_task(&self) -> Option<ProcessTask> {
        // ä»æ‰€æœ‰é˜Ÿåˆ—ä¸­æŸ¥æ‰¾ä»»åŠ¡
        for queue in &self.queues {
            if let Some(task) = queue.dequeue() {
                return Some(task);
            }
        }

        None
    }

    pub fn get_pool_stats(&self) -> PoolStats {
        let mut total_enqueued = 0;
        let mut total_dequeued = 0;
        let mut total_failed = 0;
        let mut current_size = 0;

        for queue in &self.queues {
            let stats = queue.get_stats();
            total_enqueued += stats.total_enqueued;
            total_dequeued += stats.total_dequeued;
            total_failed += stats.total_failed;
            current_size += stats.current_size;
        }

        PoolStats {
            pool_size: self.pool_size,
            total_enqueued,
            total_dequeued,
            total_failed,
            current_size,
        }
    }
}

#[derive(Debug)]
pub struct PoolStats {
    pub pool_size: usize,
    pub total_enqueued: usize,
    pub total_dequeued: usize,
    pub total_failed: usize,
    pub current_size: usize,
}
```

### 4.2 å·¥ä½œçªƒå–è°ƒåº¦å™¨

```rust
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::collections::VecDeque;
use std::time::{Duration, Instant};

pub struct WorkStealingScheduler {
    workers: Vec<Arc<Worker>>,
    global_queue: Arc<LockFreeProcessQueue>,
    stats: Arc<SchedulerStats>,
}

pub struct Worker {
    id: usize,
    local_queue: Arc<LockFreeProcessQueue>,
    is_busy: std::sync::atomic::AtomicBool,
    processed_tasks: std::sync::atomic::AtomicUsize,
}

#[derive(Debug, Default)]
pub struct SchedulerStats {
    pub total_tasks_processed: std::sync::atomic::AtomicUsize,
    pub total_steals: std::sync::atomic::AtomicUsize,
    pub average_processing_time: std::sync::atomic::AtomicUsize,
}

impl WorkStealingScheduler {
    pub fn new(num_workers: usize, queue_size: usize) -> Self {
        let mut workers = Vec::new();
        for i in 0..num_workers {
            workers.push(Arc::new(Worker {
                id: i,
                local_queue: Arc::new(LockFreeProcessQueue::new(queue_size)),
                is_busy: std::sync::atomic::AtomicBool::new(false),
                processed_tasks: std::sync::atomic::AtomicUsize::new(0),
            }));
        }

        Self {
            workers,
            global_queue: Arc::new(LockFreeProcessQueue::new(queue_size * num_workers)),
            stats: Arc::new(SchedulerStats::default()),
        }
    }

    pub fn submit_task(&self, task: ProcessTask) -> Result<(), QueueError> {
        // é¦–å…ˆå°è¯•æäº¤åˆ°å…¨å±€é˜Ÿåˆ—
        self.global_queue.enqueue(task)
    }

    pub async fn process_tasks(&self, worker_id: usize) -> Result<(), Box<dyn std::error::Error>> {
        let worker = &self.workers[worker_id];
        worker.is_busy.store(true, Ordering::Relaxed);

        loop {
            // é¦–å…ˆä»æœ¬åœ°é˜Ÿåˆ—è·å–ä»»åŠ¡
            if let Some(task) = worker.local_queue.dequeue() {
                self.process_task(worker, task).await?;
                continue;
            }

            // æœ¬åœ°é˜Ÿåˆ—ä¸ºç©ºï¼Œå°è¯•ä»å…¨å±€é˜Ÿåˆ—è·å–
            if let Some(task) = self.global_queue.dequeue() {
                self.process_task(worker, task).await?;
                continue;
            }

            // å…¨å±€é˜Ÿåˆ—ä¹Ÿä¸ºç©ºï¼Œå°è¯•å·¥ä½œçªƒå–
            if let Some(task) = self.steal_task(worker_id) {
                self.process_task(worker, task).await?;
                continue;
            }

            // æ²¡æœ‰ä»»åŠ¡å¯å¤„ç†ï¼ŒçŸ­æš‚ä¼‘çœ 
            tokio::time::sleep(Duration::from_millis(1)).await;
        }
    }

    async fn process_task(
        &self,
        worker: &Worker,
        task: ProcessTask,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let start_time = Instant::now();

        // æ¨¡æ‹Ÿä»»åŠ¡å¤„ç†
        tokio::time::sleep(Duration::from_millis(10)).await;

        let processing_time = start_time.elapsed();

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        worker.processed_tasks.fetch_add(1, Ordering::Relaxed);
        self.stats.total_tasks_processed.fetch_add(1, Ordering::Relaxed);

        // æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
        let current_avg = self.stats.average_processing_time.load(Ordering::Relaxed);
        let new_avg = (current_avg + processing_time.as_millis() as usize) / 2;
        self.stats.average_processing_time.store(new_avg, Ordering::Relaxed);

        Ok(())
    }

    fn steal_task(&self, worker_id: usize) -> Option<ProcessTask> {
        // éšæœºé€‰æ‹©å…¶ä»–å·¥ä½œçº¿ç¨‹è¿›è¡Œçªƒå–
        let mut rng = rand::thread_rng();
        let num_workers = self.workers.len();

        for _ in 0..num_workers {
            let target_worker_id = rng.gen_range(0..num_workers);
            if target_worker_id != worker_id {
                if let Some(task) = self.workers[target_worker_id].local_queue.dequeue() {
                    self.stats.total_steals.fetch_add(1, Ordering::Relaxed);
                    return Some(task);
                }
            }
        }

        None
    }

    pub fn get_scheduler_stats(&self) -> SchedulerStatsInfo {
        let mut worker_stats = Vec::new();
        let mut total_processed = 0;

        for worker in &self.workers {
            let processed = worker.processed_tasks.load(Ordering::Relaxed);
            total_processed += processed;

            worker_stats.push(WorkerStats {
                id: worker.id,
                processed_tasks: processed,
                is_busy: worker.is_busy.load(Ordering::Relaxed),
            });
        }

        SchedulerStatsInfo {
            num_workers: self.workers.len(),
            total_tasks_processed: self.stats.total_tasks_processed.load(Ordering::Relaxed),
            total_steals: self.stats.total_steals.load(Ordering::Relaxed),
            average_processing_time: Duration::from_millis(
                self.stats.average_processing_time.load(Ordering::Relaxed) as u64
            ),
            worker_stats,
        }
    }
}

#[derive(Debug)]
pub struct WorkerStats {
    pub id: usize,
    pub processed_tasks: usize,
    pub is_busy: bool,
}

#[derive(Debug)]
pub struct SchedulerStatsInfo {
    pub num_workers: usize,
    pub total_tasks_processed: usize,
    pub total_steals: usize,
    pub average_processing_time: Duration,
    pub worker_stats: Vec<WorkerStats>,
}
```

## 5. æ€§èƒ½ç›‘æ§

### 5.1 æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```rust
use std::sync::Arc;
use tokio::sync::Mutex;
use std::collections::HashMap;
use std::time::{Duration, Instant};

pub struct PerformanceMonitor {
    metrics: Arc<Mutex<PerformanceMetrics>>,
    collectors: Arc<Mutex<Vec<Box<dyn MetricCollector + Send + Sync>>>>,
    sampling_interval: Duration,
}

pub trait MetricCollector {
    fn collect(&self) -> Result<MetricData, Box<dyn std::error::Error>>;
    fn get_name(&self) -> &str;
}

#[derive(Debug, Default)]
pub struct PerformanceMetrics {
    pub cpu_usage: f64,
    pub memory_usage: u64,
    pub disk_io: DiskIOMetrics,
    pub network_io: NetworkIOMetrics,
    pub process_metrics: ProcessMetrics,
    pub timestamp: Instant,
}

#[derive(Debug, Default)]
pub struct DiskIOMetrics {
    pub read_bytes: u64,
    pub write_bytes: u64,
    pub read_ops: u64,
    pub write_ops: u64,
    pub read_time: Duration,
    pub write_time: Duration,
}

#[derive(Debug, Default)]
pub struct NetworkIOMetrics {
    pub bytes_sent: u64,
    pub bytes_received: u64,
    pub packets_sent: u64,
    pub packets_received: u64,
    pub connection_count: usize,
}

#[derive(Debug, Default)]
pub struct ProcessMetrics {
    pub total_processes: usize,
    pub running_processes: usize,
    pub zombie_processes: usize,
    pub average_cpu_usage: f64,
    pub average_memory_usage: u64,
}

#[derive(Debug)]
pub struct MetricData {
    pub name: String,
    pub value: f64,
    pub unit: String,
    pub timestamp: Instant,
}

impl PerformanceMonitor {
    pub fn new(sampling_interval: Duration) -> Self {
        Self {
            metrics: Arc::new(Mutex::new(PerformanceMetrics::default())),
            collectors: Arc::new(Mutex::new(Vec::new())),
            sampling_interval,
        }
    }

    pub async fn add_collector(&self, collector: Box<dyn MetricCollector + Send + Sync>) {
        let mut collectors = self.collectors.lock().await;
        collectors.push(collector);
    }

    pub async fn start_monitoring(&self) -> Result<(), Box<dyn std::error::Error>> {
        let mut interval = tokio::time::interval(self.sampling_interval);

        loop {
            interval.tick().await;
            self.collect_metrics().await?;
        }
    }

    async fn collect_metrics(&self) -> Result<(), Box<dyn std::error::Error>> {
        let collectors = self.collectors.lock().await;
        let mut metrics = self.metrics.lock().await;

        for collector in collectors.iter() {
            match collector.collect() {
                Ok(data) => {
                    self.update_metrics(&mut metrics, &data).await;
                }
                Err(e) => {
                    eprintln!("æ”¶é›†æŒ‡æ ‡å¤±è´¥ {}: {}", collector.get_name(), e);
                }
            }
        }

        metrics.timestamp = Instant::now();
        Ok(())
    }

    async fn update_metrics(&self, metrics: &mut PerformanceMetrics, data: &MetricData) {
        match data.name.as_str() {
            "cpu_usage" => metrics.cpu_usage = data.value,
            "memory_usage" => metrics.memory_usage = data.value as u64,
            "disk_read_bytes" => metrics.disk_io.read_bytes = data.value as u64,
            "disk_write_bytes" => metrics.disk_io.write_bytes = data.value as u64,
            "network_sent_bytes" => metrics.network_io.bytes_sent = data.value as u64,
            "network_received_bytes" => metrics.network_io.bytes_received = data.value as u64,
            "process_count" => metrics.process_metrics.total_processes = data.value as usize,
            _ => {}
        }
    }

    pub async fn get_metrics(&self) -> PerformanceMetrics {
        let metrics = self.metrics.lock().await;
        metrics.clone()
    }

    pub async fn get_metrics_summary(&self) -> PerformanceSummary {
        let metrics = self.metrics.lock().await;

        PerformanceSummary {
            cpu_usage: metrics.cpu_usage,
            memory_usage: metrics.memory_usage,
            disk_io_rate: (metrics.disk_io.read_bytes + metrics.disk_io.write_bytes) as f64,
            network_io_rate: (metrics.network_io.bytes_sent + metrics.network_io.bytes_received) as f64,
            process_count: metrics.process_metrics.total_processes,
            timestamp: metrics.timestamp,
        }
    }
}

#[derive(Debug)]
pub struct PerformanceSummary {
    pub cpu_usage: f64,
    pub memory_usage: u64,
    pub disk_io_rate: f64,
    pub network_io_rate: f64,
    pub process_count: usize,
    pub timestamp: Instant,
}

pub struct CpuUsageCollector;

impl MetricCollector for CpuUsageCollector {
    fn collect(&self) -> Result<MetricData, Box<dyn std::error::Error>> {
        // å®é™…å®ç°ä¸­åº”è¯¥è¯»å–ç³»ç»Ÿ CPU ä½¿ç”¨æƒ…å†µ
        let cpu_usage = rand::random::<f64>() * 100.0;

        Ok(MetricData {
            name: "cpu_usage".to_string(),
            value: cpu_usage,
            unit: "percent".to_string(),
            timestamp: Instant::now(),
        })
    }

    fn get_name(&self) -> &str {
        "cpu_usage"
    }
}

pub struct MemoryUsageCollector;

impl MetricCollector for MemoryUsageCollector {
    fn collect(&self) -> Result<MetricData, Box<dyn std::error::Error>> {
        // å®é™…å®ç°ä¸­åº”è¯¥è¯»å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ
        let memory_usage = rand::random::<u64>() % 1000000000; // æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨

        Ok(MetricData {
            name: "memory_usage".to_string(),
            value: memory_usage as f64,
            unit: "bytes".to_string(),
            timestamp: Instant::now(),
        })
    }

    fn get_name(&self) -> &str {
        "memory_usage"
    }
}
```

### 5.2 æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®

```rust
use std::sync::Arc;
use tokio::sync::Mutex;
use std::collections::HashMap;
use std::time::{Duration, Instant};

pub struct PerformanceAnalyzer {
    historical_data: Arc<Mutex<Vec<PerformanceMetrics>>>,
    analysis_config: AnalysisConfig,
    optimization_suggestions: Arc<Mutex<Vec<OptimizationSuggestion>>>,
}

#[derive(Debug, Clone)]
pub struct AnalysisConfig {
    pub max_history_size: usize,
    pub analysis_interval: Duration,
    pub cpu_threshold: f64,
    pub memory_threshold: u64,
    pub disk_io_threshold: u64,
    pub network_io_threshold: u64,
}

#[derive(Debug, Clone)]
pub struct OptimizationSuggestion {
    pub id: String,
    pub category: OptimizationCategory,
    pub priority: SuggestionPriority,
    pub description: String,
    pub expected_improvement: f64,
    pub implementation_cost: ImplementationCost,
    pub created_at: Instant,
}

#[derive(Debug, Clone)]
pub enum OptimizationCategory {
    Cpu,
    Memory,
    DiskIO,
    NetworkIO,
    ProcessManagement,
    Concurrency,
}

#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
pub enum SuggestionPriority {
    Low,
    Medium,
    High,
    Critical,
}

#[derive(Debug, Clone)]
pub enum ImplementationCost {
    Low,
    Medium,
    High,
    VeryHigh,
}

impl PerformanceAnalyzer {
    pub fn new(config: AnalysisConfig) -> Self {
        Self {
            historical_data: Arc::new(Mutex::new(Vec::new())),
            analysis_config: config,
            optimization_suggestions: Arc::new(Mutex::new(Vec::new())),
        }
    }

    pub async fn add_metrics(&self, metrics: PerformanceMetrics) {
        let mut historical_data = self.historical_data.lock().await;

        historical_data.push(metrics);

        // ä¿æŒå†å²æ•°æ®å¤§å°é™åˆ¶
        if historical_data.len() > self.analysis_config.max_history_size {
            historical_data.remove(0);
        }
    }

    pub async fn analyze_performance(&self) -> Result<(), Box<dyn std::error::Error>> {
        let historical_data = self.historical_data.lock().await;

        if historical_data.len() < 2 {
            return Ok(());
        }

        let mut suggestions = Vec::new();

        // åˆ†æ CPU ä½¿ç”¨æƒ…å†µ
        self.analyze_cpu_usage(&historical_data, &mut suggestions).await;

        // åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ
        self.analyze_memory_usage(&historical_data, &mut suggestions).await;

        // åˆ†æç£ç›˜ I/O
        self.analyze_disk_io(&historical_data, &mut suggestions).await;

        // åˆ†æç½‘ç»œ I/O
        self.analyze_network_io(&historical_data, &mut suggestions).await;

        // åˆ†æè¿›ç¨‹ç®¡ç†
        self.analyze_process_management(&historical_data, &mut suggestions).await;

        // æ›´æ–°ä¼˜åŒ–å»ºè®®
        let mut optimization_suggestions = self.optimization_suggestions.lock().await;
        optimization_suggestions.clear();
        optimization_suggestions.extend(suggestions);

        Ok(())
    }

    async fn analyze_cpu_usage(
        &self,
        historical_data: &[PerformanceMetrics],
        suggestions: &mut Vec<OptimizationSuggestion>,
    ) {
        let recent_data = &historical_data[historical_data.len().saturating_sub(10)..];
        let avg_cpu_usage = recent_data.iter().map(|m| m.cpu_usage).sum::<f64>() / recent_data.len() as f64;

        if avg_cpu_usage > self.analysis_config.cpu_threshold {
            suggestions.push(OptimizationSuggestion {
                id: uuid::Uuid::new_v4().to_string(),
                category: OptimizationCategory::Cpu,
                priority: if avg_cpu_usage > 90.0 { SuggestionPriority::Critical } else { SuggestionPriority::High },
                description: format!("CPU ä½¿ç”¨ç‡è¿‡é«˜: {:.2}%", avg_cpu_usage),
                expected_improvement: 20.0,
                implementation_cost: ImplementationCost::Medium,
                created_at: Instant::now(),
            });
        }
    }

    async fn analyze_memory_usage(
        &self,
        historical_data: &[PerformanceMetrics],
        suggestions: &mut Vec<OptimizationSuggestion>,
    ) {
        let recent_data = &historical_data[historical_data.len().saturating_sub(10)..];
        let avg_memory_usage = recent_data.iter().map(|m| m.memory_usage).sum::<u64>() / recent_data.len() as u64;

        if avg_memory_usage > self.analysis_config.memory_threshold {
            suggestions.push(OptimizationSuggestion {
                id: uuid::Uuid::new_v4().to_string(),
                category: OptimizationCategory::Memory,
                priority: SuggestionPriority::High,
                description: format!("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {} MB", avg_memory_usage / 1024 / 1024),
                expected_improvement: 30.0,
                implementation_cost: ImplementationCost::High,
                created_at: Instant::now(),
            });
        }
    }

    async fn analyze_disk_io(
        &self,
        historical_data: &[PerformanceMetrics],
        suggestions: &mut Vec<OptimizationSuggestion>,
    ) {
        let recent_data = &historical_data[historical_data.len().saturating_sub(10)..];
        let avg_disk_io = recent_data.iter()
            .map(|m| m.disk_io.read_bytes + m.disk_io.write_bytes)
            .sum::<u64>() / recent_data.len() as u64;

        if avg_disk_io > self.analysis_config.disk_io_threshold {
            suggestions.push(OptimizationSuggestion {
                id: uuid::Uuid::new_v4().to_string(),
                category: OptimizationCategory::DiskIO,
                priority: SuggestionPriority::Medium,
                description: format!("ç£ç›˜ I/O è¿‡é«˜: {} MB/s", avg_disk_io / 1024 / 1024),
                expected_improvement: 25.0,
                implementation_cost: ImplementationCost::Medium,
                created_at: Instant::now(),
            });
        }
    }

    async fn analyze_network_io(
        &self,
        historical_data: &[PerformanceMetrics],
        suggestions: &mut Vec<OptimizationSuggestion>,
    ) {
        let recent_data = &historical_data[historical_data.len().saturating_sub(10)..];
        let avg_network_io = recent_data.iter()
            .map(|m| m.network_io.bytes_sent + m.network_io.bytes_received)
            .sum::<u64>() / recent_data.len() as u64;

        if avg_network_io > self.analysis_config.network_io_threshold {
            suggestions.push(OptimizationSuggestion {
                id: uuid::Uuid::new_v4().to_string(),
                category: OptimizationCategory::NetworkIO,
                priority: SuggestionPriority::Medium,
                description: format!("ç½‘ç»œ I/O è¿‡é«˜: {} MB/s", avg_network_io / 1024 / 1024),
                expected_improvement: 15.0,
                implementation_cost: ImplementationCost::Low,
                created_at: Instant::now(),
            });
        }
    }

    async fn analyze_process_management(
        &self,
        historical_data: &[PerformanceMetrics],
        suggestions: &mut Vec<OptimizationSuggestion>,
    ) {
        let recent_data = &historical_data[historical_data.len().saturating_sub(10)..];
        let avg_process_count = recent_data.iter().map(|m| m.process_metrics.total_processes).sum::<usize>() / recent_data.len();

        if avg_process_count > 1000 {
            suggestions.push(OptimizationSuggestion {
                id: uuid::Uuid::new_v4().to_string(),
                category: OptimizationCategory::ProcessManagement,
                priority: SuggestionPriority::High,
                description: format!("è¿›ç¨‹æ•°é‡è¿‡å¤š: {}", avg_process_count),
                expected_improvement: 40.0,
                implementation_cost: ImplementationCost::High,
                created_at: Instant::now(),
            });
        }
    }

    pub async fn get_optimization_suggestions(&self) -> Vec<OptimizationSuggestion> {
        let suggestions = self.optimization_suggestions.lock().await;
        suggestions.clone()
    }

    pub async fn get_performance_trends(&self) -> PerformanceTrends {
        let historical_data = self.historical_data.lock().await;

        if historical_data.len() < 2 {
            return PerformanceTrends::default();
        }

        let recent_data = &historical_data[historical_data.len().saturating_sub(20)..];
        let older_data = &historical_data[historical_data.len().saturating_sub(40)..historical_data.len().saturating_sub(20)];

        let recent_avg_cpu = recent_data.iter().map(|m| m.cpu_usage).sum::<f64>() / recent_data.len() as f64;
        let older_avg_cpu = older_data.iter().map(|m| m.cpu_usage).sum::<f64>() / older_data.len() as f64;

        let recent_avg_memory = recent_data.iter().map(|m| m.memory_usage).sum::<u64>() / recent_data.len() as u64;
        let older_avg_memory = older_data.iter().map(|m| m.memory_usage).sum::<u64>() / older_data.len() as u64;

        PerformanceTrends {
            cpu_trend: if recent_avg_cpu > older_avg_cpu { TrendDirection::Increasing } else { TrendDirection::Decreasing },
            memory_trend: if recent_avg_memory > older_avg_memory { TrendDirection::Increasing } else { TrendDirection::Decreasing },
            cpu_change: (recent_avg_cpu - older_avg_cpu).abs(),
            memory_change: (recent_avg_memory as i64 - older_avg_memory as i64).abs() as u64,
        }
    }
}

#[derive(Debug, Default)]
pub struct PerformanceTrends {
    pub cpu_trend: TrendDirection,
    pub memory_trend: TrendDirection,
    pub cpu_change: f64,
    pub memory_change: u64,
}

#[derive(Debug, Clone)]
pub enum TrendDirection {
    Increasing,
    Decreasing,
    Stable,
}

impl Default for TrendDirection {
    fn default() -> Self {
        TrendDirection::Stable
    }
}
```

## 6. æ€»ç»“

æœ¬ç« è¯¦ç»†ä»‹ç»äº† Rust è¿›ç¨‹ç®¡ç†çš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼š

1. **å†…å­˜ç®¡ç†ä¼˜åŒ–**: é›¶æ‹·è´æ•°æ®ä¼ è¾“ã€å†…å­˜æ± ç®¡ç†
2. **CPU ä¼˜åŒ–**: CPU äº²å’Œæ€§è®¾ç½®ã€è¿›ç¨‹ä¼˜å…ˆçº§ç®¡ç†
3. **I/O ä¼˜åŒ–**: å¼‚æ­¥ I/O ä¼˜åŒ–ã€æ–‡ä»¶æè¿°ç¬¦ä¼˜åŒ–
4. **å¹¶å‘ä¼˜åŒ–**: æ— é”æ•°æ®ç»“æ„ã€å·¥ä½œçªƒå–è°ƒåº¦å™¨
5. **æ€§èƒ½ç›‘æ§**: æ€§èƒ½æŒ‡æ ‡æ”¶é›†ã€æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®

è¿™äº›æŠ€æœ¯ä¸ºæ„å»ºé«˜æ€§èƒ½çš„è¿›ç¨‹ç®¡ç†ç³»ç»Ÿæä¾›äº†å…¨é¢çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿçš„ååé‡ã€å“åº”æ—¶é—´å’Œèµ„æºåˆ©ç”¨ç‡ã€‚
