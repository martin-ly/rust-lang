//! åŸºå‡†æµ‹è¯•æ¼”ç¤ºç¤ºä¾‹
//! 
//! å±•ç¤ºå¦‚ä½•ä½¿ç”¨c17_iotçš„åŸºå‡†æµ‹è¯•åŠŸèƒ½æ¥è¯„ä¼°IoTç³»ç»Ÿæ€§èƒ½

use c17_iot::benchmarking::{
    Benchmarker, BenchmarkConfig, BenchmarkType
};
use std::time::Duration;
use tokio::time::sleep;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸš€ å¯åŠ¨åŸºå‡†æµ‹è¯•æ¼”ç¤º...");

    // åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    let benchmarker = Benchmarker::new();

    println!("ğŸ“Š å¼€å§‹IoTç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•...");

    // 1. å»¶è¿ŸåŸºå‡†æµ‹è¯•
    println!("\n1ï¸âƒ£ å»¶è¿ŸåŸºå‡†æµ‹è¯•");
    run_latency_benchmark(&benchmarker).await?;

    // 2. ååé‡åŸºå‡†æµ‹è¯•
    println!("\n2ï¸âƒ£ ååé‡åŸºå‡†æµ‹è¯•");
    run_throughput_benchmark(&benchmarker).await?;

    // 3. å¹¶å‘åŸºå‡†æµ‹è¯•
    println!("\n3ï¸âƒ£ å¹¶å‘åŸºå‡†æµ‹è¯•");
    run_concurrency_benchmark(&benchmarker).await?;

    // 4. å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•
    println!("\n4ï¸âƒ£ å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•");
    run_memory_benchmark(&benchmarker).await?;

    // 5. ç¨³å®šæ€§åŸºå‡†æµ‹è¯•
    println!("\n5ï¸âƒ£ ç¨³å®šæ€§åŸºå‡†æµ‹è¯•");
    run_stability_benchmark(&benchmarker).await?;

    // 6. å‹åŠ›æµ‹è¯•
    println!("\n6ï¸âƒ£ å‹åŠ›æµ‹è¯•");
    run_stress_benchmark(&benchmarker).await?;

    // 7. ç»“æœåˆ†æå’Œå¯¹æ¯”
    println!("\n7ï¸âƒ£ ç»“æœåˆ†æå’Œå¯¹æ¯”");
    analyze_benchmark_results(&benchmarker).await?;

    // 8. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    println!("\n8ï¸âƒ£ ç”Ÿæˆç»¼åˆæŠ¥å‘Š");
    generate_comprehensive_report(&benchmarker).await?;

    println!("\nâœ… åŸºå‡†æµ‹è¯•æ¼”ç¤ºå®Œæˆ!");
    println!("ğŸ¯ æ¼”ç¤ºå±•ç¤ºäº†ä»¥ä¸‹åŠŸèƒ½:");
    println!("  - å¤šç§åŸºå‡†æµ‹è¯•ç±»å‹");
    println!("  - æ€§èƒ½æŒ‡æ ‡æ”¶é›†");
    println!("  - ç»“æœåˆ†æå’Œå¯¹æ¯”");
    println!("  - è¯¦ç»†æŠ¥å‘Šç”Ÿæˆ");

    Ok(())
}

/// è¿è¡Œå»¶è¿ŸåŸºå‡†æµ‹è¯•
async fn run_latency_benchmark(benchmarker: &Benchmarker) -> Result<(), Box<dyn std::error::Error>> {
    let config = BenchmarkConfig {
        benchmark_type: BenchmarkType::Latency,
        duration: Duration::from_secs(30),
        concurrency: 1,
        data_size: 1024,
        detailed_stats: true,
        sampling_interval: Duration::from_millis(100),
        warmup_duration: Duration::from_secs(2),
    };

    println!("  å¼€å§‹å»¶è¿ŸåŸºå‡†æµ‹è¯• (30ç§’)...");
    benchmarker.start_benchmark("latency_test".to_string(), config).await?;

    // æ¨¡æ‹Ÿå„ç§å»¶è¿Ÿçš„æ“ä½œ
    for i in 0..100 {
        let operation_latency = Duration::from_millis(50 + (i % 100)); // 50-150ms
        let success = i % 10 != 0; // 90%æˆåŠŸç‡
        let error = if success { None } else { Some("æ¨¡æ‹Ÿé”™è¯¯".to_string()) };
        
        benchmarker.record_operation(operation_latency, success, error).await?;
        sleep(Duration::from_millis(300)).await;
    }

    let result = benchmarker.stop_benchmark().await?;
    println!("  å»¶è¿Ÿæµ‹è¯•å®Œæˆ:");
    println!("    å¹³å‡å»¶è¿Ÿ: {:?}", result.avg_latency);
    println!("    æœ€å°å»¶è¿Ÿ: {:?}", result.min_latency);
    println!("    æœ€å¤§å»¶è¿Ÿ: {:?}", result.max_latency);
    println!("    95%åˆ†ä½å»¶è¿Ÿ: {:?}", result.p95_latency);
    println!("    99%åˆ†ä½å»¶è¿Ÿ: {:?}", result.p99_latency);

    Ok(())
}

/// è¿è¡Œååé‡åŸºå‡†æµ‹è¯•
async fn run_throughput_benchmark(benchmarker: &Benchmarker) -> Result<(), Box<dyn std::error::Error>> {
    let config = BenchmarkConfig {
        benchmark_type: BenchmarkType::Throughput,
        duration: Duration::from_secs(20),
        concurrency: 5,
        data_size: 512,
        detailed_stats: true,
        sampling_interval: Duration::from_millis(50),
        warmup_duration: Duration::from_secs(1),
    };

    println!("  å¼€å§‹ååé‡åŸºå‡†æµ‹è¯• (20ç§’, 5å¹¶å‘)...");
    benchmarker.start_benchmark("throughput_test".to_string(), config).await?;

    // æ¨¡æ‹Ÿé«˜ååé‡æ“ä½œ
    for i in 0..200 {
        let operation_latency = Duration::from_millis(10 + (i % 20)); // 10-30ms
        let success = i % 20 != 0; // 95%æˆåŠŸç‡
        let error = if success { None } else { Some("ååé‡æµ‹è¯•é”™è¯¯".to_string()) };
        
        benchmarker.record_operation(operation_latency, success, error).await?;
        sleep(Duration::from_millis(100)).await;
    }

    let result = benchmarker.stop_benchmark().await?;
    println!("  ååé‡æµ‹è¯•å®Œæˆ:");
    println!("    æ€»æ“ä½œæ•°: {}", result.total_operations);
    println!("    ååé‡: {:.2} æ“ä½œ/ç§’", result.throughput);
    println!("    æˆåŠŸç‡: {:.1}%", (result.successful_operations as f64 / result.total_operations as f64) * 100.0);

    Ok(())
}

/// è¿è¡Œå¹¶å‘åŸºå‡†æµ‹è¯•
async fn run_concurrency_benchmark(benchmarker: &Benchmarker) -> Result<(), Box<dyn std::error::Error>> {
    let config = BenchmarkConfig {
        benchmark_type: BenchmarkType::Concurrency,
        duration: Duration::from_secs(25),
        concurrency: 10,
        data_size: 256,
        detailed_stats: true,
        sampling_interval: Duration::from_millis(200),
        warmup_duration: Duration::from_secs(3),
    };

    println!("  å¼€å§‹å¹¶å‘åŸºå‡†æµ‹è¯• (25ç§’, 10å¹¶å‘)...");
    benchmarker.start_benchmark("concurrency_test".to_string(), config).await?;

    // æ¨¡æ‹Ÿå¹¶å‘æ“ä½œ
    for i in 0..150 {
        let operation_latency = Duration::from_millis(20 + (i % 50)); // 20-70ms
        let success = i % 15 != 0; // 93%æˆåŠŸç‡
        let error = if success { None } else { Some("å¹¶å‘æµ‹è¯•é”™è¯¯".to_string()) };
        
        benchmarker.record_operation(operation_latency, success, error).await?;
        sleep(Duration::from_millis(150)).await;
    }

    let result = benchmarker.stop_benchmark().await?;
    println!("  å¹¶å‘æµ‹è¯•å®Œæˆ:");
    println!("    å¹¶å‘æ•°: 10");
    println!("    å¹³å‡å»¶è¿Ÿ: {:?}", result.avg_latency);
    println!("    ååé‡: {:.2} æ“ä½œ/ç§’", result.throughput);
    println!("    é”™è¯¯ç‡: {:.2}%", result.error_rate);

    Ok(())
}

/// è¿è¡Œå†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•
async fn run_memory_benchmark(benchmarker: &Benchmarker) -> Result<(), Box<dyn std::error::Error>> {
    let config = BenchmarkConfig {
        benchmark_type: BenchmarkType::MemoryUsage,
        duration: Duration::from_secs(15),
        concurrency: 3,
        data_size: 2048,
        detailed_stats: true,
        sampling_interval: Duration::from_millis(100),
        warmup_duration: Duration::from_secs(2),
    };

    println!("  å¼€å§‹å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯• (15ç§’)...");
    benchmarker.start_benchmark("memory_test".to_string(), config).await?;

    // æ¨¡æ‹Ÿå†…å­˜å¯†é›†å‹æ“ä½œ
    for i in 0..80 {
        let operation_latency = Duration::from_millis(30 + (i % 40)); // 30-70ms
        let success = i % 12 != 0; // 92%æˆåŠŸç‡
        let error = if success { None } else { Some("å†…å­˜æµ‹è¯•é”™è¯¯".to_string()) };
        
        benchmarker.record_operation(operation_latency, success, error).await?;
        sleep(Duration::from_millis(180)).await;
    }

    let result = benchmarker.stop_benchmark().await?;
    println!("  å†…å­˜æµ‹è¯•å®Œæˆ:");
    println!("    å¹³å‡å†…å­˜ä½¿ç”¨: {} å­—èŠ‚", result.avg_memory_usage);
    println!("    å³°å€¼å†…å­˜ä½¿ç”¨: {} å­—èŠ‚", result.peak_memory_usage);
    println!("    å¹³å‡CPUä½¿ç”¨ç‡: {:.2}%", result.avg_cpu_usage);

    Ok(())
}

/// è¿è¡Œç¨³å®šæ€§åŸºå‡†æµ‹è¯•
async fn run_stability_benchmark(benchmarker: &Benchmarker) -> Result<(), Box<dyn std::error::Error>> {
    let config = BenchmarkConfig {
        benchmark_type: BenchmarkType::Stability,
        duration: Duration::from_secs(40),
        concurrency: 2,
        data_size: 1024,
        detailed_stats: true,
        sampling_interval: Duration::from_millis(500),
        warmup_duration: Duration::from_secs(5),
    };

    println!("  å¼€å§‹ç¨³å®šæ€§åŸºå‡†æµ‹è¯• (40ç§’)...");
    benchmarker.start_benchmark("stability_test".to_string(), config).await?;

    // æ¨¡æ‹Ÿé•¿æ—¶é—´ç¨³å®šè¿è¡Œ
    for i in 0..120 {
        let operation_latency = Duration::from_millis(40 + (i % 30)); // 40-70ms
        let success = i % 25 != 0; // 96%æˆåŠŸç‡
        let error = if success { None } else { Some("ç¨³å®šæ€§æµ‹è¯•é”™è¯¯".to_string()) };
        
        benchmarker.record_operation(operation_latency, success, error).await?;
        sleep(Duration::from_millis(300)).await;
    }

    let result = benchmarker.stop_benchmark().await?;
    println!("  ç¨³å®šæ€§æµ‹è¯•å®Œæˆ:");
    println!("    è¿è¡Œæ—¶é—´: {:?}", result.total_duration);
    println!("    æ€»æ“ä½œæ•°: {}", result.total_operations);
    println!("    é”™è¯¯ç‡: {:.2}%", result.error_rate);
    println!("    å¹³å‡å»¶è¿Ÿ: {:?}", result.avg_latency);

    Ok(())
}

/// è¿è¡Œå‹åŠ›æµ‹è¯•
async fn run_stress_benchmark(benchmarker: &Benchmarker) -> Result<(), Box<dyn std::error::Error>> {
    let config = BenchmarkConfig {
        benchmark_type: BenchmarkType::Stress,
        duration: Duration::from_secs(30),
        concurrency: 20,
        data_size: 4096,
        detailed_stats: true,
        sampling_interval: Duration::from_millis(50),
        warmup_duration: Duration::from_secs(2),
    };

    println!("  å¼€å§‹å‹åŠ›æµ‹è¯• (30ç§’, 20å¹¶å‘)...");
    benchmarker.start_benchmark("stress_test".to_string(), config).await?;

    // æ¨¡æ‹Ÿé«˜å‹åŠ›æ“ä½œ
    for i in 0..300 {
        let operation_latency = Duration::from_millis(5 + (i % 25)); // 5-30ms
        let success = i % 8 != 0; // 87%æˆåŠŸç‡
        let error = if success { None } else { Some("å‹åŠ›æµ‹è¯•é”™è¯¯".to_string()) };
        
        benchmarker.record_operation(operation_latency, success, error).await?;
        sleep(Duration::from_millis(100)).await;
    }

    let result = benchmarker.stop_benchmark().await?;
    println!("  å‹åŠ›æµ‹è¯•å®Œæˆ:");
    println!("    å¹¶å‘æ•°: 20");
    println!("    æ€»æ“ä½œæ•°: {}", result.total_operations);
    println!("    ååé‡: {:.2} æ“ä½œ/ç§’", result.throughput);
    println!("    é”™è¯¯ç‡: {:.2}%", result.error_rate);
    println!("    å³°å€¼CPUä½¿ç”¨ç‡: {:.2}%", result.peak_cpu_usage);

    Ok(())
}

/// åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ
async fn analyze_benchmark_results(benchmarker: &Benchmarker) -> Result<(), Box<dyn std::error::Error>> {
    let all_results = benchmarker.get_all_results().await;
    
    println!("  åŸºå‡†æµ‹è¯•ç»“æœåˆ†æ:");
    println!("    æ€»æµ‹è¯•æ•°: {}", all_results.len());
    
    // æ‰¾å‡ºæœ€ä½³æ€§èƒ½
    let best_throughput = all_results.iter()
        .max_by(|a, b| a.throughput.partial_cmp(&b.throughput).unwrap())
        .unwrap();
    println!("    æœ€é«˜ååé‡: {:.2} æ“ä½œ/ç§’ ({})", best_throughput.throughput, best_throughput.name);
    
    let lowest_latency = all_results.iter()
        .min_by(|a, b| a.avg_latency.cmp(&b.avg_latency))
        .unwrap();
    println!("    æœ€ä½å»¶è¿Ÿ: {:?} ({})", lowest_latency.avg_latency, lowest_latency.name);
    
    let lowest_error_rate = all_results.iter()
        .min_by(|a, b| a.error_rate.partial_cmp(&b.error_rate).unwrap())
        .unwrap();
    println!("    æœ€ä½é”™è¯¯ç‡: {:.2}% ({})", lowest_error_rate.error_rate, lowest_error_rate.name);

    // æ€§èƒ½å¯¹æ¯”
    if all_results.len() >= 2 {
        let result1 = &all_results[0];
        let result2 = &all_results[1];
        
        if let Ok(comparison) = benchmarker.compare_results(&result1.name, &result2.name).await {
            println!("  æ€§èƒ½å¯¹æ¯” ({} vs {}):", result1.name, result2.name);
            println!("    å»¶è¿Ÿæ”¹è¿›: {:.1}%", comparison.latency_improvement);
            println!("    ååé‡æ”¹è¿›: {:.1}%", comparison.throughput_improvement);
            println!("    å†…å­˜ä½¿ç”¨æ”¹è¿›: {:.1}%", comparison.memory_improvement);
            println!("    CPUä½¿ç”¨æ”¹è¿›: {:.1}%", comparison.cpu_improvement);
            println!("    é”™è¯¯ç‡æ”¹è¿›: {:.1}%", comparison.error_rate_improvement);
        }
    }

    Ok(())
}

/// ç”Ÿæˆç»¼åˆæŠ¥å‘Š
async fn generate_comprehensive_report(benchmarker: &Benchmarker) -> Result<(), Box<dyn std::error::Error>> {
    let all_results = benchmarker.get_all_results().await;
    
    println!("  ç”Ÿæˆç»¼åˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š...");
    
    let mut report = String::new();
    report.push_str("# IoTç³»ç»ŸåŸºå‡†æµ‹è¯•ç»¼åˆæŠ¥å‘Š\n\n");
    report.push_str(&format!("ç”Ÿæˆæ—¶é—´: {}\n", chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC")));
    report.push_str(&format!("æµ‹è¯•æ€»æ•°: {}\n\n", all_results.len()));

    // æ€»ä½“ç»Ÿè®¡
    let total_operations: u64 = all_results.iter().map(|r| r.total_operations).sum();
    let total_duration: Duration = all_results.iter().map(|r| r.total_duration).sum();
    let avg_throughput: f64 = all_results.iter().map(|r| r.throughput).sum::<f64>() / all_results.len() as f64;
    let avg_error_rate: f64 = all_results.iter().map(|r| r.error_rate).sum::<f64>() / all_results.len() as f64;

    report.push_str("## æ€»ä½“ç»Ÿè®¡\n\n");
    report.push_str(&format!("æ€»æ“ä½œæ•°: {}\n", total_operations));
    report.push_str(&format!("æ€»æµ‹è¯•æ—¶é—´: {:?}\n", total_duration));
    report.push_str(&format!("å¹³å‡ååé‡: {:.2} æ“ä½œ/ç§’\n", avg_throughput));
    report.push_str(&format!("å¹³å‡é”™è¯¯ç‡: {:.2}%\n\n", avg_error_rate));

    // å„æµ‹è¯•è¯¦ç»†ç»“æœ
    report.push_str("## è¯¦ç»†æµ‹è¯•ç»“æœ\n\n");
    for result in &all_results {
        report.push_str(&format!("### {}\n", result.name));
        report.push_str(&format!("- æµ‹è¯•ç±»å‹: {:?}\n", result.benchmark_type));
        report.push_str(&format!("- æŒç»­æ—¶é—´: {:?}\n", result.total_duration));
        report.push_str(&format!("- æ€»æ“ä½œæ•°: {}\n", result.total_operations));
        report.push_str(&format!("- æˆåŠŸç‡: {:.1}%\n", (result.successful_operations as f64 / result.total_operations as f64) * 100.0));
        report.push_str(&format!("- å¹³å‡å»¶è¿Ÿ: {:?}\n", result.avg_latency));
        report.push_str(&format!("- ååé‡: {:.2} æ“ä½œ/ç§’\n", result.throughput));
        report.push_str(&format!("- é”™è¯¯ç‡: {:.2}%\n", result.error_rate));
        report.push_str(&format!("- å¹³å‡å†…å­˜ä½¿ç”¨: {} å­—èŠ‚\n", result.avg_memory_usage));
        report.push_str(&format!("- å¹³å‡CPUä½¿ç”¨ç‡: {:.2}%\n\n", result.avg_cpu_usage));
    }

    // æ€§èƒ½å»ºè®®
    report.push_str("## æ€§èƒ½ä¼˜åŒ–å»ºè®®\n\n");
    if avg_error_rate > 5.0 {
        report.push_str("- é”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥é”™è¯¯å¤„ç†æœºåˆ¶å’Œç³»ç»Ÿç¨³å®šæ€§\n");
    }
    if avg_throughput < 100.0 {
        report.push_str("- ååé‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•å’Œå¹¶å‘å¤„ç†\n");
    }
    if all_results.iter().any(|r| r.avg_latency > Duration::from_millis(100)) {
        report.push_str("- å­˜åœ¨é«˜å»¶è¿Ÿæ“ä½œï¼Œå»ºè®®ä¼˜åŒ–ç½‘ç»œé€šä¿¡å’Œæ•°æ®å¤„ç†\n");
    }
    if all_results.iter().any(|r| r.avg_cpu_usage > 80.0) {
        report.push_str("- CPUä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–è®¡ç®—å¯†é›†å‹æ“ä½œ\n");
    }
    if all_results.iter().any(|r| r.avg_memory_usage > 100 * 1024 * 1024) {
        report.push_str("- å†…å­˜ä½¿ç”¨é‡è¾ƒå¤§ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†å’Œæ•°æ®ç»“æ„\n");
    }

    println!("  ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ ({} å­—ç¬¦)", report.len());
    
    // æ˜¾ç¤ºæŠ¥å‘Šçš„å…³é”®éƒ¨åˆ†
    let lines: Vec<&str> = report.lines().take(30).collect();
    for line in lines {
        println!("  {}", line);
    }
    println!("  ... (æŠ¥å‘Šå·²æˆªæ–­)");

    Ok(())
}
