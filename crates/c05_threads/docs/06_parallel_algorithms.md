# Rust 2025 å¹¶è¡Œç®—æ³• (c05_threads_06)

> **å…ƒæ•°æ®**
> æ–‡æ¡£ç¼–å·: c05_threads_06
> åˆ›å»ºæ—¥æœŸ: 2025-01-27
> æœ€åæ›´æ–°: 2025-10-19 (å¢å¼ºç‰ˆ)
> é€‚ç”¨ç‰ˆæœ¬: Rust 1.92.0+
> å¢å¼ºå†…å®¹: âœ… çŸ¥è¯†å›¾è°± | âœ… å¤šç»´å¯¹æ¯” | âœ… Rust 1.92.0 ç¤ºä¾‹ | âœ… æ€ç»´å¯¼å›¾
> çŠ¶æ€: âœ… å·²å®Œæˆ

---

## ğŸ“Š ç›®å½•

- [Rust 2025 å¹¶è¡Œç®—æ³• (c05\_threads\_06)](#rust-2025-å¹¶è¡Œç®—æ³•-c05_threads_06)
  - [ğŸ“Š ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ å¹¶è¡Œç®—æ³•æ ¸å¿ƒçŸ¥è¯†å›¾è°±](#-å¹¶è¡Œç®—æ³•æ ¸å¿ƒçŸ¥è¯†å›¾è°±)
    - [å¹¶è¡Œç®—æ³•åˆ†ç±»å…³ç³»å›¾](#å¹¶è¡Œç®—æ³•åˆ†ç±»å…³ç³»å›¾)
    - [å¹¶è¡Œç®—æ³•é€‰æ‹©å†³ç­–æ ‘](#å¹¶è¡Œç®—æ³•é€‰æ‹©å†³ç­–æ ‘)
  - [ğŸ“Š å¹¶è¡Œç®—æ³•å¤šç»´å¯¹æ¯”çŸ©é˜µ](#-å¹¶è¡Œç®—æ³•å¤šç»´å¯¹æ¯”çŸ©é˜µ)
    - [å¹¶è¡Œç®—æ³•æ€§èƒ½å¯¹æ¯”](#å¹¶è¡Œç®—æ³•æ€§èƒ½å¯¹æ¯”)
    - [å¹¶è¡Œç®—æ³•é€‚ç”¨åœºæ™¯å¯¹æ¯”](#å¹¶è¡Œç®—æ³•é€‚ç”¨åœºæ™¯å¯¹æ¯”)
    - [Rayon vs æ‰‹å·¥å¹¶è¡Œå¯¹æ¯”](#rayon-vs-æ‰‹å·¥å¹¶è¡Œå¯¹æ¯”)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 å¹¶è¡Œç®—æ³•åˆ†ç±»](#11-å¹¶è¡Œç®—æ³•åˆ†ç±»)
    - [1.2 æ€§èƒ½æ¨¡å‹](#12-æ€§èƒ½æ¨¡å‹)
      - [å®šä¹‰ 1.1 (åˆ†æ²»å¤æ‚åº¦)](#å®šä¹‰-11-åˆ†æ²»å¤æ‚åº¦)
    - [1.3 è®¾è®¡åŸåˆ™](#13-è®¾è®¡åŸåˆ™)
  - [2. åˆ†æ²»ç­–ç•¥](#2-åˆ†æ²»ç­–ç•¥)
    - [2.1 åŸºæœ¬åˆ†æ²»æ¡†æ¶](#21-åŸºæœ¬åˆ†æ²»æ¡†æ¶)
      - [2.1.1 é€šç”¨åˆ†æ²»æ¡†æ¶](#211-é€šç”¨åˆ†æ²»æ¡†æ¶)
    - [2.2 å¹¶è¡Œå½’å¹¶æ’åº](#22-å¹¶è¡Œå½’å¹¶æ’åº)
      - [2.2.1 åŸºæœ¬å¹¶è¡Œå½’å¹¶æ’åº](#221-åŸºæœ¬å¹¶è¡Œå½’å¹¶æ’åº)
    - [2.3 å¹¶è¡Œå¿«é€Ÿæ’åº](#23-å¹¶è¡Œå¿«é€Ÿæ’åº)
      - [2.3.1 å¹¶è¡Œå¿«é€Ÿæ’åºå®ç°](#231-å¹¶è¡Œå¿«é€Ÿæ’åºå®ç°)
  - [3. å¹¶è¡Œå½’çº¦](#3-å¹¶è¡Œå½’çº¦)
    - [3.1 åŸºæœ¬å½’çº¦ç®—æ³•](#31-åŸºæœ¬å½’çº¦ç®—æ³•)
      - [3.1.1 å¹¶è¡Œå½’çº¦å®ç°](#311-å¹¶è¡Œå½’çº¦å®ç°)
    - [3.2 æ ‘å½¢å½’çº¦](#32-æ ‘å½¢å½’çº¦)
      - [3.2.1 æ ‘å½¢å½’çº¦ç®—æ³•](#321-æ ‘å½¢å½’çº¦ç®—æ³•)
  - [4. å¹¶è¡Œæ˜ å°„](#4-å¹¶è¡Œæ˜ å°„)
    - [4.1 åŸºæœ¬æ˜ å°„ç®—æ³•](#41-åŸºæœ¬æ˜ å°„ç®—æ³•)
      - [4.1.1 å¹¶è¡Œæ˜ å°„å®ç°](#411-å¹¶è¡Œæ˜ å°„å®ç°)
    - [4.2 åˆ†å—æ˜ å°„](#42-åˆ†å—æ˜ å°„)
      - [4.2.1 åˆ†å—æ˜ å°„ä¼˜åŒ–](#421-åˆ†å—æ˜ å°„ä¼˜åŒ–)
  - [5. å¹¶è¡Œæ‰«æ](#5-å¹¶è¡Œæ‰«æ)
    - [5.1 å‰ç¼€å’Œç®—æ³•](#51-å‰ç¼€å’Œç®—æ³•)
      - [5.1.1 å¹¶è¡Œå‰ç¼€å’Œ](#511-å¹¶è¡Œå‰ç¼€å’Œ)
  - [6. å¹¶è¡Œæœç´¢](#6-å¹¶è¡Œæœç´¢)
    - [6.1 å¹¶è¡Œçº¿æ€§æœç´¢](#61-å¹¶è¡Œçº¿æ€§æœç´¢)
      - [6.1.1 å¹¶è¡Œçº¿æ€§æœç´¢å®ç°](#611-å¹¶è¡Œçº¿æ€§æœç´¢å®ç°)
    - [6.2 å¹¶è¡ŒäºŒåˆ†æœç´¢](#62-å¹¶è¡ŒäºŒåˆ†æœç´¢)
      - [6.2.1 å¹¶è¡ŒäºŒåˆ†æœç´¢å®ç°](#621-å¹¶è¡ŒäºŒåˆ†æœç´¢å®ç°)
  - [7. å›¾ç®—æ³•å¹¶è¡ŒåŒ–](#7-å›¾ç®—æ³•å¹¶è¡ŒåŒ–)
    - [7.1 å¹¶è¡ŒBFS](#71-å¹¶è¡Œbfs)
      - [7.1.1 å¹¶è¡Œå¹¿åº¦ä¼˜å…ˆæœç´¢](#711-å¹¶è¡Œå¹¿åº¦ä¼˜å…ˆæœç´¢)
  - [8. æœ€ä½³å®è·µ](#8-æœ€ä½³å®è·µ)
    - [8.1 ä»»åŠ¡ç²’åº¦æ§åˆ¶](#81-ä»»åŠ¡ç²’åº¦æ§åˆ¶)
      - [8.1.1 è‡ªé€‚åº”ä»»åŠ¡ç²’åº¦](#811-è‡ªé€‚åº”ä»»åŠ¡ç²’åº¦)
    - [8.2 è´Ÿè½½å‡è¡¡](#82-è´Ÿè½½å‡è¡¡)
      - [8.2.1 å·¥ä½œçªƒå–è´Ÿè½½å‡è¡¡](#821-å·¥ä½œçªƒå–è´Ÿè½½å‡è¡¡)
    - [8.3 æ€§èƒ½ä¼˜åŒ–](#83-æ€§èƒ½ä¼˜åŒ–)
      - [8.3.1 ç¼“å­˜å‹å¥½çš„å¹¶è¡Œç®—æ³•](#831-ç¼“å­˜å‹å¥½çš„å¹¶è¡Œç®—æ³•)
  - [ğŸ’¡ æ€ç»´å¯¼å›¾ï¼šå¹¶è¡Œç®—æ³•è®¾è®¡ç­–ç•¥](#-æ€ç»´å¯¼å›¾å¹¶è¡Œç®—æ³•è®¾è®¡ç­–ç•¥)
  - [ğŸ“‹ å¿«é€Ÿå‚è€ƒ](#-å¿«é€Ÿå‚è€ƒ)
    - [Rayon å¹¶è¡Œç®—æ³• API é€ŸæŸ¥](#rayon-å¹¶è¡Œç®—æ³•-api-é€ŸæŸ¥)
    - [å¹¶è¡Œç®—æ³•å¤æ‚åº¦é€ŸæŸ¥](#å¹¶è¡Œç®—æ³•å¤æ‚åº¦é€ŸæŸ¥)
    - [æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•](#æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•)
    - [Rust 1.92.0 å¹¶è¡Œæ€§èƒ½æå‡æ±‡æ€»](#rust-1920-å¹¶è¡Œæ€§èƒ½æå‡æ±‡æ€»)
  - [9. æ€»ç»“](#9-æ€»ç»“)
    - [æ ¸å¿ƒä¼˜åŠ¿](#æ ¸å¿ƒä¼˜åŠ¿)
    - [9.1 å…³é”®è¦ç‚¹](#91-å…³é”®è¦ç‚¹)
    - [Rust 1.92.0 å…³é”®æ”¹è¿›](#rust-1920-å…³é”®æ”¹è¿›)
    - [9.2 æœ€ä½³å®è·µ](#92-æœ€ä½³å®è·µ)
    - [æ€§èƒ½æƒè¡¡](#æ€§èƒ½æƒè¡¡)
    - [9.3 æœªæ¥å‘å±•æ–¹å‘](#93-æœªæ¥å‘å±•æ–¹å‘)
    - [å­¦ä¹ è·¯å¾„](#å­¦ä¹ è·¯å¾„)

---

## ğŸ¯ å¹¶è¡Œç®—æ³•æ ¸å¿ƒçŸ¥è¯†å›¾è°±

### å¹¶è¡Œç®—æ³•åˆ†ç±»å…³ç³»å›¾

```mermaid
graph TB
    A[å¹¶è¡Œç®—æ³•] --> B[æ•°æ®å¹¶è¡Œ]
    A --> C[ä»»åŠ¡å¹¶è¡Œ]
    A --> D[æµæ°´çº¿å¹¶è¡Œ]
    A --> E[åˆ†æ²»å¹¶è¡Œ]

    B --> B1[å¹¶è¡Œæ˜ å°„ Map]
    B --> B2[å¹¶è¡Œå½’çº¦ Reduce]
    B --> B3[å¹¶è¡Œæ‰«æ Scan]
    B --> B4[å¹¶è¡Œç­›é€‰ Filter]

    C --> C1[ç‹¬ç«‹ä»»åŠ¡è°ƒåº¦]
    C --> C2[ä»»åŠ¡ä¾èµ–å›¾]
    C --> C3[å·¥ä½œçªƒå–]

    D --> D1[æµæ°´çº¿é˜¶æ®µ]
    D --> D2[ç¼“å†²é˜Ÿåˆ—]
    D --> D3[èƒŒå‹æ§åˆ¶]

    E --> E1[å¹¶è¡Œå½’å¹¶æ’åº]
    E --> E2[å¹¶è¡Œå¿«é€Ÿæ’åº]
    E --> E3[å¹¶è¡Œæœç´¢]
    E --> E4[å¹¶è¡Œå›¾ç®—æ³•]

    B1 -->|é€‚ç”¨| F1[å…ƒç´ ç‹¬ç«‹å¤„ç†]
    B2 -->|é€‚ç”¨| F2[èšåˆè®¡ç®—]
    E1 -->|é€‚ç”¨| F3[æ’åºé—®é¢˜]
    E3 -->|é€‚ç”¨| F4[æŸ¥æ‰¾é—®é¢˜]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#e8f5e9
    style D fill:#f3e5f5
    style E fill:#fce4ec
```

### å¹¶è¡Œç®—æ³•é€‰æ‹©å†³ç­–æ ‘

```mermaid
graph TD
    Start[é€‰æ‹©å¹¶è¡Œç®—æ³•] --> Q1{é—®é¢˜ç±»å‹?}

    Q1 -->|æ’åº| Q2{æ•°æ®è§„æ¨¡?}
    Q1 -->|æœç´¢| Q3{æ•°æ®æœ‰åº?}
    Q1 -->|èšåˆ| Reduce[å¹¶è¡Œå½’çº¦]
    Q1 -->|è½¬æ¢| Map[å¹¶è¡Œæ˜ å°„]

    Q2 -->|å¤§è§„æ¨¡| MergeSort[å¹¶è¡Œå½’å¹¶æ’åº]
    Q2 -->|ä¸­ç­‰| QuickSort[å¹¶è¡Œå¿«é€Ÿæ’åº]

    Q3 -->|æœ‰åº| BinarySearch[å¹¶è¡ŒäºŒåˆ†æœç´¢]
    Q3 -->|æ— åº| LinearSearch[å¹¶è¡Œçº¿æ€§æœç´¢]

    MergeSort --> Perf1[O(n log n)<br/>ç¨³å®š]
    QuickSort --> Perf2[å¹³å‡O(n log n)<br/>ä¸ç¨³å®š]
    Reduce --> Perf3[O(log n)<br/>ç»“åˆå¾‹]

    style Start fill:#e1f5ff
    style MergeSort fill:#c8e6c9
    style Reduce fill:#fff59d
    style Map fill:#90caf9
```

---

## ğŸ“Š å¹¶è¡Œç®—æ³•å¤šç»´å¯¹æ¯”çŸ©é˜µ

### å¹¶è¡Œç®—æ³•æ€§èƒ½å¯¹æ¯”

| ç®—æ³• | é¡ºåºå¤æ‚åº¦ | å¹¶è¡Œå¤æ‚åº¦ | åŠ é€Ÿæ¯” | å†…å­˜å¼€é”€ | å¯æ‰©å±•æ€§ | ç¨³å®šæ€§ | Rust 1.92.0 ä¼˜åŒ– |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **å¹¶è¡Œå½’å¹¶æ’åº** | O(n log n) | O(logÂ²n) | O(n/log n) | â­â­â­ | â­â­â­â­â­ | âœ… ç¨³å®š | +15% SIMD |
| **å¹¶è¡Œå¿«é€Ÿæ’åº** | O(n log n) | O(log n) | O(n) | â­â­â­â­ | â­â­â­â­ | âŒ ä¸ç¨³å®š | +20% ä¼˜åŒ– |
| **å¹¶è¡Œå½’çº¦** | O(n) | O(log n) | O(n/log n) | â­â­â­â­â­ | â­â­â­â­â­ | âœ… ç¨³å®š | +10% æ ‘å½¢ä¼˜åŒ– |
| **å¹¶è¡Œæ˜ å°„** | O(n) | O(1) | O(n) | â­â­â­â­â­ | â­â­â­â­â­ | âœ… ç¨³å®š | +8% å‘é‡åŒ– |
| **å¹¶è¡Œæ‰«æ** | O(n) | O(log n) | O(n/log n) | â­â­â­â­ | â­â­â­â­ | âœ… ç¨³å®š | +12% ä¼˜åŒ– |
| **å¹¶è¡Œæœç´¢** | O(n) | O(n/p) | O(p) | â­â­â­â­â­ | â­â­â­â­ | N/A | +5% ç¼“å­˜ä¼˜åŒ– |

> **å›¾ä¾‹**: â­ è¶Šå¤šè¡¨ç¤ºå¼€é”€è¶Šä½ï¼›p = å¤„ç†å™¨æ•°é‡

### å¹¶è¡Œç®—æ³•é€‚ç”¨åœºæ™¯å¯¹æ¯”

| ç®—æ³• | æœ€ä½³åº”ç”¨åœºæ™¯ | ä¸é€‚ç”¨åœºæ™¯ | æ•°æ®ä¾èµ– | é€šä¿¡å¼€é”€ | å…¸å‹åŠ é€Ÿæ¯” |
| --- | --- | --- | --- | --- | --- |
| **å¹¶è¡Œå½’å¹¶æ’åº** | å¤§è§„æ¨¡ç¨³å®šæ’åº | å°æ•°æ®é›† | ä½ | ä¸­ç­‰ | 4-8x (8æ ¸) |
| **å¹¶è¡Œå¿«é€Ÿæ’åº** | ä¸€èˆ¬æ’åºä»»åŠ¡ | éœ€è¦ç¨³å®šæ€§ | ä½ | ä½ | 6-12x (8æ ¸) |
| **å¹¶è¡Œå½’çº¦** | æ±‚å’Œã€æœ€å¤§æœ€å°å€¼ | éç»“åˆè¿ç®— | ä½ | ä½ | 8-16x (8æ ¸) |
| **å¹¶è¡Œæ˜ å°„** | ç‹¬ç«‹å…ƒç´ å˜æ¢ | æœ‰æ•°æ®ä¾èµ– | æ—  | æä½ | æ¥è¿‘p |
| **å¹¶è¡Œæ‰«æ** | å‰ç¼€å’Œã€ç§¯åˆ† | å°æ•°æ®é›† | é«˜ | ä¸­ç­‰ | 4-8x (8æ ¸) |
| **å¹¶è¡ŒBFS** | å›¾éå†ã€æœ€çŸ­è·¯ | ç¨€ç–å›¾ | é«˜ | é«˜ | 2-6x (8æ ¸) |

### Rayon vs æ‰‹å·¥å¹¶è¡Œå¯¹æ¯”

| ç‰¹æ€§ | Rayon | æ‰‹å·¥çº¿ç¨‹ | æ— é”ç®—æ³• | æ¨èåœºæ™¯ |
| --- | --- | --- | --- | --- |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­ | â­ | Rayon: å¿«é€Ÿå¼€å‘ |
| **æ€§èƒ½** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | æ— é”: æè‡´æ€§èƒ½ |
| **çµæ´»æ€§** | â­â­â­ | â­â­â­â­â­ | â­â­â­ | æ‰‹å·¥: ç‰¹æ®Šéœ€æ±‚ |
| **å®‰å…¨æ€§** | â­â­â­â­â­ | â­â­â­ | â­â­ | Rayon: ç”Ÿäº§ç¯å¢ƒ |
| **è°ƒè¯•éš¾åº¦** | â­â­ | â­â­â­â­ | â­â­â­â­â­ | Rayon: æ˜“äºè°ƒè¯• |

---

## 1. æ¦‚è¿°

### 1.1 å¹¶è¡Œç®—æ³•åˆ†ç±»

å¹¶è¡Œç®—æ³•æŒ‰å¤„ç†æ–¹å¼å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š

- **æ•°æ®å¹¶è¡Œ**: å°†æ•°æ®åˆ†å‰²åˆ°ä¸åŒå¤„ç†å™¨ä¸Šå¹¶è¡Œå¤„ç†
- **ä»»åŠ¡å¹¶è¡Œ**: å°†ä¸åŒä»»åŠ¡åˆ†é…ç»™ä¸åŒå¤„ç†å™¨
- **æµæ°´çº¿å¹¶è¡Œ**: å°†ç®—æ³•åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µï¼Œå½¢æˆæµæ°´çº¿
- **åˆ†æ²»å¹¶è¡Œ**: å°†é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜å¹¶è¡Œè§£å†³

### 1.2 æ€§èƒ½æ¨¡å‹

#### å®šä¹‰ 1.1 (åˆ†æ²»å¤æ‚åº¦)

å¯¹äºé—®é¢˜è§„æ¨¡ $n$ å’Œçº¿ç¨‹æ•° $p$ï¼Œåˆ†æ²»ç®—æ³•çš„å¤æ‚åº¦ä¸ºï¼š

$$T(n, p) = T_{divide}(n) + T_{conquer}(n/p) + T_{merge}(n)$$

å…¶ä¸­ï¼š

- $T_{divide}$ æ˜¯åˆ†å‰²æ—¶é—´
- $T_{conquer}$ æ˜¯å¹¶è¡Œè§£å†³æ—¶é—´
- $T_{merge}$ æ˜¯åˆå¹¶æ—¶é—´

### 1.3 è®¾è®¡åŸåˆ™

å¹¶è¡Œç®—æ³•è®¾è®¡åº”éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. **å¯æ‰©å±•æ€§**: æ€§èƒ½åº”éšå¤„ç†å™¨æ•°é‡çº¿æ€§å¢é•¿
2. **è´Ÿè½½å‡è¡¡**: å·¥ä½œè´Ÿè½½åº”å‡åŒ€åˆ†å¸ƒ
3. **æœ€å°é€šä¿¡**: å‡å°‘çº¿ç¨‹é—´é€šä¿¡å¼€é”€
4. **å®¹é”™æ€§**: å¤„ç†éƒ¨åˆ†å¤±è´¥çš„æƒ…å†µ

## 2. åˆ†æ²»ç­–ç•¥

### 2.1 åŸºæœ¬åˆ†æ²»æ¡†æ¶

#### 2.1.1 é€šç”¨åˆ†æ²»æ¡†æ¶

```rust
use std::sync::{Arc, Mutex};
use std::thread;

struct DivideAndConquer<T, R> {
    threshold: usize,
    thread_pool: Arc<ThreadPool>,
}

impl<T, R> DivideAndConquer<T, R>
where
    T: Send + Sync + Clone,
    R: Send + Sync + Clone,
{
    fn new(threshold: usize, num_threads: usize) -> Self {
        Self {
            threshold,
            thread_pool: Arc::new(ThreadPool::new(num_threads)),
        }
    }

    fn solve<F, G, H>(
        &self,
        problem: T,
        divide: F,
        conquer: G,
        merge: H,
    ) -> R
    where
        F: Fn(T) -> Vec<T> + Send + Sync,
        G: Fn(T) -> R + Send + Sync,
        H: Fn(Vec<R>) -> R + Send + Sync,
    {
        if self.should_conquer(&problem) {
            conquer(problem)
        } else {
            let sub_problems = divide(problem);
            let results = Arc::new(Mutex::new(Vec::new()));

            let mut handles = vec![];

            for sub_problem in sub_problems {
                let results = Arc::clone(&results);
                let conquer = conquer.clone();

                let handle = thread::spawn(move || {
                    let result = conquer(sub_problem);
                    results.lock().unwrap().push(result);
                });
                handles.push(handle);
            }

            for handle in handles {
                handle.join().unwrap();
            }

            let results = results.lock().unwrap().clone();
            merge(results)
        }
    }

    fn should_conquer(&self, problem: &T) -> bool {
        // å®ç°é—®é¢˜è§„æ¨¡åˆ¤æ–­é€»è¾‘
        true // ç®€åŒ–å®ç°
    }
}

// ç®€åŒ–çš„çº¿ç¨‹æ± å®ç°
struct ThreadPool {
    workers: Vec<thread::JoinHandle<()>>,
}

impl ThreadPool {
    fn new(_size: usize) -> Self {
        Self { workers: vec![] }
    }
}
```

### 2.2 å¹¶è¡Œå½’å¹¶æ’åº

#### 2.2.1 åŸºæœ¬å¹¶è¡Œå½’å¹¶æ’åº

```rust
use std::sync::{Arc, Mutex};
use std::thread;

struct ParallelMergeSort {
    threshold: usize,
}

impl ParallelMergeSort {
    fn new(threshold: usize) -> Self {
        Self { threshold }
    }

    fn sort(&self, data: &mut [i32]) -> Vec<i32> {
        if data.len() <= self.threshold {
            // ä¸²è¡Œæ’åº
            let mut sorted = data.to_vec();
            sorted.sort();
            sorted
        } else {
            // å¹¶è¡Œæ’åº
            let mid = data.len() / 2;
            let (left, right) = data.split_at_mut(mid);

            let left_data = left.to_vec();
            let right_data = right.to_vec();

            let left_handle = thread::spawn(move || {
                let mut left_sorted = left_data;
                left_sorted.sort();
                left_sorted
            });

            let right_handle = thread::spawn(move || {
                let mut right_sorted = right_data;
                right_sorted.sort();
                right_sorted
            });

            let left_sorted = left_handle.join().unwrap();
            let right_sorted = right_handle.join().unwrap();

            // åˆå¹¶ä¸¤ä¸ªæœ‰åºæ•°ç»„
            self.merge(&left_sorted, &right_sorted)
        }
    }

    fn merge(&self, left: &[i32], right: &[i32]) -> Vec<i32> {
        let mut result = Vec::with_capacity(left.len() + right.len());
        let mut i = 0;
        let mut j = 0;

        while i < left.len() && j < right.len() {
            if left[i] <= right[j] {
                result.push(left[i]);
                i += 1;
            } else {
                result.push(right[j]);
                j += 1;
            }
        }

        result.extend_from_slice(&left[i..]);
        result.extend_from_slice(&right[j..]);

        result
    }
}

fn main() {
    let mut data = vec![64, 34, 25, 12, 22, 11, 90];
    let sorter = ParallelMergeSort::new(3);
    let sorted = sorter.sort(&mut data);
    println!("Sorted: {:?}", sorted);
}
```

### 2.3 å¹¶è¡Œå¿«é€Ÿæ’åº

#### 2.3.1 å¹¶è¡Œå¿«é€Ÿæ’åºå®ç°

```rust
use std::sync::{Arc, Mutex};
use std::thread;

struct ParallelQuickSort {
    threshold: usize,
}

impl ParallelQuickSort {
    fn new(threshold: usize) -> Self {
        Self { threshold }
    }

    fn sort(&self, data: &mut [i32]) {
        if data.len() <= self.threshold {
            data.sort();
            return;
        }

        let pivot_index = self.partition(data);

        if pivot_index > 0 {
            let (left, right) = data.split_at_mut(pivot_index);

            // å¹¶è¡Œå¤„ç†å·¦å³ä¸¤éƒ¨åˆ†
            let left_handle = thread::spawn(move || {
                let mut left_sorter = ParallelQuickSort::new(100);
                left_sorter.sort(left);
            });

            let right_handle = thread::spawn(move || {
                let mut right_sorter = ParallelQuickSort::new(100);
                right_sorter.sort(right);
            });

            left_handle.join().unwrap();
            right_handle.join().unwrap();
        }
    }

    fn partition(&self, data: &mut [i32]) -> usize {
        let len = data.len();
        let pivot = data[len - 1];
        let mut i = 0;

        for j in 0..len - 1 {
            if data[j] <= pivot {
                data.swap(i, j);
                i += 1;
            }
        }

        data.swap(i, len - 1);
        i
    }
}

fn main() {
    let mut data = vec![64, 34, 25, 12, 22, 11, 90];
    let mut sorter = ParallelQuickSort::new(3);
    sorter.sort(&mut data);
    println!("Sorted: {:?}", data);
}
```

## 3. å¹¶è¡Œå½’çº¦

### 3.1 åŸºæœ¬å½’çº¦ç®—æ³•

#### 3.1.1 å¹¶è¡Œå½’çº¦å®ç°

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn parallel_reduce<T, F>(
    data: &[T],
    num_threads: usize,
    identity: T,
    op: F,
) -> T
where
    T: Send + Sync + Clone,
    F: Fn(T, &T) -> T + Send + Sync,
{
    if data.is_empty() {
        return identity;
    }

    let chunk_size = (data.len() + num_threads - 1) / num_threads;
    let data = Arc::new(data.to_vec());
    let results = Arc::new(Mutex::new(Vec::new()));

    let handles: Vec<_> = (0..num_threads)
        .map(|i| {
            let data = Arc::clone(&data);
            let results = Arc::clone(&results);

            thread::spawn(move || {
                let start = i * chunk_size;
                let end = std::cmp::min(start + chunk_size, data.len());

                if start < end {
                    let chunk = &data[start..end];
                    let local_result = chunk.iter().fold(identity.clone(), |acc, x| op(acc, x));

                    let mut results = results.lock().unwrap();
                    results.push(local_result);
                }
            })
        })
        .collect();

    for handle in handles {
        handle.join().unwrap();
    }

    let results = results.lock().unwrap();
    results.iter().fold(identity, |acc, x| op(acc, x))
}

fn main() {
    let data = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
    let sum = parallel_reduce(&data, 4, 0, |acc, &x| acc + x);
    println!("Sum: {}", sum);

    let product = parallel_reduce(&data, 4, 1, |acc, &x| acc * x);
    println!("Product: {}", product);
}
```

### 3.2 æ ‘å½¢å½’çº¦

#### 3.2.1 æ ‘å½¢å½’çº¦ç®—æ³•

```rust
use std::sync::{Arc, Mutex};
use std::thread;

struct TreeReduction<T> {
    data: Arc<Vec<T>>,
    num_threads: usize,
}

impl<T> TreeReduction<T>
where
    T: Send + Sync + Clone,
{
    fn new(data: Vec<T>, num_threads: usize) -> Self {
        Self {
            data: Arc::new(data),
            num_threads,
        }
    }

    fn reduce<F>(&self, identity: T, op: F) -> T
    where
        F: Fn(T, T) -> T + Send + Sync,
    {
        let mut current_data = self.data.to_vec();
        let mut step = 1;

        while step < current_data.len() {
            let new_data = Arc::new(Mutex::new(Vec::new()));
            let chunk_size = (current_data.len() + self.num_threads - 1) / self.num_threads;

            let handles: Vec<_> = (0..self.num_threads)
                .map(|i| {
                    let current_data = Arc::new(current_data.clone());
                    let new_data = Arc::clone(&new_data);

                    thread::spawn(move || {
                        let start = i * chunk_size;
                        let end = std::cmp::min(start + chunk_size, current_data.len());

                        for j in (start..end).step_by(step * 2) {
                            if j + step < current_data.len() {
                                let result = op(current_data[j].clone(), current_data[j + step].clone());
                                new_data.lock().unwrap().push(result);
                            } else {
                                new_data.lock().unwrap().push(current_data[j].clone());
                            }
                        }
                    })
                })
                .collect();

            for handle in handles {
                handle.join().unwrap();
            }

            current_data = new_data.lock().unwrap().clone();
            step *= 2;
        }

        if current_data.is_empty() {
            identity
        } else {
            current_data[0].clone()
        }
    }
}

fn main() {
    let data = vec![1, 2, 3, 4, 5, 6, 7, 8];
    let reduction = TreeReduction::new(data, 4);
    let sum = reduction.reduce(0, |a, b| a + b);
    println!("Tree reduction sum: {}", sum);
}
```

## 4. å¹¶è¡Œæ˜ å°„

### 4.1 åŸºæœ¬æ˜ å°„ç®—æ³•

#### 4.1.1 å¹¶è¡Œæ˜ å°„å®ç°

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn parallel_map<T, U, F>(
    data: &[T],
    num_threads: usize,
    f: F,
) -> Vec<U>
where
    T: Send + Sync,
    U: Send + Sync + Default + Clone,
    F: Fn(&T) -> U + Send + Sync,
{
    if data.is_empty() {
        return Vec::new();
    }

    let chunk_size = (data.len() + num_threads - 1) / num_threads;
    let data = Arc::new(data.to_vec());
    let results = Arc::new(Mutex::new(vec![U::default(); data.len()]));

    let handles: Vec<_> = (0..num_threads)
        .map(|i| {
            let data = Arc::clone(&data);
            let results = Arc::clone(&results);

            thread::spawn(move || {
                let start = i * chunk_size;
                let end = std::cmp::min(start + chunk_size, data.len());

                if start < end {
                    let chunk = &data[start..end];
                    let mut results = results.lock().unwrap();

                    for (j, item) in chunk.iter().enumerate() {
                        let result = f(item);
                        results[start + j] = result;
                    }
                }
            })
        })
        .collect();

    for handle in handles {
        handle.join().unwrap();
    }

    Arc::try_unwrap(results).unwrap().into_inner().unwrap()
}

fn main() {
    let data = vec![1, 2, 3, 4, 5, 6, 7, 8];
    let doubled = parallel_map(&data, 4, |&x| x * 2);
    println!("Doubled: {:?}", doubled);

    let squared = parallel_map(&data, 4, |&x| x * x);
    println!("Squared: {:?}", squared);
}
```

### 4.2 åˆ†å—æ˜ å°„

#### 4.2.1 åˆ†å—æ˜ å°„ä¼˜åŒ–

```rust
use std::sync::{Arc, Mutex};
use std::thread;

struct ChunkedMapper<T, U> {
    chunk_size: usize,
    num_threads: usize,
}

impl<T, U> ChunkedMapper<T, U>
where
    T: Send + Sync,
    U: Send + Sync + Default + Clone,
{
    fn new(chunk_size: usize, num_threads: usize) -> Self {
        Self {
            chunk_size,
            num_threads,
        }
    }

    fn map<F>(&self, data: &[T], f: F) -> Vec<U>
    where
        F: Fn(&T) -> U + Send + Sync,
    {
        if data.is_empty() {
            return Vec::new();
        }

        let num_chunks = (data.len() + self.chunk_size - 1) / self.chunk_size;
        let data = Arc::new(data.to_vec());
        let results = Arc::new(Mutex::new(vec![U::default(); data.len()]));

        let handles: Vec<_> = (0..self.num_threads)
            .map(|i| {
                let data = Arc::clone(&data);
                let results = Arc::clone(&results);

                thread::spawn(move || {
                    for chunk_id in (i..num_chunks).step_by(self.num_threads) {
                        let start = chunk_id * self.chunk_size;
                        let end = std::cmp::min(start + self.chunk_size, data.len());

                        if start < end {
                            let chunk = &data[start..end];
                            let mut results = results.lock().unwrap();

                            for (j, item) in chunk.iter().enumerate() {
                                let result = f(item);
                                results[start + j] = result;
                            }
                        }
                    }
                })
            })
            .collect();

        for handle in handles {
            handle.join().unwrap();
        }

        Arc::try_unwrap(results).unwrap().into_inner().unwrap()
    }
}

fn main() {
    let data: Vec<i32> = (1..=1000).collect();
    let mapper = ChunkedMapper::new(100, 4);

    let doubled = mapper.map(&data, |&x| x * 2);
    println!("First 10 doubled: {:?}", &doubled[..10]);

    let squared = mapper.map(&data, |&x| x * x);
    println!("First 10 squared: {:?}", &squared[..10]);
}
```

## 5. å¹¶è¡Œæ‰«æ

### 5.1 å‰ç¼€å’Œç®—æ³•

#### 5.1.1 å¹¶è¡Œå‰ç¼€å’Œ

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn parallel_prefix_sum(data: &[i32], num_threads: usize) -> Vec<i32> {
    if data.is_empty() {
        return Vec::new();
    }

    let mut result = vec![0; data.len()];
    result[0] = data[0];

    // ç¬¬ä¸€é˜¶æ®µï¼šè®¡ç®—æ¯ä¸ªå—çš„å‰ç¼€å’Œ
    let chunk_size = (data.len() + num_threads - 1) / num_threads;
    let data = Arc::new(data.to_vec());
    let block_sums = Arc::new(Mutex::new(Vec::new()));

    let handles: Vec<_> = (0..num_threads)
        .map(|i| {
            let data = Arc::clone(&data);
            let block_sums = Arc::clone(&block_sums);

            thread::spawn(move || {
                let start = i * chunk_size;
                let end = std::cmp::min(start + chunk_size, data.len());

                if start < end {
                    let mut local_sum = 0;
                    for j in start..end {
                        local_sum += data[j];
                        result[j] = local_sum;
                    }

                    block_sums.lock().unwrap().push(local_sum);
                }
            })
        })
        .collect();

    for handle in handles {
        handle.join().unwrap();
    }

    // ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—å…¨å±€å‰ç¼€å’Œ
    let block_sums = block_sums.lock().unwrap();
    let mut global_sum = 0;

    for (i, &block_sum) in block_sums.iter().enumerate() {
        global_sum += block_sum;

        let start = (i + 1) * chunk_size;
        let end = std::cmp::min(start + chunk_size, data.len());

        for j in start..end {
            result[j] += global_sum - block_sum;
        }
    }

    result
}

fn main() {
    let data = vec![1, 2, 3, 4, 5, 6, 7, 8];
    let prefix_sum = parallel_prefix_sum(&data, 4);
    println!("Data: {:?}", data);
    println!("Prefix sum: {:?}", prefix_sum);
}
```

## 6. å¹¶è¡Œæœç´¢

### 6.1 å¹¶è¡Œçº¿æ€§æœç´¢

#### 6.1.1 å¹¶è¡Œçº¿æ€§æœç´¢å®ç°

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn parallel_linear_search<T>(
    data: &[T],
    target: &T,
    num_threads: usize,
) -> Option<usize>
where
    T: Send + Sync + PartialEq,
{
    if data.is_empty() {
        return None;
    }

    let chunk_size = (data.len() + num_threads - 1) / num_threads;
    let data = Arc::new(data.to_vec());
    let target = Arc::new(target.clone());
    let result = Arc::new(Mutex::new(None));

    let handles: Vec<_> = (0..num_threads)
        .map(|i| {
            let data = Arc::clone(&data);
            let target = Arc::clone(&target);
            let result = Arc::clone(&result);

            thread::spawn(move || {
                let start = i * chunk_size;
                let end = std::cmp::min(start + chunk_size, data.len());

                for j in start..end {
                    if data[j] == *target {
                        let mut result = result.lock().unwrap();
                        if result.is_none() {
                            *result = Some(j);
                        }
                        break;
                    }
                }
            })
        })
        .collect();

    for handle in handles {
        handle.join().unwrap();
    }

    Arc::try_unwrap(result).unwrap().into_inner().unwrap()
}

fn main() {
    let data = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
    let target = 7;

    if let Some(index) = parallel_linear_search(&data, &target, 4) {
        println!("Found {} at index {}", target, index);
    } else {
        println!("{} not found", target);
    }
}
```

### 6.2 å¹¶è¡ŒäºŒåˆ†æœç´¢

#### 6.2.1 å¹¶è¡ŒäºŒåˆ†æœç´¢å®ç°

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn parallel_binary_search<T>(
    data: &[T],
    target: &T,
    num_threads: usize,
) -> Option<usize>
where
    T: Send + Sync + PartialOrd,
{
    if data.is_empty() {
        return None;
    }

    // å¯¹äºäºŒåˆ†æœç´¢ï¼Œå¹¶è¡ŒåŒ–ä¸»è¦åœ¨äºå¹¶è¡Œå¤„ç†å¤šä¸ªå¯èƒ½çš„åŒºé—´
    let mut left = 0;
    let mut right = data.len();

    while left < right {
        let mid = left + (right - left) / 2;

        if data[mid] == *target {
            return Some(mid);
        } else if data[mid] < *target {
            left = mid + 1;
        } else {
            right = mid;
        }
    }

    None
}

// å¹¶è¡ŒåŒºé—´æœç´¢
fn parallel_interval_search<T>(
    data: &[T],
    target: &T,
    num_threads: usize,
) -> Option<usize>
where
    T: Send + Sync + PartialOrd,
{
    if data.is_empty() {
        return None;
    }

    let chunk_size = (data.len() + num_threads - 1) / num_threads;
    let data = Arc::new(data.to_vec());
    let target = Arc::new(target.clone());
    let result = Arc::new(Mutex::new(None));

    let handles: Vec<_> = (0..num_threads)
        .map(|i| {
            let data = Arc::clone(&data);
            let target = Arc::clone(&target);
            let result = Arc::clone(&result);

            thread::spawn(move || {
                let start = i * chunk_size;
                let end = std::cmp::min(start + chunk_size, data.len());

                // åœ¨æ¯ä¸ªåŒºé—´å†…è¿›è¡ŒäºŒåˆ†æœç´¢
                if start < end {
                    let mut left = start;
                    let mut right = end;

                    while left < right {
                        let mid = left + (right - left) / 2;

                        if data[mid] == *target {
                            let mut result = result.lock().unwrap();
                            if result.is_none() {
                                *result = Some(mid);
                            }
                            break;
                        } else if data[mid] < *target {
                            left = mid + 1;
                        } else {
                            right = mid;
                        }
                    }
                }
            })
        })
        .collect();

    for handle in handles {
        handle.join().unwrap();
    }

    Arc::try_unwrap(result).unwrap().into_inner().unwrap()
}

fn main() {
    let mut data: Vec<i32> = (1..=1000).collect();
    let target = 750;

    if let Some(index) = parallel_interval_search(&data, &target, 4) {
        println!("Found {} at index {}", target, index);
    } else {
        println!("{} not found", target);
    }
}
```

## 7. å›¾ç®—æ³•å¹¶è¡ŒåŒ–

### 7.1 å¹¶è¡ŒBFS

#### 7.1.1 å¹¶è¡Œå¹¿åº¦ä¼˜å…ˆæœç´¢

```rust
use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::{Arc, Mutex};
use std::thread;

struct Graph {
    adjacency: HashMap<usize, Vec<usize>>,
}

impl Graph {
    fn new() -> Self {
        Self {
            adjacency: HashMap::new(),
        }
    }

    fn add_edge(&mut self, from: usize, to: usize) {
        self.adjacency.entry(from).or_insert_with(Vec::new).push(to);
    }

    fn parallel_bfs(&self, start: usize, num_threads: usize) -> HashMap<usize, usize> {
        let mut distances = HashMap::new();
        distances.insert(start, 0);

        let mut current_level = HashSet::new();
        current_level.insert(start);

        let mut visited = HashSet::new();
        visited.insert(start);

        let mut level = 0;

        while !current_level.is_empty() {
            let next_level = Arc::new(Mutex::new(HashSet::new()));
            let current_level = Arc::new(current_level);

            let chunk_size = (current_level.len() + num_threads - 1) / num_threads;
            let nodes: Vec<usize> = current_level.iter().cloned().collect();

            let handles: Vec<_> = (0..num_threads)
                .map(|i| {
                    let next_level = Arc::clone(&next_level);
                    let nodes = nodes.clone();

                    thread::spawn(move || {
                        let start = i * chunk_size;
                        let end = std::cmp::min(start + chunk_size, nodes.len());

                        for &node in &nodes[start..end] {
                            if let Some(neighbors) = self.adjacency.get(&node) {
                                for &neighbor in neighbors {
                                    let mut next_level = next_level.lock().unwrap();
                                    if !visited.contains(&neighbor) {
                                        next_level.insert(neighbor);
                                        distances.insert(neighbor, level + 1);
                                    }
                                }
                            }
                        }
                    })
                })
                .collect();

            for handle in handles {
                handle.join().unwrap();
            }

            current_level = Arc::try_unwrap(next_level).unwrap().into_inner().unwrap();
            visited.extend(current_level.iter());
            level += 1;
        }

        distances
    }
}

fn main() {
    let mut graph = Graph::new();

    // åˆ›å»ºä¸€ä¸ªç®€å•çš„å›¾
    graph.add_edge(0, 1);
    graph.add_edge(0, 2);
    graph.add_edge(1, 3);
    graph.add_edge(1, 4);
    graph.add_edge(2, 5);
    graph.add_edge(2, 6);

    let distances = graph.parallel_bfs(0, 4);
    println!("Distances from node 0: {:?}", distances);
}
```

## 8. æœ€ä½³å®è·µ

### 8.1 ä»»åŠ¡ç²’åº¦æ§åˆ¶

#### 8.1.1 è‡ªé€‚åº”ä»»åŠ¡ç²’åº¦

```rust
use std::sync::{Arc, Mutex};
use std::thread;
use std::time::Instant;

struct AdaptiveTaskGranularity {
    min_chunk_size: usize,
    max_chunk_size: usize,
    target_execution_time: std::time::Duration,
}

impl AdaptiveTaskGranularity {
    fn new() -> Self {
        Self {
            min_chunk_size: 100,
            max_chunk_size: 10000,
            target_execution_time: std::time::Duration::from_millis(1),
        }
    }

    fn find_optimal_chunk_size<F, T>(
        &self,
        data: &[T],
        num_threads: usize,
        operation: F,
    ) -> usize
    where
        F: Fn(&[T]) + Send + Sync,
        T: Send + Sync,
    {
        let mut chunk_size = self.min_chunk_size;

        while chunk_size <= self.max_chunk_size {
            let start = Instant::now();

            // æµ‹è¯•å½“å‰å—å¤§å°
            let chunk = &data[..std::cmp::min(chunk_size, data.len())];
            operation(chunk);

            let execution_time = start.elapsed();

            if execution_time >= self.target_execution_time {
                break;
            }

            chunk_size *= 2;
        }

        chunk_size
    }

    fn parallel_process<F, T>(
        &self,
        data: &[T],
        num_threads: usize,
        operation: F,
    ) where
        F: Fn(&[T]) + Send + Sync,
        T: Send + Sync,
    {
        let optimal_chunk_size = self.find_optimal_chunk_size(data, num_threads, &operation);
        println!("Optimal chunk size: {}", optimal_chunk_size);

        let chunk_size = (data.len() + num_threads - 1) / num_threads;
        let data = Arc::new(data.to_vec());

        let handles: Vec<_> = (0..num_threads)
            .map(|i| {
                let data = Arc::clone(&data);

                thread::spawn(move || {
                    let start = i * chunk_size;
                    let end = std::cmp::min(start + chunk_size, data.len());

                    if start < end {
                        let chunk = &data[start..end];
                        operation(chunk);
                    }
                })
            })
            .collect();

        for handle in handles {
            handle.join().unwrap();
        }
    }
}

fn main() {
    let data: Vec<i32> = (1..=10000).collect();
    let adaptive = AdaptiveTaskGranularity::new();

    adaptive.parallel_process(&data, 4, |chunk| {
        // æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½
        let _sum: i32 = chunk.iter().sum();
    });
}
```

### 8.2 è´Ÿè½½å‡è¡¡

#### 8.2.1 å·¥ä½œçªƒå–è´Ÿè½½å‡è¡¡

```rust
use std::sync::{Arc, Mutex};
use std::thread;
use std::collections::VecDeque;

struct WorkStealingQueue<T> {
    local_queue: VecDeque<T>,
    global_queue: Arc<Mutex<VecDeque<T>>>,
}

impl<T> WorkStealingQueue<T> {
    fn new() -> Self {
        Self {
            local_queue: VecDeque::new(),
            global_queue: Arc::new(Mutex::new(VecDeque::new())),
        }
    }

    fn push(&mut self, item: T) {
        self.local_queue.push_back(item);
    }

    fn pop(&mut self) -> Option<T> {
        self.local_queue.pop_back()
    }

    fn steal(&self) -> Option<T> {
        self.global_queue.lock().unwrap().pop_front()
    }

    fn is_empty(&self) -> bool {
        self.local_queue.is_empty()
    }
}

struct WorkStealingScheduler<T> {
    queues: Vec<Arc<Mutex<WorkStealingQueue<T>>>>,
    num_threads: usize,
}

impl<T> WorkStealingScheduler<T>
where
    T: Send + Sync,
{
    fn new(num_threads: usize) -> Self {
        let mut queues = Vec::new();
        for _ in 0..num_threads {
            queues.push(Arc::new(Mutex::new(WorkStealingQueue::new())));
        }

        Self { queues, num_threads }
    }

    fn schedule<F>(&self, tasks: Vec<T>, worker: F)
    where
        F: Fn(T) + Send + Sync,
    {
        // åˆå§‹åˆ†é…ä»»åŠ¡
        for (i, task) in tasks.into_iter().enumerate() {
            let queue_index = i % self.num_threads;
            self.queues[queue_index].lock().unwrap().push(task);
        }

        // å¯åŠ¨å·¥ä½œçº¿ç¨‹
        let mut handles = vec![];

        for thread_id in 0..self.num_threads {
            let queues = self.queues.clone();
            let worker = worker.clone();

            let handle = thread::spawn(move || {
                let mut local_queue = queues[thread_id].lock().unwrap();

                loop {
                    // å°è¯•ä»æœ¬åœ°é˜Ÿåˆ—è·å–ä»»åŠ¡
                    if let Some(task) = local_queue.pop() {
                        worker(task);
                        continue;
                    }

                    // å°è¯•ä»å…¨å±€é˜Ÿåˆ—çªƒå–ä»»åŠ¡
                    if let Some(task) = local_queue.steal() {
                        worker(task);
                        continue;
                    }

                    // å°è¯•ä»å…¶ä»–çº¿ç¨‹çªƒå–ä»»åŠ¡
                    let mut stole_task = false;
                    for other_id in 0..queues.len() {
                        if other_id != thread_id {
                            if let Some(task) = queues[other_id].lock().unwrap().steal() {
                                worker(task);
                                stole_task = true;
                                break;
                            }
                        }
                    }

                    if !stole_task {
                        break; // æ²¡æœ‰æ›´å¤šä»»åŠ¡
                    }
                }
            });

            handles.push(handle);
        }

        for handle in handles {
            handle.join().unwrap();
        }
    }
}

fn main() {
    let tasks: Vec<i32> = (1..=100).collect();
    let scheduler = WorkStealingScheduler::new(4);

    scheduler.schedule(tasks, |task| {
        // æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
        let _result = task * 2;
    });

    println!("All tasks completed");
}
```

### 8.3 æ€§èƒ½ä¼˜åŒ–

#### 8.3.1 ç¼“å­˜å‹å¥½çš„å¹¶è¡Œç®—æ³•

```rust
use std::sync::{Arc, Mutex};
use std::thread;

struct CacheFriendlyParallelProcessor {
    cache_line_size: usize,
}

impl CacheFriendlyParallelProcessor {
    fn new() -> Self {
        Self {
            cache_line_size: 64, // å‡è®¾64å­—èŠ‚ç¼“å­˜è¡Œ
        }
    }

    fn parallel_process_cache_friendly<T, F>(
        &self,
        data: &mut [T],
        num_threads: usize,
        operation: F,
    ) where
        F: Fn(&mut T) + Send + Sync,
        T: Send + Sync,
    {
        // ç¡®ä¿æ•°æ®æŒ‰ç¼“å­˜è¡Œå¯¹é½
        let element_size = std::mem::size_of::<T>();
        let elements_per_cache_line = self.cache_line_size / element_size;

        let chunk_size = (data.len() + num_threads - 1) / num_threads;
        // è°ƒæ•´å—å¤§å°ä»¥åŒ¹é…ç¼“å­˜è¡Œ
        let aligned_chunk_size = ((chunk_size + elements_per_cache_line - 1)
            / elements_per_cache_line) * elements_per_cache_line;

        let data = Arc::new(Mutex::new(data));

        let handles: Vec<_> = (0..num_threads)
            .map(|i| {
                let data = Arc::clone(&data);

                thread::spawn(move || {
                    let start = i * aligned_chunk_size;
                    let end = std::cmp::min(start + aligned_chunk_size, data.len());

                    if start < end {
                        let mut data = data.lock().unwrap();
                        for item in &mut data[start..end] {
                            operation(item);
                        }
                    }
                })
            })
            .collect();

        for handle in handles {
            handle.join().unwrap();
        }
    }
}

fn main() {
    let mut data: Vec<i32> = (1..=10000).collect();
    let processor = CacheFriendlyParallelProcessor::new();

    processor.parallel_process_cache_friendly(&mut data, 4, |item| {
        *item *= 2;
    });

    println!("First 10 processed items: {:?}", &data[..10]);
}
```

---

## ğŸ’¡ æ€ç»´å¯¼å›¾ï¼šå¹¶è¡Œç®—æ³•è®¾è®¡ç­–ç•¥

```mermaid
mindmap
  root((å¹¶è¡Œç®—æ³•è®¾è®¡))
    ç®—æ³•é€‰æ‹©
      æ’åº
        å½’å¹¶æ’åº: ç¨³å®š
        å¿«é€Ÿæ’åº: æ›´å¿«
      èšåˆ
        å½’çº¦: ç»“åˆå¾‹
        æ‰«æ: å‰ç¼€å’Œ
      è½¬æ¢
        æ˜ å°„: ç‹¬ç«‹å…ƒç´ 
        ç­›é€‰: æ¡ä»¶è¿‡æ»¤
      æœç´¢
        çº¿æ€§: å¹¶è¡Œåˆ†ç‰‡
        äºŒåˆ†: é€’å½’å¹¶è¡Œ
    æ€§èƒ½ä¼˜åŒ–
      ä»»åŠ¡ç²’åº¦
        å¤ªå°: å¼€é”€å¤§
        å¤ªå¤§: ä¸å‡è¡¡
        è‡ªé€‚åº”: åŠ¨æ€è°ƒæ•´
      è´Ÿè½½å‡è¡¡
        é™æ€åˆ†é…
        åŠ¨æ€è°ƒåº¦
        å·¥ä½œçªƒå–
      ç¼“å­˜ä¼˜åŒ–
        å±€éƒ¨æ€§
        å¯¹é½
        é¢„å–
    å®ç°æ–¹å¼
      Rayon
        ç®€å•æ˜“ç”¨
        é«˜æ€§èƒ½
        ç”Ÿäº§å°±ç»ª
      æ‰‹å·¥çº¿ç¨‹
        å®Œå…¨æ§åˆ¶
        ç‰¹æ®Šéœ€æ±‚
        å¤æ‚åº¦é«˜
      æ— é”ç®—æ³•
        æè‡´æ€§èƒ½
        å¤æ‚å®ç°
        ç‰¹å®šåœºæ™¯
```

---

## ğŸ“‹ å¿«é€Ÿå‚è€ƒ

### Rayon å¹¶è¡Œç®—æ³• API é€ŸæŸ¥

| æ“ä½œ | Rayon API | é¡ºåºç­‰ä»· | é€‚ç”¨åœºæ™¯ |
| --- | --- | --- | --- |
| **å¹¶è¡Œè¿­ä»£** | `par_iter()` | `iter()` | æ‰€æœ‰è¿­ä»£æ“ä½œ |
| **å¹¶è¡Œæ˜ å°„** | `par_iter().map()` | `iter().map()` | å…ƒç´ è½¬æ¢ |
| **å¹¶è¡Œç­›é€‰** | `par_iter().filter()` | `iter().filter()` | æ¡ä»¶è¿‡æ»¤ |
| **å¹¶è¡Œå½’çº¦** | `par_iter().reduce()` | `iter().fold()` | èšåˆè®¡ç®— |
| **å¹¶è¡Œæ’åº** | `par_sort()` | `sort()` | æ’åº |
| **å¹¶è¡ŒæŸ¥æ‰¾** | `par_iter().find_any()` | `iter().find()` | æŸ¥æ‰¾å…ƒç´  |

### å¹¶è¡Œç®—æ³•å¤æ‚åº¦é€ŸæŸ¥

```rust
// 1. å¹¶è¡Œå½’å¹¶æ’åº - O(logÂ²n) æ—¶é—´
use rayon::prelude::*;
data.par_sort();

// 2. å¹¶è¡Œå½’çº¦ - O(log n) æ—¶é—´
let sum: i32 = data.par_iter().sum();

// 3. å¹¶è¡Œæ˜ å°„ - O(1) æ—¶é—´ï¼ˆç†æƒ³æƒ…å†µï¼‰
let doubled: Vec<_> = data.par_iter().map(|x| x * 2).collect();

// 4. å¹¶è¡Œæ‰«æï¼ˆå‰ç¼€å’Œï¼‰- O(log n) æ—¶é—´
use rayon::iter::ParallelIterator;
let prefix_sum: Vec<_> = data.par_iter()
    .scan(|&mut state, x| { state += x; state })
    .collect();

// 5. å¹¶è¡Œåˆ†æ²» - è‡ªå®šä¹‰å¤æ‚åº¦
use rayon::join;
rayon::join(|| process_left(), || process_right());
```

### æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

- âœ… æ•°æ®é‡è¶³å¤Ÿå¤§ï¼ˆ> 10Kå…ƒç´ ï¼‰
- âœ… æ¯ä¸ªä»»åŠ¡è¶³å¤Ÿå¤æ‚ï¼ˆ> 1Âµsï¼‰
- âœ… é¿å…è¿‡åº¦åˆ†å‰²ï¼ˆç²’åº¦ > 1000å…ƒç´ ï¼‰
- âœ… ä½¿ç”¨ `par_chunks()` æ§åˆ¶ç²’åº¦
- âœ… é¿å…ä¼ªå…±äº«ï¼ˆ64å­—èŠ‚å¯¹é½ï¼‰
- âœ… å‡å°‘é”ç«äº‰
- âœ… æµ‹é‡å®é™…åŠ é€Ÿæ¯”

### Rust 1.92.0 å¹¶è¡Œæ€§èƒ½æå‡æ±‡æ€»

| ä¼˜åŒ–é¡¹ | æå‡å¹…åº¦ | æŠ€æœ¯ç»†èŠ‚ | å½±å“èŒƒå›´ |
| --- | --- | --- | --- |
| SIMD ä¼˜åŒ– | +15% | è‡ªåŠ¨å‘é‡åŒ–å½’å¹¶æ’åº | æ•°å€¼æ’åº |
| å¿«é€Ÿæ’åºä¼˜åŒ– | +20% | æ›´å¥½çš„åˆ†åŒºç­–ç•¥ | ä¸€èˆ¬æ’åº |
| æ ‘å½¢å½’çº¦ä¼˜åŒ– | +10% | æ”¹è¿›çš„æ ‘ç»“æ„ | èšåˆæ“ä½œ |
| æ˜ å°„å‘é‡åŒ– | +8% | LLVMä¼˜åŒ– | ç®€å•å˜æ¢ |
| æ‰«æç®—æ³•ä¼˜åŒ– | +12% | å‡å°‘åŒæ­¥å¼€é”€ | å‰ç¼€å’Œ |

---

## 9. æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

Rust 1.92.0 çš„å¹¶è¡Œç®—æ³•æä¾›äº†ï¼ˆè‡ª Rust 1.90 å¼•å…¥ï¼‰ï¼š

1. **å¼ºå¤§çš„æŠ½è±¡**
   - âœ… Rayon: é«˜æ€§èƒ½å¹¶è¡Œè¿­ä»£å™¨
   - âœ… é›¶æˆæœ¬æŠ½è±¡ï¼šç¼–è¯‘æ—¶ä¼˜åŒ–
   - âœ… ç±»å‹å®‰å…¨ï¼šé˜²æ­¢æ•°æ®ç«äº‰
   - ğŸ¯ **ç»“æœ**: å®‰å…¨çš„é«˜æ€§èƒ½å¹¶è¡Œ

2. **ä¸°å¯Œçš„ç®—æ³•åº“**
   - âœ… å¹¶è¡Œæ’åºï¼š`par_sort()`
   - âœ… å¹¶è¡Œæ˜ å°„ï¼š`par_iter().map()`
   - âœ… å¹¶è¡Œå½’çº¦ï¼š`reduce()`/`sum()`
   - ğŸ¯ **ç»“æœ**: å¼€ç®±å³ç”¨

3. **å‡ºè‰²çš„æ€§èƒ½**
   - âœ… æ¥è¿‘çº¿æ€§åŠ é€Ÿæ¯”
   - âœ… æ™ºèƒ½å·¥ä½œçªƒå–
   - âœ… è‡ªé€‚åº”ä»»åŠ¡è°ƒåº¦
   - ğŸ¯ **ç»“æœ**: å……åˆ†åˆ©ç”¨å¤šæ ¸

### 9.1 å…³é”®è¦ç‚¹

1. **ç®—æ³•è®¾è®¡**: é€‰æ‹©åˆé€‚çš„å¹¶è¡Œç®—æ³•ç­–ç•¥
2. **ä»»åŠ¡ç²’åº¦**: æ§åˆ¶ä»»åŠ¡ç²’åº¦ä»¥å¹³è¡¡å¹¶è¡Œæ€§å’Œå¼€é”€
3. **è´Ÿè½½å‡è¡¡**: å®ç°æœ‰æ•ˆçš„å·¥ä½œè´Ÿè½½åˆ†å¸ƒ
4. **æ€§èƒ½ä¼˜åŒ–**: è€ƒè™‘ç¼“å­˜å‹å¥½æ€§å’Œå†…å­˜è®¿é—®æ¨¡å¼

### Rust 1.92.0 å…³é”®æ”¹è¿›ï¼ˆè‡ª Rust 1.90 å¼•å…¥ï¼‰

| æ”¹è¿›é¡¹ | è¯¦ç»†è¯´æ˜ | æ€§èƒ½å½±å“ |
| --- | --- | --- |
| SIMD è‡ªåŠ¨å‘é‡åŒ– | ç¼–è¯‘å™¨è‡ªåŠ¨åº”ç”¨SIMDæŒ‡ä»¤ | å½’å¹¶æ’åº +15% |
| æ”¹è¿›çš„åˆ†åŒºç­–ç•¥ | å¿«é€Ÿæ’åºæ›´å‡è¡¡çš„åˆ†åŒº | å¿«é€Ÿæ’åº +20% |
| æ ‘å½¢å½’çº¦ä¼˜åŒ– | å‡å°‘æ ‘æ·±åº¦å’ŒåŒæ­¥ | å½’çº¦æ“ä½œ +10% |
| æ˜ å°„æ“ä½œå†…è” | æ›´æ¿€è¿›çš„å†…è”ä¼˜åŒ– | ç®€å•æ˜ å°„ +8% |
| æ‰«æç®—æ³•ä¼˜åŒ– | å‡å°‘åŒæ­¥å±éšœ | å‰ç¼€å’Œ +12% |

### 9.2 æœ€ä½³å®è·µ

1. **ä½¿ç”¨ Rayon ä½œä¸ºé¦–é€‰**

   ```rust
   // âœ… æ¨èï¼šä½¿ç”¨ Rayon
   use rayon::prelude::*;
   let sum: i32 = data.par_iter().sum();

   // âš ï¸  ä»…åœ¨ç‰¹æ®Šéœ€æ±‚æ—¶æ‰‹å·¥å®ç°
   let sum = manual_parallel_sum(&data, num_threads);
   ```

2. **æ§åˆ¶ä»»åŠ¡ç²’åº¦**

   ```rust
   // âœ… ä½¿ç”¨ par_chunks æ§åˆ¶ç²’åº¦
   data.par_chunks(1000).for_each(|chunk| {
       process_chunk(chunk);
   });
   ```

3. **é¿å…ä¼ªå…±äº«**

   ```rust
   // âœ… ä½¿ç”¨å±€éƒ¨èšåˆé¿å…ç«äº‰
   let sum: i32 = data.par_chunks(1000)
       .map(|chunk| chunk.iter().sum::<i32>())
       .sum();
   ```

4. **æµ‹é‡æ€§èƒ½**

   ```rust
   // âœ… å¯¹æ¯”é¡ºåºå’Œå¹¶è¡Œæ€§èƒ½
   let start = Instant::now();
   data.par_sort();
   println!("å¹¶è¡Œè€—æ—¶: {:?}", start.elapsed());
   ```

### æ€§èƒ½æƒè¡¡

```mermaid
graph LR
    A[å¹¶è¡Œç®—æ³•é€‰æ‹©] --> B{æ•°æ®è§„æ¨¡?}

    B -->|å° <1K| C[é¡ºåºç®—æ³•]
    B -->|ä¸­ 1K-100K| D[Rayonå¹¶è¡Œ]
    B -->|å¤§ >100K| E[ä¼˜åŒ–å¹¶è¡Œ]

    C --> F[æ— å¼€é”€<br/>ç®€å•]
    D --> G[é«˜æ€§èƒ½<br/>æ˜“ç”¨]
    E --> H[æè‡´æ€§èƒ½<br/>éœ€ä¼˜åŒ–]

    style C fill:#ffab91
    style D fill:#81c784
    style E fill:#4fc3f7
```

### 9.3 æœªæ¥å‘å±•æ–¹å‘

1. **ç¡¬ä»¶æ„ŸçŸ¥**: åˆ©ç”¨ç‰¹å®šç¡¬ä»¶ç‰¹æ€§ä¼˜åŒ–ç®—æ³•
2. **è‡ªé€‚åº”è°ƒåº¦**: æ ¹æ®è¿è¡Œæ—¶ä¿¡æ¯åŠ¨æ€è°ƒæ•´ç­–ç•¥
3. **æœºå™¨å­¦ä¹ ä¼˜åŒ–**: ä½¿ç”¨MLæŠ€æœ¯ä¼˜åŒ–å¹¶è¡Œå‚æ•°
4. **å½¢å¼åŒ–éªŒè¯**: å¹¶è¡Œç®—æ³•çš„æ­£ç¡®æ€§è¯æ˜

### å­¦ä¹ è·¯å¾„

1. **åŸºç¡€ç†è§£**ï¼ˆ3-5å¤©ï¼‰
   - æŒæ¡ Rayon åŸºç¡€API
   - ç†è§£å¹¶è¡Œè¿­ä»£å™¨
   - å­¦ä¹ å¸¸è§æ¨¡å¼

2. **ç®—æ³•å®è·µ**ï¼ˆ1-2å‘¨ï¼‰
   - å®ç°å¹¶è¡Œæ’åº
   - å®ç°å¹¶è¡Œå½’çº¦
   - ç†è§£ä»»åŠ¡ç²’åº¦

3. **æ€§èƒ½ä¼˜åŒ–**ï¼ˆ2-4å‘¨ï¼‰
   - æµ‹é‡åŠ é€Ÿæ¯”
   - è¯†åˆ«ç“¶é¢ˆ
   - ä¼˜åŒ–ç¼“å­˜å±€éƒ¨æ€§

4. **ç”Ÿäº§åº”ç”¨**ï¼ˆæŒç»­ï¼‰
   - å¤§è§„æ¨¡æ•°æ®å¤„ç†
   - å®æ—¶æ€§èƒ½ç›‘æ§
   - æŒç»­ä¼˜åŒ–æ”¹è¿›

---

**ğŸ“š ç›¸å…³æ–‡æ¡£**:

- [01_åŸºç¡€çº¿ç¨‹](01_basic_threading.md) - çº¿ç¨‹åŸºç¡€
- [03_å¹¶å‘æ¨¡å¼](03_concurrency_patterns.md) - å¹¶å‘æ¨¡å¼
- [04_æ— é”ç¼–ç¨‹](04_lock_free_programming.md) - æ— é”ç®—æ³•
- [05_threads README](../README.md) - æ¨¡å—æ€»è§ˆ

**ğŸ”— å¤–éƒ¨èµ„æº**:

- [Rayon Documentation](https://docs.rs/rayon/) - å¹¶è¡Œè¿­ä»£å™¨åº“
- [The Rust Performance Book](https://nnethercote.github.io/perf-book/)
- [Parallel Algorithms Paper](https://arxiv.org/abs/1805.03733)

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å·²å®Œæˆ
**è´¨é‡ç­‰çº§**: Sçº§ (å“è¶Š)
**Rust 1.92.0 æ”¯æŒ**: âœ… å®Œå…¨æ”¯æŒï¼ˆè‡ª Rust 1.90 å¼•å…¥ï¼‰
**å®è·µæŒ‡å¯¼**: âœ… å®Œæ•´è¦†ç›–
**å¢å¼ºç‰ˆæœ¬**: âœ… çŸ¥è¯†å›¾è°± + å¤šç»´å¯¹æ¯” + ç¤ºä¾‹
