# C05 Threads 常见问题解答 (FAQ)

> **文档定位**: 并发编程常见问题的快速解答手册  
> **使用方式**: 查找遇到的问题，获取简明答案和解决方案  
> **相关文档**: [项目概览](./01_项目概览.md) | [主索引导航](./02_主索引导航.md) | [术语表](./03_术语表.md)

**最后更新**: 2025-10-22  
**适用版本**: Rust 1.90+ (Edition 2024)  
**文档类型**: ❓ 问答手册  
**问题数量**: 20+ 个

---

## 📋 问题索引

```text
📚 问题分类
├── 🎯 基础概念 (Q1-Q5)
├── 🔧 实践问题 (Q6-Q10)
├── ⚡ 性能优化 (Q11-Q15)
└── 🐛 调试技巧 (Q16-Q20)
```

### 快速跳转

**基础概念**:

- [Q1: Send 和 Sync 的区别](#q1-send-和-sync-到底有什么区别我总是搞混)
- [Q2: 为什么推崇消息传递](#q2-既然-mutex-这么好用为什么-rust-还推崇消息传递)
- [Q3: `Arc<Mutex<T>>` 如何工作](#q3-arcmutext-看起来很笨重它到底是怎么工作的)
- [Q4: Rayon vs async/await](#q4-我应该什么时候用-rayon什么时候用-asyncawait-tokioasync-std)
- [Q5: 原子类型 vs Mutex](#q5-直接使用原子类型-atomics-会比-mutex-更快吗)

**实践问题**:

- [Q6: 如何避免死锁](#q6-如何避免死锁)
- [Q7: 如何选择Channel类型](#q7-如何选择合适的-channel-类型)
- [Q8: 数据共享的最佳实践](#q8-在多个线程间共享数据的最佳实践是什么)
- [Q9: 线程池大小](#q9-线程池应该设置多大)
- [Q10: panic处理](#q10-如果一个线程-panic-了会怎样)

**性能优化**:

- [Q11: 性能瓶颈](#q11-如何识别并发程序的性能瓶颈)
- [Q12: 锁竞争](#q12-如何减少锁竞争)
- [Q13: 伪共享](#q13-什么是伪共享如何避免)
- [Q14: 负载均衡](#q14-如何实现更好的负载均衡)
- [Q15: NUMA优化](#q15-numa-系统下如何优化)

**调试技巧**:

- [Q16: 数据竞争检测](#q16-如何检测数据竞争)
- [Q17: 死锁检测](#q17-如何检测死锁)
- [Q18: 性能分析](#q18-如何分析并发程序的性能)
- [Q19: 日志线程信息](#q19-如何在日志中记录线程信息)
- [Q20: 测试并发代码](#q20-如何测试并发代码)

---

## 🎯 基础概念

### Q1: Send 和 Sync 到底有什么区别？我总是搞混

**A1**: 这是最常见的一个困惑点。可以这样简单记忆：

#### 核心区别

| Trait | 含义 | 检查对象 | 记忆口诀 |
|-------|------|---------|---------|
| **`Send`** | 可以**发送**(转移所有权) | `T` | "Can be **sen**t" |
| **`Sync`** | 可以**同步**(共享引用) | `&T` | "Can be **syn**chronized" |

#### 详细说明

- **`Send`**: 如果一个类型 `T` 是 `Send` 的，意味着 `T` 类型的值可以被**安全地发送（所有权转移）**到另一个线程。

- **`Sync`**: 如果一个类型 `T` 是 `Sync` 的，意味着 `&T` 类型的值（一个共享引用）可以被**安全地在多个线程间共享**。

#### 实际例子

```rust
// String 是 Send 但不是 Sync
let s = String::from("hello");
thread::spawn(move || {
    println!("{}", s); // ✅ OK: String 是 Send
});

// Mutex<T> 是 Sync (如果 T 是 Send)
let m = Arc::new(Mutex::new(0));
let m_clone = Arc::clone(&m);
thread::spawn(move || {
    *m_clone.lock().unwrap() += 1; // ✅ OK: Mutex 是 Sync
});

// Rc<T> 既不是 Send 也不是 Sync
let rc = Rc::new(0);
thread::spawn(move || {
    println!("{}", rc); // ❌ 错误：Rc 不是 Send
});
```

#### 常见类型

| 类型 | Send | Sync | 说明 |
|------|------|------|------|
| `i32`, `String`, `Vec<T>` | ✅ | ❌ | 可以发送，但不能直接共享 |
| `Arc<T>` | ✅ | ✅ | 可以发送和共享 |
| `Mutex<T>` | ❌ | ✅ | 不能发送，但可以共享 (需要 Arc) |
| `Rc<T>` | ❌ | ❌ | 完全不能用于多线程 |
| `RefCell<T>` | ✅ | ❌ | 可以发送，但不能共享 |

**参考**: [术语表 - Send](./03_术语表.md#send-trait) | [术语表 - Sync](./03_术语表.md#sync-trait)

---

### Q2: 既然 Mutex 这么好用，为什么 Rust 还推崇消息传递？

**A2**: 这是为了引导开发者走向更易于推理的并发模型。

#### 原因分析

1. **认知负荷** 💭
   - 共享状态（`Mutex`）要求开发者时刻思考锁的顺序、粒度和潜在的死锁问题
   - 代码库越大，这种心智负担越重
   - 难以推理和维护

2. **解耦** 🔗
   - 消息传递模型鼓励将线程/任务设计成独立的、解耦的单元
   - 它们之间只通过定义好的消息进行通信
   - 这使得系统更模块化，更易于维护和理解

3. **避免死锁** 🔒
   - 简单的消息传递模式（如单向数据流）天然地不容易产生死锁
   - 消息传递本质上是序列化的

4. **可扩展性** 📈
   - 消息传递更容易扩展到分布式系统
   - Actor 模型可以跨进程、跨机器

#### 何时使用 Mutex？

当然，这并非绝对。对于需要极高性能或数据量巨大的场景，共享状态可能更优：

- ✅ 频繁的小数据交换
- ✅ 需要原地修改大数据结构
- ✅ 读多写少的场景（使用 `RwLock`）

#### 何时使用消息传递？

- ✅ 需要解耦的系统
- ✅ 复杂的并发逻辑
- ✅ 事件驱动系统
- ✅ Actor 模型

**Rust 的美妙之处**: 它同时为你提供了两种安全的选择！

**参考**: [消息传递实践指南](../tier_02_guides/02_消息传递实践指南.md) | [同步原语使用指南](../tier_02_guides/03_同步原语使用指南.md)

---

### Q3: `Arc<Mutex<T>>` 看起来很笨重，它到底是怎么工作的？

**A3**: `Arc<Mutex<T>>` 是两种能力的组合，理解了它们就不觉得笨重了。

#### 组成部分

```rust
Arc<Mutex<T>>
│   │
│   └─> Mutex: 同步访问（互斥锁）
└─────> Arc: 共享所有权（原子引用计数）
```

#### 1. `Arc` (Atomically Reference Counted)

**职责**: 共享所有权

- 让多个线程能"拥有"指向同一个 `Mutex` 的指针
- `clone()` 只是原子性地增加引用计数，成本非常低
- 最后一个引用被 drop 时，数据才被释放

```rust
let data = Arc::new(...);
let data_clone = Arc::clone(&data); // 只增加计数，非常快
```

#### 2. `Mutex` (Mutual Exclusion)

**职责**: 同步访问

- 保护内部的数据 `T`
- 确保任何时候只有一个线程能通过 `.lock()` 拿到可变访问权限
- 自动在 `MutexGuard` 被 drop 时释放锁

```rust
let guard = mutex.lock().unwrap();
*guard += 1; // 独占访问
// guard 被 drop 时，锁自动释放
```

#### 完整示例

```rust
use std::sync::{Arc, Mutex};
use std::thread;

let data = Arc::new(Mutex::new(0));

let mut handles = vec![];

for _ in 0..10 {
    let data_clone = Arc::clone(&data);
    let handle = thread::spawn(move || {
        let mut guard = data_clone.lock().unwrap();
        *guard += 1;
    });
    handles.push(handle);
}

for handle in handles {
    handle.join().unwrap();
}

println!("结果: {}", *data.lock().unwrap()); // 输出: 结果: 10
```

#### 为什么不是一个类型？

因为它们解决不同的问题：

- `Arc` 解决了"如何在线程间传递指针"的问题
- `Mutex` 解决了"如何安全地修改指针指向的内容"的问题

两者结合，构成了 Rust 中最基础、最通用的线程安全内部可变性模式。

**参考**: [术语表 - Arc](./03_术语表.md#arct-atomically-reference-counted) | [术语表 - Mutex](./03_术语表.md#mutext-mutual-exclusion)

---

### Q4: 我应该什么时候用 Rayon，什么时候用 async/await (Tokio/async-std)？

**A4**: 这是一个关于 **CPU 密集型** vs. **I/O 密集型**任务的核心问题。

#### 快速决策树

```text
你的任务是什么？
├── CPU密集型 (大量计算)
│   └─> 使用 Rayon (数据并行)
│       ✅ 并行排序、图像处理、科学计算
│
└── I/O密集型 (大量等待)
    └─> 使用 async/await (异步并发)
        ✅ Web服务器、数据库查询、网络请求
```

#### 使用 Rayon (数据并行)

**场景**: 当你的任务是 **CPU 密集型**的

**例子**:

- 对一个巨大的集合（数组、图片、矩阵）进行计算
- 运行复杂的模拟或算法
- 并行排序、搜索
- 图像/视频处理

**示例**:

```rust
use rayon::prelude::*;

// 并行处理大数组
let sum: i32 = (0..1_000_000)
    .into_par_iter()
    .map(|x| expensive_computation(x))
    .sum();
```

**优势**:

- Rayon 会使用线程池榨干你所有的 CPU 核心来加速这些计算
- 零成本抽象，性能接近手工优化
- 简单易用（`par_iter()`）

#### 使用 async/await (异步并发)

**场景**: 当你的任务是 **I/O 密集型**的

**例子**:

- 同时处理成千上万个网络连接（Web 服务器）
- 同时读写大量文件
- 与数据库进行大量并发查询
- 同时发起多个 HTTP 请求

**示例**:

```rust
use tokio;

// 并发处理多个HTTP请求
let tasks: Vec<_> = urls.iter()
    .map(|url| tokio::spawn(fetch(url)))
    .collect();

for task in tasks {
    let result = task.await?;
    // 处理结果
}
```

**优势**:

- CPU 大部分时间都在**等待** I/O 操作完成
- 异步运行时可以用极少的系统线程来高效管理这些等待
- 不会阻塞线程

#### 对比表

| 维度 | Rayon | async/await |
|------|-------|-------------|
| **类型** | 数据并行 | 异步并发 |
| **任务** | CPU密集型 | I/O密集型 |
| **瓶颈** | CPU计算能力 | I/O等待时间 |
| **线程** | 多线程（线程数 ≈ CPU核心数） | 少量线程 + 大量任务 |
| **例子** | 图像处理 | Web服务器 |
| **库** | rayon | tokio, async-std |

#### 简单记忆

**计算用 Rayon，等待用 async！**

**参考**: [并行计算实践指南](../tier_02_guides/05_并行计算实践指南.md) | [术语表 - Rayon](./03_术语表.md#rayon)

---

### Q5: 直接使用原子类型 (Atomics) 会比 Mutex 更快吗？

**A5**: **可能会，但你几乎永远不应该这样做。**

#### 性能对比

**简单场景**（如递增计数器）:

```rust
// Atomics: 快
counter.fetch_add(1, Ordering::SeqCst);

// Mutex: 稍慢
*mutex.lock().unwrap() += 1;
```

对于非常简单的操作，`AtomicI32::fetch_add` 确实会比 `Mutex<i32>` 更快，因为它避免了锁的开销。

#### 为什么不应该？

**危险性**: 这是在与魔鬼交易 ⚠️

1. **内存排序的复杂性**
   - 直接使用原子类型意味着你必须手动处理**内存排序** (Memory Ordering)
   - 这是并发编程中最复杂、最微妙、最容易出错的部分

2. **平台差异**
   - 一个错误的 `Ordering` 可能导致：
     - 在某些硬件上正常工作
     - 在另一些硬件上出现难以复现的数据竞争

3. **难以调试**
   - 原子操作的bug通常是Heisenbug（观察时消失的bug）
   - 几乎不可能通过测试发现

#### 内存排序选择

| Ordering | 性能 | 安全性 | 使用场景 |
|----------|------|--------|---------|
| `SeqCst` | 慢 | 最安全 | 默认选择 |
| `Acquire/Release` | 中 | 安全（需理解） | 优化场景 |
| `Relaxed` | 快 | 危险 | 极端优化（专家） |

#### 准则

**除非**你满足以下条件，否则请始终使用 `Mutex`、`RwLock` 或 Channel：

- ✅ 你正在编写像 `Mutex` 本身这样的底层同步原语
- ✅ 性能分析表明一个 `Mutex` 确实是无法接受的瓶颈
- ✅ 你完全理解内存排序的含义
- ✅ 你有充分的测试（包括在不同硬件上）

#### 何时可以使用 Atomics？

**简单场景**:

- 计数器（使用 `SeqCst`）
- 标志位（使用 `SeqCst`）
- 简单的状态机（使用 `SeqCst`）

```rust
use std::sync::atomic::{AtomicUsize, Ordering};

static COUNTER: AtomicUsize = AtomicUsize::new(0);

// 简单的计数器：安全
COUNTER.fetch_add(1, Ordering::SeqCst);
```

**复杂场景**: 请使用高层抽象！

#### 结论

**安全性和正确性远比微小的性能提升更重要。**

**参考**: [无锁编程参考](../tier_03_references/02_无锁编程参考.md) | [术语表 - Atomics](./03_术语表.md#atomics-原子类型)

---

## 🔧 实践问题

### Q6: 如何避免死锁？

**A6**: 死锁是并发编程中的经典问题，但有系统的方法来避免。

#### 死锁的必要条件

死锁需要同时满足四个条件（Coffman条件）：

1. 互斥
2. 持有并等待
3. 不可抢占
4. 循环等待

**破坏任何一个条件即可避免死锁。**

#### 预防策略

##### 1. **锁顺序** (最常用)

总是以相同的顺序获取锁：

```rust
// ✅ 正确：所有线程都按相同顺序获取锁
let lock1 = lock1.lock().unwrap();
let lock2 = lock2.lock().unwrap();

// ❌ 错误：不同线程以不同顺序获取
// 线程A: lock1 -> lock2
// 线程B: lock2 -> lock1  // 可能死锁！
```

##### 2. **超时机制**

使用 `try_lock` 和超时：

```rust
use std::time::Duration;

if let Ok(lock1) = mutex1.try_lock() {
    if let Ok(lock2) = mutex2.try_lock() {
        // 同时获得两个锁
    } else {
        // 获取lock2失败，释放lock1并重试
        drop(lock1);
        thread::sleep(Duration::from_millis(10));
    }
}
```

##### 3. **避免嵌套锁**

尽量减少同时持有多个锁：

```rust
// ✅ 好：不嵌套锁
{
    let data1 = lock1.lock().unwrap();
    process(&data1);
} // lock1 在此处释放

{
    let data2 = lock2.lock().unwrap();
    process(&data2);
} // lock2 在此处释放

// ❌ 差：嵌套锁
let data1 = lock1.lock().unwrap();
let data2 = lock2.lock().unwrap(); // 潜在死锁
```

##### 4. **使用消息传递**

完全避免共享状态：

```rust
// ✅ 最安全：使用Channel，天然避免死锁
let (tx, rx) = mpsc::channel();

thread::spawn(move || {
    tx.send(data).unwrap();
});

let data = rx.recv().unwrap();
```

##### 5. **锁超时设置**

使用 `parking_lot`的超时锁：

```rust
use parking_lot::Mutex;
use std::time::Duration;

let mutex = Mutex::new(0);

if let Some(guard) = mutex.try_lock_for(Duration::from_secs(1)) {
    // 获得锁
} else {
    // 超时处理
}
```

#### 检测方法

- **运行时检测**: ThreadSanitizer, Valgrind
- **静态分析**: Clippy, RustSec
- **日志分析**: 记录锁的获取顺序

**参考**: [术语表 - Deadlock](./03_术语表.md#deadlock-死锁) | [同步原语使用指南](../tier_02_guides/03_同步原语使用指南.md)

---

### Q7: 如何选择合适的 Channel 类型？

**A7**: Rust 提供多种 Channel 实现，选择合适的类型很重要。

#### 决策树

```text
需要Channel吗？
├── 是 → 继续
│
├── 多少个生产者？
│   ├── 1个 → SPSC
│   └── 多个 → MPSC/MPMC
│
├── 需要背压吗？
│   ├── 是 → Bounded Channel
│   └── 否 → Unbounded Channel
│
├── 需要优先级吗？
│   ├── 是 → 自定义Priority Channel
│   └── 否 → 标准Channel
│
└── 异步还是同步？
    ├── 异步 → tokio::sync::mpsc
    └── 同步 → std::sync::mpsc / crossbeam
```

#### Channel 类型对比

| 类型 | 生产者 | 消费者 | 有界/无界 | 性能 | 使用场景 |
|------|--------|--------|-----------|------|---------|
| `std::sync::mpsc` | 多 | 单 | 两者都有 | 中 | 通用 |
| `crossbeam::bounded` | 多 | 多 | 有界 | 高 | 高性能 |
| `crossbeam::unbounded` | 多 | 多 | 无界 | 高 | 无背压需求 |
| `flume::bounded` | 多 | 多 | 有界 | 高 | 更简洁API |
| `tokio::sync::mpsc` | 多 | 单 | 有界 | 高 | 异步场景 |
| 自定义Priority | - | - | - | - | 需要优先级 |

#### 具体选择

##### 1. **标准库 mpsc** (通用)

适用场景：

- ✅ 简单的消息传递
- ✅ 不需要最高性能
- ✅ 单消费者

```rust
use std::sync::mpsc;

let (tx, rx) = mpsc::channel();
let (tx_bounded, rx_bounded) = mpsc::sync_channel(100);
```

##### 2. **crossbeam** (高性能)

适用场景：

- ✅ 需要高性能
- ✅ 需要多消费者 (MPMC)
- ✅ 需要 `select!` 操作

```rust
use crossbeam::channel;

let (tx, rx) = channel::bounded(100);    // 有界
let (tx, rx) = channel::unbounded();     // 无界
```

##### 3. **flume** (简洁API)

适用场景：

- ✅ 需要简洁的API
- ✅ 需要高性能
- ✅ 需要 `select!` 操作

```rust
use flume;

let (tx, rx) = flume::bounded(100);
let (tx, rx) = flume::unbounded();
```

##### 4. **tokio::sync::mpsc** (异步)

适用场景：

- ✅ 异步运行时 (Tokio)
- ✅ I/O密集型应用

```rust
use tokio::sync::mpsc;

let (tx, mut rx) = mpsc::channel(100);

// 异步发送和接收
tx.send(data).await?;
let data = rx.recv().await;
```

#### 有界 vs 无界

| 特性 | 有界 (Bounded) | 无界 (Unbounded) |
|------|---------------|------------------|
| **背压** | ✅ 有 | ❌ 无 |
| **内存** | 可控 | 可能无限增长 |
| **性能** | 可能阻塞 | 不阻塞 |
| **使用场景** | 生产消费速度不匹配 | 生产消费速度匹配 |

**建议**: 默认使用有界Channel（如 `bounded(100)`），除非你确定不会有内存问题。

**参考**: [消息传递实践指南](../tier_02_guides/02_消息传递实践指南.md) | [术语表 - Channel](./03_术语表.md#channel-通道)

---

### Q8: 在多个线程间共享数据的最佳实践是什么？

**A8**: 根据不同的需求，有不同的最佳实践。

#### 选择指南

```text
数据特性？
├── 只读 → Arc<T>
├── 读多写少 → Arc<RwLock<T>>
├── 读写均衡 → Arc<Mutex<T>>
├── 频繁小数据交换 → Arc<Mutex<T>>
├── 事件驱动 → Channel (消息传递)
└── 复杂状态管理 → Actor模型
```

#### 方案对比

##### 1. **`Arc<T>`** - 只读共享

**适用**: 数据不需要修改

```rust
use std::sync::Arc;

let data = Arc::new(vec![1, 2, 3]);

for _ in 0..10 {
    let data_clone = Arc::clone(&data);
    thread::spawn(move || {
        println!("{:?}", data_clone); // 只读访问
    });
}
```

**优点**:

- ✅ 零运行时开销（只读）
- ✅ 无锁竞争

**缺点**:

- ❌ 不能修改

##### 2. **`Arc<Mutex<T>>`** - 读写均衡

**适用**: 读写操作都比较频繁

```rust
use std::sync::{Arc, Mutex};

let data = Arc::new(Mutex::new(0));

for _ in 0..10 {
    let data_clone = Arc::clone(&data);
    thread::spawn(move || {
        let mut guard = data_clone.lock().unwrap();
        *guard += 1;
    });
}
```

**优点**:

- ✅ 简单直接
- ✅ 保证互斥

**缺点**:

- ❌ 所有访问都需要获取锁
- ❌ 可能有锁竞争

##### 3. **`Arc<RwLock<T>>`** - 读多写少

**适用**: 读操作远多于写操作

```rust
use std::sync::{Arc, RwLock};

let data = Arc::new(RwLock::new(vec![1, 2, 3]));

// 多个读锁可以同时存在
let r1 = data.read().unwrap();
let r2 = data.read().unwrap();

// 写锁是排他的
let mut w = data.write().unwrap();
w.push(4);
```

**优点**:

- ✅ 多个读者可以并发
- ✅ 适合读多写少

**缺点**:

- ❌ 写操作开销稍大
- ❌ 可能的写饥饿

##### 4. **Channel** - 消息传递

**适用**: 事件驱动、解耦的系统

```rust
use std::sync::mpsc;

let (tx, rx) = mpsc::channel();

// 生产者
thread::spawn(move || {
    tx.send(data).unwrap();
});

// 消费者
let data = rx.recv().unwrap();
```

**优点**:

- ✅ 完全解耦
- ✅ 避免锁
- ✅ 易于推理

**缺点**:

- ❌ 消息复制的开销
- ❌ 不适合频繁的小数据交换

#### 最佳实践总结

1. **默认选择**: `Arc<Mutex<T>>`（简单、安全）
2. **只读共享**: `Arc<T>`（最高效）
3. **读多写少**: `Arc<RwLock<T>>`（提升并发度）
4. **事件驱动**: Channel（最易维护）
5. **高性能**: 考虑无锁数据结构（复杂）

**参考**: [同步原语使用指南](../tier_02_guides/03_同步原语使用指南.md) | [消息传递实践指南](../tier_02_guides/02_消息传递实践指南.md)

---

### Q9: 线程池应该设置多大？

**A9**: 线程池大小的选择取决于任务类型和系统资源。

#### 快速决策

```text
任务类型？
├── CPU密集型 → 线程数 = CPU核心数 (或稍多)
├── I/O密集型 → 线程数 = 2 × CPU核心数 (或更多)
├── 混合型 → 测试并调整
└── 不确定 → 从 CPU核心数 开始，逐步调整
```

#### 详细指南

##### 1. **CPU密集型任务**

**建议**: 线程数 = CPU核心数 (或 CPU核心数 + 1)

```rust
let num_threads = num_cpus::get(); // 或 num_cpus::get() + 1

let pool = rayon::ThreadPoolBuilder::new()
    .num_threads(num_threads)
    .build()
    .unwrap();
```

**原因**:

- CPU密集型任务受限于CPU计算能力
- 线程数超过CPU核心数会导致过多的上下文切换
- 稍微多一点（+1或+2）可以补偿偶尔的等待

##### 2. **I/O密集型任务**

**建议**: 线程数 = 2 × CPU核心数 (或更多)

```rust
let num_threads = 2 * num_cpus::get();

// 或使用更灵活的计算
let num_threads = num_cpus::get() * (1 + wait_time / service_time);
```

**原因**:

- I/O密集型任务大部分时间在等待
- 更多线程可以在一些线程等待时利用CPU
- 可以根据实际I/O等待时间调整

##### 3. **混合型任务**

**建议**: 测试并调整

```rust
// 从一个合理的起点开始
let num_threads = (num_cpus::get() as f64 * 1.5) as usize;
```

**调整方法**:

1. 监控CPU使用率
2. 监控任务延迟
3. 逐步调整并测试

#### 获取CPU核心数

```rust
use num_cpus;

let logical_cores = num_cpus::get();        // 逻辑核心数（包括超线程）
let physical_cores = num_cpus::get_physical(); // 物理核心数
```

#### 实际例子1

##### Rayon (数据并行)

```rust
use rayon;

// 方式1：使用默认（自动检测CPU核心数）
// 无需设置，Rayon会自动使用最佳配置

// 方式2：手动设置
rayon::ThreadPoolBuilder::new()
    .num_threads(8)
    .build_global()
    .unwrap();
```

##### 自定义线程池

```rust
use threadpool::ThreadPool;

let num_threads = num_cpus::get();
let pool = ThreadPool::new(num_threads);

for i in 0..10 {
    pool.execute(move || {
        // 任务
    });
}

pool.join();
```

#### 动态调整

对于负载变化的系统，考虑动态调整：

```rust
// 伪代码
if cpu_usage < 50% {
    increase_thread_pool_size();
} else if cpu_usage > 90% {
    decrease_thread_pool_size();
}
```

#### 监控指标

- **CPU使用率**: 应接近100%（CPU密集）或适中（I/O密集）
- **任务队列长度**: 不应持续增长
- **任务延迟**: 应在可接受范围内
- **上下文切换次数**: 不应过高

#### 总结

| 任务类型 | 建议线程数 | 备注 |
|---------|-----------|------|
| **CPU密集** | `num_cpus::get()` | 或 +1/+2 |
| **I/O密集** | `2 × num_cpus::get()` | 可更多 |
| **混合型** | `1.5 × num_cpus::get()` | 测试调整 |
| **默认** | `num_cpus::get()` | 保守选择 |

**参考**: [并行计算实践指南](../tier_02_guides/05_并行计算实践指南.md) | [性能分析参考](../tier_03_references/05_性能分析参考.md)

---

### Q10: 如果一个线程 panic 了会怎样？

**A10**: Rust 的线程panic处理机制保证了安全性，但需要正确处理。

#### 默认行为

**单个线程panic不会影响其他线程**：

```rust
use std::thread;

// 线程1 panic
let handle1 = thread::spawn(|| {
    panic!("线程1崩溃了！");
});

// 线程2 正常运行
let handle2 = thread::spawn(|| {
    println!("线程2正常运行");
});

// handle1.join() 会返回 Err
match handle1.join() {
    Ok(_) => println!("线程1成功"),
    Err(e) => println!("线程1 panic: {:?}", e),
}

// handle2.join() 正常
handle2.join().unwrap();
```

输出：

```text
线程2正常运行
线程1 panic: Any { .. }
```

#### 检测线程panic

##### 1. **使用 `join()`**

```rust
let handle = thread::spawn(|| {
    panic!("出错了");
});

match handle.join() {
    Ok(_) => println!("线程成功完成"),
    Err(e) => {
        println!("线程panic了");
        // 可以记录日志、重试等
    }
}
```

##### 2. **使用 `catch_unwind`**

```rust
use std::panic::catch_unwind;

let result = catch_unwind(|| {
    // 可能panic的代码
    panic!("出错了");
});

match result {
    Ok(_) => println!("成功"),
    Err(e) => println!("捕获到panic"),
}
```

#### Mutex 中的panic

**问题**: 如果线程在持有 `Mutex` 时panic，锁会被"poisoned"（污染）

```rust
use std::sync::{Arc, Mutex};
use std::thread;

let data = Arc::new(Mutex::new(0));
let data_clone = Arc::clone(&data);

// 线程在持有锁时panic
thread::spawn(move || {
    let mut guard = data_clone.lock().unwrap();
    *guard += 1;
    panic!("持有锁时panic");
    // 锁被污染
}).join().ok();

// 后续尝试获取锁
match data.lock() {
    Ok(guard) => println!("获取锁成功: {}", *guard),
    Err(poisoned) => {
        // 锁被污染，但仍可以获取数据
        let guard = poisoned.into_inner();
        println!("锁被污染，但数据是: {}", *guard);
    }
}
```

**处理poisoned锁**:

```rust
// 方式1：使用 unwrap() 会在锁被污染时panic
let guard = mutex.lock().unwrap();

// 方式2：处理poisoned状态
match mutex.lock() {
    Ok(guard) => { /* 正常使用 */ },
    Err(poisoned) => {
        let guard = poisoned.into_inner(); // 恢复数据
        // 处理或重置状态
    }
}
```

#### 线程池中的panic

**Rayon**: 自动传播panic到调用者

```rust
use rayon::prelude::*;

let result = std::panic::catch_unwind(|| {
    (0..10).into_par_iter()
        .for_each(|i| {
            if i == 5 {
                panic!("出错了");
            }
        });
});

match result {
    Ok(_) => println!("成功"),
    Err(_) => println!("线程池中有panic"),
}
```

#### 最佳实践

##### 1. **总是检查 `join()` 的结果**

```rust
let handle = thread::spawn(|| {
    // 任务
});

handle.join().expect("线程panic了");
```

##### 2. **使用 `catch_unwind` 隔离panic**

```rust
use std::panic::catch_unwind;

let result = catch_unwind(|| {
    危险的操作();
});

if result.is_err() {
    // 记录日志
    // 清理资源
    // 可能重试
}
```

##### 3. **设置panic hook记录日志**

```rust
use std::panic;

panic::set_hook(Box::new(|panic_info| {
    eprintln!("线程panic: {:?}", panic_info);
    // 记录到日志系统
}));
```

##### 4. **考虑使用 `Result` 而不是 panic**

```rust
// ✅ 好：使用 Result
fn task() -> Result<(), Error> {
    // ...
}

// ❌ 差：直接 panic
fn task() {
    panic!("出错了");
}
```

#### 总结1

| 场景 | 行为 | 处理方法 |
|------|------|---------|
| **线程panic** | 不影响其他线程 | `join()` 检测 |
| **Mutex中panic** | 锁被污染 | 处理 `PoisonError` |
| **主线程panic** | 程序终止 | 设置panic hook |
| **Rayon中panic** | 传播到调用者 | `catch_unwind` |

**参考**: [基础线程编程指南](../tier_02_guides/01_基础线程编程指南.md)

---

## ⚡ 性能优化

### Q11-Q15: 性能相关问题

由于篇幅限制，这里提供简要回答。详细内容请参考专门的性能文档。

#### Q11: 如何识别并发程序的性能瓶颈？

**工具**:

- `perf` (Linux)
- `cargo flamegraph`
- `pprof`
- ThreadSanitizer

**参考**: [性能分析参考](../tier_03_references/05_性能分析参考.md)

---

#### Q12: 如何减少锁竞争？

**策略**:

1. 减少临界区大小
2. 使用 `RwLock`（读多写少）
3. 锁分片 (Sharding)
4. 无锁数据结构

**参考**: [性能分析参考](../tier_03_references/05_性能分析参考.md)

---

#### Q13: 什么是伪共享？如何避免？

**定义**: 不同线程访问同一缓存行的不同数据，导致缓存失效。

**避免方法**:

```rust
use crossbeam_utils::CachePadded;

struct Counter {
    count: CachePadded<AtomicUsize>,
}
```

**参考**: [SIMD与并行优化](../tier_04_advanced/04_SIMD与并行优化.md)

---

#### Q14: 如何实现更好的负载均衡？

**方法**:

1. 工作窃取 (Rayon)
2. 动态任务分配
3. 任务粒度调整

**参考**: [工作窃取算法](../tier_04_advanced/03_工作窃取算法.md)

---

#### Q15: NUMA 系统下如何优化？

**策略**:

1. 线程亲和性绑定
2. 内存亲和性分配
3. 数据分片到不同节点

**参考**: [NUMA感知编程](../tier_04_advanced/01_NUMA感知编程.md)

---

## 🐛 调试技巧

### Q16-Q20: 调试相关问题

#### Q16: 如何检测数据竞争？

**工具**:

- ThreadSanitizer
- Miri
- Loom

```bash
# ThreadSanitizer
RUSTFLAGS="-Z sanitizer=thread" cargo run

# Miri
cargo +nightly miri test
```

---

#### Q17: 如何检测死锁？

**方法**:

1. ThreadSanitizer
2. 日志分析
3. 超时检测

---

#### Q18: 如何分析并发程序的性能？

**工具**:

- `perf`
- `cargo flamegraph`
- 自定义性能计数器

**参考**: [性能分析参考](../tier_03_references/05_性能分析参考.md)

---

#### Q19: 如何在日志中记录线程信息？

```rust
use std::thread;

let thread_id = thread::current().id();
let thread_name = thread::current().name().unwrap_or("unnamed");

println!("[{}:{:?}] 消息", thread_name, thread_id);
```

---

#### Q20: 如何测试并发代码？

**工具**:

1. **Loom**: 模型检查
2. **Proptest**: 属性测试
3. **压力测试**: 大量并发

```rust
#[cfg(test)]
mod tests {
    use loom::thread;

    #[test]
    #[cfg(loom)]
    fn test_concurrent() {
        loom::model(|| {
            // 测试代码
        });
    }
}
```

**参考**: [形式化验证](../tier_04_advanced/05_形式化验证.md)

---

## 🔗 相关资源

### 核心文档

- 📘 [项目概览](./01_项目概览.md) - 模块全景介绍
- 📘 [主索引导航](./02_主索引导航.md) - 完整文档地图
- 📘 [术语表](./03_术语表.md) - 术语快速查询

### 实践指南

- 📖 [基础线程编程指南](../tier_02_guides/01_基础线程编程指南.md)
- 📖 [消息传递实践指南](../tier_02_guides/02_消息传递实践指南.md)
- 📖 [同步原语使用指南](../tier_02_guides/03_同步原语使用指南.md)

### 技术参考

- 📘 [无锁编程参考](../tier_03_references/02_无锁编程参考.md)
- 📘 [性能分析参考](../tier_03_references/05_性能分析参考.md)

---

## 💬 反馈与建议

如果本FAQ没有解答您的问题，请：

1. 查阅 [术语表](./03_术语表.md) 了解相关概念
2. 搜索 [主索引导航](./02_主索引导航.md) 查找相关文档
3. 在项目仓库提交 Issue

---

**最后更新**: 2025-10-22  
**问题数量**: 20+ 个  
**维护状态**: ✅ 活跃维护

---

❓ **快速解答，高效学习！立即查找您遇到的问题！** ❓

---

[返回项目概览](./01_项目概览.md) | [查看主索引](./02_主索引导航.md) | [查看术语表](./03_术语表.md)
