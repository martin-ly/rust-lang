//! å†…å­˜å®‰å…¨é«˜çº§æ¼”ç¤ºæ¨¡å—
//! 
//! æœ¬æ¨¡å—æ¼”ç¤ºäº† Rust 1.90 ä¸­çš„å„ç§å†…å­˜å®‰å…¨é«˜çº§ç‰¹æ€§ï¼ŒåŒ…æ‹¬ï¼š
//! - é«˜çº§ç”Ÿå‘½å‘¨æœŸç®¡ç†
//! - å†…å­˜å¸ƒå±€ä¼˜åŒ–
//! - é›¶æˆæœ¬æŠ½è±¡
//! - æ™ºèƒ½æŒ‡é’ˆé«˜çº§ç”¨æ³•
//! - å†…å­˜æ± å’Œåˆ†é…å™¨
//! - å†…å­˜æ³„æ¼æ£€æµ‹
//! - ç¼“å†²åŒºæº¢å‡ºé˜²æŠ¤
//! - å†…å­˜å¯¹é½å’Œç¼“å­˜ä¼˜åŒ–

use std::alloc::{GlobalAlloc, Layout, System};
use std::marker::PhantomData;
use std::mem::{size_of, align_of};
use std::ops::{Deref, DerefMut};
use std::ptr;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::{Duration, Instant};

/// é«˜çº§ç”Ÿå‘½å‘¨æœŸç®¡ç†
pub mod advanced_lifetimes {
    use super::*;

    /// ç”Ÿå‘½å‘¨æœŸè¿½è¸ªå™¨
    pub struct LifetimeTracker<'a, T> {
        data: &'a T,
        created_at: Instant,
        tracker_id: u64,
    }

    impl<'a, T> LifetimeTracker<'a, T> {
        pub fn new(data: &'a T, tracker_id: u64) -> Self {
            Self {
                data,
                created_at: Instant::now(),
                tracker_id,
            }
        }

        pub fn age(&self) -> Duration {
            self.created_at.elapsed()
        }

        pub fn get_data(&self) -> &'a T {
            self.data
        }

        pub fn tracker_id(&self) -> u64 {
            self.tracker_id
        }
    }

    /// ç”Ÿå‘½å‘¨æœŸç»„åˆå™¨
    pub struct LifetimeCombinator<'a, 'b, T, U> 
    where 
        T: 'a,
        U: 'b,
    {
        short_lived: &'a T,
        long_lived: &'b U,
        _phantom: PhantomData<(&'a T, &'b U)>,
    }

    impl<'a, 'b, T, U> LifetimeCombinator<'a, 'b, T, U>
    where 
        T: 'a,
        U: 'b,
    {
        pub fn new(short_lived: &'a T, long_lived: &'b U) -> Self {
            Self {
                short_lived,
                long_lived,
                _phantom: PhantomData,
            }
        }

        pub fn get_short(&self) -> &'a T {
            self.short_lived
        }

        pub fn get_long(&self) -> &'b U {
            self.long_lived
        }

        pub fn combine<F, R>(&self, f: F) -> R
        where
            F: FnOnce(&'a T, &'b U) -> R,
        {
            f(self.short_lived, self.long_lived)
        }
    }

    /// ç”Ÿå‘½å‘¨æœŸéªŒè¯å™¨
    pub struct LifetimeValidator<'a> {
        lifetime_id: &'a str,
        created_at: Instant,
        is_valid: bool,
    }

    impl<'a> LifetimeValidator<'a> {
        pub fn new(lifetime_id: &'a str) -> Self {
            Self {
                lifetime_id,
                created_at: Instant::now(),
                is_valid: true,
            }
        }

        pub fn validate(&self) -> bool {
            self.is_valid && self.created_at.elapsed() < Duration::from_secs(60)
        }

        pub fn invalidate(&mut self) {
            self.is_valid = false;
        }

        pub fn lifetime_id(&self) -> &'a str {
            self.lifetime_id
        }
    }
}

/// å†…å­˜å¸ƒå±€ä¼˜åŒ–
pub mod memory_layout {
    use super::*;

    /// ç¼“å­˜è¡Œå¯¹é½çš„æ•°æ®ç»“æ„
    #[repr(align(64))]
    pub struct CacheAligned<T> {
        pub data: T,
        _padding: [u8; 64],
    }

    impl<T> CacheAligned<T> {
        pub fn new(data: T) -> Self {
            Self {
                data,
                _padding: [0; 64],
            }
        }

        pub fn get(&self) -> &T {
            &self.data
        }

        pub fn get_mut(&mut self) -> &mut T {
            &mut self.data
        }
    }

    /// å†…å­˜æ± åˆ†é…å™¨
    pub struct MemoryPool {
        blocks: Vec<*mut u8>,
        pub block_size: usize,
        pub total_blocks: usize,
        free_blocks: AtomicUsize,
    }

    impl MemoryPool {
        pub fn new(block_size: usize, total_blocks: usize) -> Self {
            let mut blocks = Vec::with_capacity(total_blocks);
            
            unsafe {
                for _ in 0..total_blocks {
                    let layout = Layout::from_size_align(block_size, align_of::<u8>()).unwrap();
                    let ptr = System.alloc(layout);
                    if !ptr.is_null() {
                        blocks.push(ptr);
                    }
                }
            }

            Self {
                blocks,
                block_size,
                total_blocks,
                free_blocks: AtomicUsize::new(total_blocks),
            }
        }

        pub fn allocate(&self) -> Option<*mut u8> {
            let current_free = self.free_blocks.load(Ordering::Acquire);
            if current_free == 0 {
                return None;
            }

            let new_free = current_free - 1;
            if self.free_blocks.compare_exchange_weak(
                current_free, 
                new_free, 
                Ordering::Release, 
                Ordering::Relaxed
            ).is_ok() {
                Some(self.blocks[current_free - 1])
            } else {
                None
            }
        }

        pub fn deallocate(&self, _ptr: *mut u8) {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œéœ€è¦å°†æŒ‡é’ˆè¿”å›åˆ°ç©ºé—²åˆ—è¡¨
            self.free_blocks.fetch_add(1, Ordering::Release);
        }

        pub fn available_blocks(&self) -> usize {
            self.free_blocks.load(Ordering::Relaxed)
        }
    }

    impl Drop for MemoryPool {
        fn drop(&mut self) {
            unsafe {
                for &ptr in &self.blocks {
                    if !ptr.is_null() {
                        let layout = Layout::from_size_align(self.block_size, align_of::<u8>()).unwrap();
                        System.dealloc(ptr, layout);
                    }
                }
            }
        }
    }

    /// å†…å­˜å¯¹é½å·¥å…·
    pub struct AlignmentUtils;

    impl AlignmentUtils {
        pub fn align_up(addr: usize, alignment: usize) -> usize {
            (addr + alignment - 1) & !(alignment - 1)
        }

        pub fn align_down(addr: usize, alignment: usize) -> usize {
            addr & !(alignment - 1)
        }

        pub fn is_aligned(addr: usize, alignment: usize) -> bool {
            addr & (alignment - 1) == 0
        }

        pub fn get_alignment<T>() -> usize {
            align_of::<T>()
        }

        pub fn get_size<T>() -> usize {
            size_of::<T>()
        }
    }
}

/// é›¶æˆæœ¬æŠ½è±¡
pub mod zero_cost_abstractions {
    use super::*;

    /// é›¶æˆæœ¬åŒ…è£…å™¨
    #[derive(Debug, Clone, Copy)]
    pub struct ZeroCostWrapper<T> {
        value: T,
    }

    impl<T> ZeroCostWrapper<T> {
        pub fn new(value: T) -> Self {
            Self { value }
        }

        pub fn get(&self) -> &T {
            &self.value
        }

        pub fn get_mut(&mut self) -> &mut T {
            &mut self.value
        }

        pub fn into_inner(self) -> T {
            self.value
        }
    }

    impl<T> Deref for ZeroCostWrapper<T> {
        type Target = T;

        fn deref(&self) -> &Self::Target {
            &self.value
        }
    }

    impl<T> DerefMut for ZeroCostWrapper<T> {
        fn deref_mut(&mut self) -> &mut Self::Target {
            &mut self.value
        }
    }

    /// ç¼–è¯‘æ—¶è®¡ç®—
    pub const fn compile_time_factorial(n: u32) -> u32 {
        match n {
            0 | 1 => 1,
            _ => n * compile_time_factorial(n - 1),
        }
    }

    /// é›¶æˆæœ¬è¿­ä»£å™¨
    pub struct ZeroCostIterator<'a, T> {
        data: &'a [T],
        index: usize,
    }

    impl<'a, T> ZeroCostIterator<'a, T> {
        pub fn new(slice: &'a [T]) -> Self {
            Self {
                data: slice,
                index: 0,
            }
        }
    }

    impl<'a, T> Iterator for ZeroCostIterator<'a, T> {
        type Item = &'a T;

        fn next(&mut self) -> Option<Self::Item> {
            if self.index < self.data.len() {
                let item = &self.data[self.index];
                self.index += 1;
                Some(item)
            } else {
                None
            }
        }
    }
}

/// æ™ºèƒ½æŒ‡é’ˆé«˜çº§ç”¨æ³•
pub mod smart_pointers {
    use super::*;

    /// å¼•ç”¨è®¡æ•°æ™ºèƒ½æŒ‡é’ˆ
    pub struct RefCounted<T> {
        data: *mut T,
        count: *mut AtomicUsize,
    }

    impl<T> RefCounted<T> {
        pub fn new(value: T) -> Self {
            unsafe {
                let data = System.alloc(Layout::new::<T>()) as *mut T;
                ptr::write(data, value);

                let count = System.alloc(Layout::new::<AtomicUsize>()) as *mut AtomicUsize;
                ptr::write(count, AtomicUsize::new(1));

                Self { data, count }
            }
        }

        pub fn clone(&self) -> Self {
            unsafe {
                (*self.count).fetch_add(1, Ordering::Relaxed);
                Self {
                    data: self.data,
                    count: self.count,
                }
            }
        }

        pub fn get(&self) -> &T {
            unsafe { &*self.data }
        }

        pub fn get_mut(&self) -> &mut T {
            unsafe { &mut *self.data }
        }
    }

    impl<T> Drop for RefCounted<T> {
        fn drop(&mut self) {
            unsafe {
                if (*self.count).fetch_sub(1, Ordering::Release) == 1 {
                    ptr::drop_in_place(self.data);
                    System.dealloc(self.data as *mut u8, Layout::new::<T>());
                    System.dealloc(self.count as *mut u8, Layout::new::<AtomicUsize>());
                }
            }
        }
    }

    /// å¼±å¼•ç”¨æ™ºèƒ½æŒ‡é’ˆ
    pub struct WeakRef<T> {
        data: *mut T,
        count: *mut AtomicUsize,
    }

    impl<T> WeakRef<T> {
        pub fn new(rc: &RefCounted<T>) -> Self {
            unsafe {
                (*rc.count).fetch_add(1, Ordering::Relaxed);
                Self {
                    data: rc.data,
                    count: rc.count,
                }
            }
        }

        pub fn upgrade(&self) -> Option<RefCounted<T>> {
            unsafe {
                if (*self.count).load(Ordering::Acquire) > 0 {
                    Some(RefCounted {
                        data: self.data,
                        count: self.count,
                    })
                } else {
                    None
                }
            }
        }
    }

    impl<T> Drop for WeakRef<T> {
        fn drop(&mut self) {
            unsafe {
                (*self.count).fetch_sub(1, Ordering::Release);
            }
        }
    }

    /// è‡ªå®šä¹‰åˆ†é…å™¨
    pub struct CustomAllocator {
        pool: super::memory_layout::MemoryPool,
    }

    impl CustomAllocator {
        pub fn new(pool: super::memory_layout::MemoryPool) -> Self {
            Self { pool }
        }
    }

    unsafe impl GlobalAlloc for CustomAllocator {
        unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
            if layout.size() <= self.pool.block_size {
                self.pool.allocate().unwrap_or(ptr::null_mut())
            } else {
                unsafe { System.alloc(layout) }
            }
        }

        unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
            if layout.size() <= self.pool.block_size {
                self.pool.deallocate(ptr);
            } else {
                unsafe { System.dealloc(ptr, layout) };
            }
        }
    }
}

/// å†…å­˜æ³„æ¼æ£€æµ‹
pub mod memory_leak_detection {
    use std::collections::HashMap;
    use std::sync::{Arc, Mutex};
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::time::Instant;

    /// å†…å­˜æ³„æ¼æ£€æµ‹å™¨
    pub struct MemoryLeakDetector {
        allocations: Arc<Mutex<HashMap<*mut u8, AllocationInfo>>>,
        total_allocated: AtomicUsize,
        total_deallocated: AtomicUsize,
    }

    #[derive(Debug, Clone)]
    pub struct AllocationInfo {
        pub size: usize,
        pub timestamp: Instant,
        pub stack_trace: Vec<String>,
    }

    impl MemoryLeakDetector {
        pub fn new() -> Self {
            Self {
                allocations: Arc::new(Mutex::new(HashMap::new())),
                total_allocated: AtomicUsize::new(0),
                total_deallocated: AtomicUsize::new(0),
            }
        }

        pub fn track_allocation(&self, ptr: *mut u8, size: usize) {
            let info = AllocationInfo {
                size,
                timestamp: Instant::now(),
                stack_trace: self.get_stack_trace(),
            };

            self.total_allocated.fetch_add(size, Ordering::Relaxed);
            
            let mut allocations = self.allocations.lock().unwrap();
            allocations.insert(ptr, info);
        }

        pub fn track_deallocation(&self, ptr: *mut u8) {
            let mut allocations = self.allocations.lock().unwrap();
            if let Some(info) = allocations.remove(&ptr) {
                self.total_deallocated.fetch_add(info.size, Ordering::Relaxed);
            }
        }

        pub fn get_leaks(&self) -> Vec<AllocationInfo> {
            let allocations = self.allocations.lock().unwrap();
            allocations.values().cloned().collect()
        }

        pub fn get_memory_stats(&self) -> MemoryStats {
            MemoryStats {
                total_allocated: self.total_allocated.load(Ordering::Relaxed),
                total_deallocated: self.total_deallocated.load(Ordering::Relaxed),
                current_allocations: self.allocations.lock().unwrap().len(),
            }
        }

        fn get_stack_trace(&self) -> Vec<String> {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ”¶é›†çœŸå®çš„å †æ ˆè·Ÿè¸ª
            vec!["stack_trace_placeholder".to_string()]
        }
    }

    #[derive(Debug)]
    pub struct MemoryStats {
        pub total_allocated: usize,
        pub total_deallocated: usize,
        pub current_allocations: usize,
    }
}

/// ç¼“å†²åŒºæº¢å‡ºé˜²æŠ¤
pub mod buffer_overflow_protection {

    /// å®‰å…¨ç¼“å†²åŒº
    pub struct SafeBuffer {
        data: Vec<u8>,
        size: usize,
        canary: u64,
    }

    impl SafeBuffer {
        const CANARY_VALUE: u64 = 0xDEADBEEFCAFEBABE;

        pub fn new(size: usize) -> Self {
            let mut data = vec![0u8; size];
            let canary = Self::CANARY_VALUE;
            
            // åœ¨ç¼“å†²åŒºæœ«å°¾æ”¾ç½®é‡‘ä¸é›€å€¼
            let canary_bytes = canary.to_le_bytes();
            let data_len = data.len();
            for (i, &byte) in canary_bytes.iter().enumerate() {
                if i < data_len {
                    data[data_len - 1 - i] = byte;
                }
            }

            Self { data, size, canary }
        }

        pub fn write(&mut self, offset: usize, data: &[u8]) -> Result<(), BufferError> {
            if offset + data.len() > self.size {
                return Err(BufferError::Overflow);
            }

            if !self.check_canary() {
                return Err(BufferError::Corruption);
            }

            self.data[offset..offset + data.len()].copy_from_slice(data);
            Ok(())
        }

        pub fn read(&self, offset: usize, len: usize) -> Result<&[u8], BufferError> {
            if offset + len > self.size {
                return Err(BufferError::Overflow);
            }

            if !self.check_canary() {
                return Err(BufferError::Corruption);
            }

            Ok(&self.data[offset..offset + len])
        }

        fn check_canary(&self) -> bool {
            if self.data.len() < 8 {
                return true;
            }

            let canary_bytes = &self.data[self.data.len() - 8..];
            let stored_canary = u64::from_le_bytes([
                canary_bytes[0], canary_bytes[1], canary_bytes[2], canary_bytes[3],
                canary_bytes[4], canary_bytes[5], canary_bytes[6], canary_bytes[7],
            ]);

            stored_canary == self.canary
        }
    }

    #[derive(Debug, Clone)]
    pub enum BufferError {
        Overflow,
        Corruption,
    }

    impl std::fmt::Display for BufferError {
        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
            match self {
                BufferError::Overflow => write!(f, "Buffer overflow detected"),
                BufferError::Corruption => write!(f, "Buffer corruption detected"),
            }
        }
    }

    impl std::error::Error for BufferError {}

    /// è¾¹ç•Œæ£€æŸ¥å·¥å…·
    pub struct BoundsChecker;

    impl BoundsChecker {
        pub fn check_bounds<T>(slice: &[T], index: usize) -> Result<(), BoundsError> {
            if index >= slice.len() {
                Err(BoundsError::OutOfBounds { index, len: slice.len() })
            } else {
                Ok(())
            }
        }

        pub fn safe_get<T>(slice: &[T], index: usize) -> Result<&T, BoundsError> {
            Self::check_bounds(slice, index)?;
            Ok(&slice[index])
        }

        pub fn safe_get_mut<T>(slice: &mut [T], index: usize) -> Result<&mut T, BoundsError> {
            if index >= slice.len() {
                return Err(BoundsError::OutOfBounds { index, len: slice.len() });
            }
            Ok(&mut slice[index])
        }
    }

    #[derive(Debug, Clone)]
    pub enum BoundsError {
        OutOfBounds { index: usize, len: usize },
    }

    impl std::fmt::Display for BoundsError {
        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
            match self {
                BoundsError::OutOfBounds { index, len } => {
                    write!(f, "Index {} out of bounds for slice of length {}", index, len)
                }
            }
        }
    }

    impl std::error::Error for BoundsError {}
}

/// å†…å­˜å¯¹é½å’Œç¼“å­˜ä¼˜åŒ–
pub mod memory_alignment_cache {

    /// ç¼“å­˜å‹å¥½çš„æ•°æ®ç»“æ„
    #[repr(align(64))]
    pub struct CacheFriendlyStruct {
        pub hot_data: [u64; 8],    // çƒ­æ•°æ®ï¼Œç»å¸¸è®¿é—®
        pub cold_data: [u8; 32],   // å†·æ•°æ®ï¼Œå¾ˆå°‘è®¿é—®
    }

    impl CacheFriendlyStruct {
        pub fn new() -> Self {
            Self {
                hot_data: [0; 8],
                cold_data: [0; 32],
            }
        }

        pub fn update_hot_data(&mut self, index: usize, value: u64) {
            if index < self.hot_data.len() {
                self.hot_data[index] = value;
            }
        }

        pub fn get_hot_data(&self, index: usize) -> Option<u64> {
            self.hot_data.get(index).copied()
        }
    }

    /// å†…å­˜é¢„å–å·¥å…·
    pub struct MemoryPrefetcher;

    impl MemoryPrefetcher {
        pub fn prefetch_read<T>(ptr: *const T) {
            unsafe {
                #[cfg(target_arch = "x86_64")]
                {
                    use std::arch::x86_64::_mm_prefetch;
                    _mm_prefetch(ptr as *const i8, 0); // _MM_HINT_T0
                }
            }
        }

        pub fn prefetch_write<T>(ptr: *const T) {
            unsafe {
                #[cfg(target_arch = "x86_64")]
                {
                    use std::arch::x86_64::_mm_prefetch;
                    _mm_prefetch(ptr as *const i8, 1); // _MM_HINT_T1
                }
            }
        }
    }

    /// å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
    pub struct MemoryAccessOptimizer;

    impl MemoryAccessOptimizer {
        /// é¡ºåºè®¿é—®æ¨¡å¼
        pub fn sequential_access<T>(data: &[T], mut f: impl FnMut(&T)) {
            for item in data {
                f(item);
            }
        }

        /// éšæœºè®¿é—®æ¨¡å¼
        pub fn random_access<T>(data: &[T], indices: &[usize], mut f: impl FnMut(&T)) {
            for &index in indices {
                if let Some(item) = data.get(index) {
                    f(item);
                }
            }
        }

        /// åˆ†å—è®¿é—®æ¨¡å¼
        pub fn blocked_access<T>(data: &[T], block_size: usize, mut f: impl FnMut(&[T])) {
            for chunk in data.chunks(block_size) {
                f(chunk);
            }
        }
    }
}

/// ä¸»æ¼”ç¤ºå‡½æ•°
pub fn demonstrate_memory_safety_advanced() {
    println!("ğŸ›¡ï¸  Rust 1.90 å†…å­˜å®‰å…¨é«˜çº§æ¼”ç¤º");
    println!("=================================");
    
    // 1. é«˜çº§ç”Ÿå‘½å‘¨æœŸç®¡ç†æ¼”ç¤º
    println!("\n=== é«˜çº§ç”Ÿå‘½å‘¨æœŸç®¡ç†æ¼”ç¤º ===");
    let data = 42i32;
    let tracker = advanced_lifetimes::LifetimeTracker::new(&data, 1);
    println!("ç”Ÿå‘½å‘¨æœŸè¿½è¸ªå™¨ ID: {}, å¹´é¾„: {:?}", tracker.tracker_id(), tracker.age());
    
    let long_lived = "é•¿æœŸæ•°æ®".to_string();
    let combinator = advanced_lifetimes::LifetimeCombinator::new(&data, &long_lived);
    let result = combinator.combine(|short, long| format!("{} + {}", short, long));
    println!("ç”Ÿå‘½å‘¨æœŸç»„åˆç»“æœ: {}", result);
    
    // 2. å†…å­˜å¸ƒå±€ä¼˜åŒ–æ¼”ç¤º
    println!("\n=== å†…å­˜å¸ƒå±€ä¼˜åŒ–æ¼”ç¤º ===");
    let aligned_data = memory_layout::CacheAligned::new(42u64);
    println!("ç¼“å­˜å¯¹é½æ•°æ®: {}", aligned_data.get());
    
    let memory_pool = memory_layout::MemoryPool::new(1024, 10);
    println!("å†…å­˜æ± å¯ç”¨å—æ•°: {}", memory_pool.available_blocks());
    
    // 3. é›¶æˆæœ¬æŠ½è±¡æ¼”ç¤º
    println!("\n=== é›¶æˆæœ¬æŠ½è±¡æ¼”ç¤º ===");
    let wrapper = zero_cost_abstractions::ZeroCostWrapper::new(100);
    println!("é›¶æˆæœ¬åŒ…è£…å™¨å€¼: {}", wrapper.get());
    
    const FACTORIAL_5: u32 = zero_cost_abstractions::compile_time_factorial(5);
    println!("ç¼–è¯‘æ—¶è®¡ç®— 5!: {}", FACTORIAL_5);
    
    // 4. æ™ºèƒ½æŒ‡é’ˆæ¼”ç¤º
    println!("\n=== æ™ºèƒ½æŒ‡é’ˆæ¼”ç¤º ===");
    let rc = smart_pointers::RefCounted::new(200);
    let rc2 = rc.clone();
    println!("å¼•ç”¨è®¡æ•°æ™ºèƒ½æŒ‡é’ˆå€¼: {}", rc.get());
    println!("å…‹éš†åå€¼: {}", rc2.get());
    
    // 5. å†…å­˜æ³„æ¼æ£€æµ‹æ¼”ç¤º
    println!("\n=== å†…å­˜æ³„æ¼æ£€æµ‹æ¼”ç¤º ===");
    let detector = memory_leak_detection::MemoryLeakDetector::new();
    let stats = detector.get_memory_stats();
    println!("å†…å­˜ç»Ÿè®¡: æ€»åˆ†é…={}, æ€»é‡Šæ”¾={}, å½“å‰åˆ†é…={}", 
             stats.total_allocated, stats.total_deallocated, stats.current_allocations);
    
    // 6. ç¼“å†²åŒºæº¢å‡ºé˜²æŠ¤æ¼”ç¤º
    println!("\n=== ç¼“å†²åŒºæº¢å‡ºé˜²æŠ¤æ¼”ç¤º ===");
    let mut safe_buffer = buffer_overflow_protection::SafeBuffer::new(100);
    match safe_buffer.write(0, b"Hello, World!") {
        Ok(_) => println!("å®‰å…¨å†™å…¥æˆåŠŸ"),
        Err(e) => println!("å†™å…¥å¤±è´¥: {}", e),
    }
    
    match safe_buffer.read(0, 13) {
        Ok(data) => println!("å®‰å…¨è¯»å–: {}", String::from_utf8_lossy(data)),
        Err(e) => println!("è¯»å–å¤±è´¥: {}", e),
    }
    
    // 7. å†…å­˜å¯¹é½å’Œç¼“å­˜ä¼˜åŒ–æ¼”ç¤º
    println!("\n=== å†…å­˜å¯¹é½å’Œç¼“å­˜ä¼˜åŒ–æ¼”ç¤º ===");
    let mut cache_friendly = memory_alignment_cache::CacheFriendlyStruct::new();
    cache_friendly.update_hot_data(0, 123);
    println!("ç¼“å­˜å‹å¥½ç»“æ„çƒ­æ•°æ®: {:?}", cache_friendly.hot_data);
    
    let data_array = vec![1, 2, 3, 4, 5];
    memory_alignment_cache::MemoryAccessOptimizer::sequential_access(&data_array, |x| {
        println!("é¡ºåºè®¿é—®: {}", x);
    });
    
    println!("\nâœ… å†…å­˜å®‰å…¨é«˜çº§æ¼”ç¤ºå®Œæˆï¼");
}
