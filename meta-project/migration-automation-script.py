#!/usr/bin/env python3
"""
Rustå½¢å¼åŒ–ç†è®ºé¡¹ç›®è‡ªåŠ¨åŒ–è¿ç§»è„šæœ¬
åŠŸèƒ½ï¼šå†…å®¹è¿ç§»ã€è´¨é‡æ£€æŸ¥ã€æœ¯è¯­æ ‡å‡†åŒ–ã€äº¤å‰å¼•ç”¨æ›´æ–°
"""

import os
import re
import json
import shutil
import hashlib
from typing import Dict, List, Set, Tuple, Optional
from pathlib import Path
from dataclasses import dataclass
from collections import defaultdict

@dataclass
class MigrationConfig:
    """è¿ç§»é…ç½®"""
    source_dirs: List[str]
    target_structure: Dict[str, str]
    terminology_dict: Dict[str, str]
    quality_standards: Dict[str, any]
    backup_dir: str = "migration-backup"

@dataclass
class ContentAnalysis:
    """å†…å®¹åˆ†æç»“æœ"""
    file_path: str
    line_count: int
    concept_count: int
    terminology_issues: List[str]
    quality_score: float
    dependencies: List[str]
    duplicates: List[str]

class TerminologyChecker:
    """æœ¯è¯­ä¸€è‡´æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self, terminology_dict: Dict[str, str]):
        self.terminology_dict = terminology_dict
        self.forbidden_terms = self._build_forbidden_terms()
    
    def _build_forbidden_terms(self) -> Dict[str, str]:
        """æ„å»ºç¦ç”¨æœ¯è¯­æ˜ å°„"""
        forbidden = {}
        for standard, forbidden_list in self.terminology_dict.items():
            if isinstance(forbidden_list, list):
                for forbidden_term in forbidden_list:
                    forbidden[forbidden_term] = standard
        return forbidden
    
    def check_file(self, file_path: str) -> List[str]:
        """æ£€æŸ¥æ–‡ä»¶ä¸­çš„æœ¯è¯­é—®é¢˜"""
        issues = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            for line_num, line in enumerate(content.split('\n'), 1):
                for forbidden_term, standard_term in self.forbidden_terms.items():
                    if forbidden_term in line:
                        issues.append(f"ç¬¬{line_num}è¡Œ: åº”ä½¿ç”¨'{standard_term}'è€Œé'{forbidden_term}'")
        except Exception as e:
            issues.append(f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
        
        return issues

class ContentDeduplicator:
    """å†…å®¹å»é‡å™¨"""
    
    def __init__(self):
        self.content_hashes = {}
        self.similar_contents = defaultdict(list)
    
    def analyze_content(self, file_path: str) -> str:
        """åˆ†æå†…å®¹å¹¶è¿”å›å“ˆå¸Œå€¼"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ ‡å‡†åŒ–å†…å®¹ï¼ˆå»é™¤ç©ºç™½å­—ç¬¦ã€æ³¨é‡Šç­‰ï¼‰
            normalized = self._normalize_content(content)
            content_hash = hashlib.md5(normalized.encode()).hexdigest()
            
            if content_hash in self.content_hashes:
                self.similar_contents[content_hash].append(file_path)
            else:
                self.content_hashes[content_hash] = file_path
                self.similar_contents[content_hash] = [file_path]
            
            return content_hash
        except Exception as e:
            return f"error:{e}"
    
    def _normalize_content(self, content: str) -> str:
        """æ ‡å‡†åŒ–å†…å®¹ç”¨äºæ¯”è¾ƒ"""
        # ç§»é™¤æ³¨é‡Š
        content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)
        content = re.sub(r'//.*', '', content)
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        
        # ç§»é™¤å¸¸è§å…ƒæ•°æ®
        content = re.sub(r'æœ€åæ›´æ–°.*?\n', '', content)
        content = re.sub(r'åˆ›å»ºæ—¥æœŸ.*?\n', '', content)
        
        return content.strip()
    
    def get_duplicates(self) -> List[List[str]]:
        """è·å–é‡å¤å†…å®¹çš„æ–‡ä»¶åˆ—è¡¨"""
        return [files for files in self.similar_contents.values() if len(files) > 1]

class QualityAssessor:
    """è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, standards: Dict[str, any]):
        self.standards = standards
    
    def assess_file(self, file_path: str) -> float:
        """è¯„ä¼°æ–‡ä»¶è´¨é‡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            score = 0.0
            max_score = 0.0
            
            # é•¿åº¦è¯„ä¼° (15%)
            line_count = len(content.split('\n'))
            length_score = min(line_count / self.standards.get('min_lines', 100), 1.0)
            score += length_score * 15
            max_score += 15
            
            # ç»“æ„è¯„ä¼° (25%)
            structure_score = self._assess_structure(content)
            score += structure_score * 25
            max_score += 25
            
            # æœ¯è¯­ä¸€è‡´æ€§è¯„ä¼° (20%)
            terminology_score = self._assess_terminology(content)
            score += terminology_score * 20
            max_score += 20
            
            # å†…å®¹æ·±åº¦è¯„ä¼° (25%)
            depth_score = self._assess_depth(content)
            score += depth_score * 25
            max_score += 25
            
            # æ ¼å¼è§„èŒƒè¯„ä¼° (15%)
            format_score = self._assess_format(content)
            score += format_score * 15
            max_score += 15
            
            return (score / max_score) * 10  # è½¬æ¢ä¸º10åˆ†åˆ¶
            
        except Exception as e:
            return 0.0
    
    def _assess_structure(self, content: str) -> float:
        """è¯„ä¼°æ–‡æ¡£ç»“æ„"""
        score = 0.0
        
        # æ£€æŸ¥æ ‡é¢˜å±‚æ¬¡
        if re.search(r'^# ', content, re.MULTILINE):
            score += 0.3
        if re.search(r'^## ', content, re.MULTILINE):
            score += 0.3
        if re.search(r'^### ', content, re.MULTILINE):
            score += 0.2
        
        # æ£€æŸ¥ä»£ç å—
        if '```' in content:
            score += 0.2
        
        return min(score, 1.0)
    
    def _assess_terminology(self, content: str) -> float:
        """è¯„ä¼°æœ¯è¯­ä¸€è‡´æ€§"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šæ£€æŸ¥å¸¸è§æœ¯è¯­ä¸ä¸€è‡´
        inconsistencies = 0
        
        # æ£€æŸ¥ä¸€äº›å¸¸è§çš„ä¸ä¸€è‡´æœ¯è¯­
        if 'ç‰¹å¾' in content and 'ç‰¹è´¨' in content:
            inconsistencies += 1
        if 'å¼•ç”¨' in content and 'å€Ÿç”¨' in content:
            inconsistencies += 1
        
        return max(0.0, 1.0 - inconsistencies * 0.3)
    
    def _assess_depth(self, content: str) -> float:
        """è¯„ä¼°å†…å®¹æ·±åº¦"""
        score = 0.0
        
        # æ£€æŸ¥ç†è®ºæ·±åº¦æŒ‡æ ‡
        if 'å®šä¹‰' in content or 'Definition' in content:
            score += 0.3
        if 'å®šç†' in content or 'Theorem' in content:
            score += 0.3
        if 'è¯æ˜' in content or 'Proof' in content:
            score += 0.2
        if 'ç¤ºä¾‹' in content or 'Example' in content:
            score += 0.2
        
        return min(score, 1.0)
    
    def _assess_format(self, content: str) -> float:
        """è¯„ä¼°æ ¼å¼è§„èŒƒ"""
        score = 1.0
        
        # æ£€æŸ¥å¸¸è§æ ¼å¼é—®é¢˜
        if re.search(r'[a-zA-Z][ä¸­æ–‡]', content):  # è‹±æ–‡ä¸­æ–‡æ²¡æœ‰ç©ºæ ¼
            score -= 0.2
        if re.search(r'[ä¸­æ–‡][a-zA-Z]', content):  # ä¸­æ–‡è‹±æ–‡æ²¡æœ‰ç©ºæ ¼
            score -= 0.2
        
        return max(0.0, score)

class DependencyAnalyzer:
    """ä¾èµ–å…³ç³»åˆ†æå™¨"""
    
    def __init__(self):
        self.dependencies = defaultdict(set)
        self.reverse_dependencies = defaultdict(set)
    
    def analyze_file(self, file_path: str) -> List[str]:
        """åˆ†ææ–‡ä»¶ä¾èµ–å…³ç³»"""
        dependencies = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŸ¥æ‰¾äº¤å‰å¼•ç”¨
            links = re.findall(r'\[.*?\]\((.*?)\)', content)
            for link in links:
                if link.endswith('.md'):
                    dependencies.append(link)
            
            # æŸ¥æ‰¾importæˆ–includeè¯­å¥
            imports = re.findall(r'import.*?from.*?"(.*?)"', content)
            dependencies.extend(imports)
            
            self.dependencies[file_path] = set(dependencies)
            for dep in dependencies:
                self.reverse_dependencies[dep].add(file_path)
            
        except Exception as e:
            pass
        
        return dependencies
    
    def detect_circular_dependencies(self) -> List[List[str]]:
        """æ£€æµ‹å¾ªç¯ä¾èµ–"""
        visited = set()
        rec_stack = set()
        cycles = []
        
        def dfs(node, path):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in self.dependencies.get(node, []):
                if neighbor not in visited:
                    cycle = dfs(neighbor, path + [neighbor])
                    if cycle:
                        cycles.append(cycle)
                elif neighbor in rec_stack:
                    # æ‰¾åˆ°å¾ªç¯
                    cycle_start = path.index(neighbor)
                    cycles.append(path[cycle_start:] + [neighbor])
            
            rec_stack.remove(node)
            return None
        
        for node in self.dependencies:
            if node not in visited:
                dfs(node, [node])
        
        return cycles

class MigrationEngine:
    """è¿ç§»å¼•æ“"""
    
    def __init__(self, config: MigrationConfig):
        self.config = config
        self.terminology_checker = TerminologyChecker(config.terminology_dict)
        self.deduplicator = ContentDeduplicator()
        self.quality_assessor = QualityAssessor(config.quality_standards)
        self.dependency_analyzer = DependencyAnalyzer()
        
        self.migration_log = []
        self.analysis_results = []
    
    def run_migration(self):
        """è¿è¡Œå®Œæ•´è¿ç§»æµç¨‹"""
        print("ğŸš€ å¼€å§‹Rustå½¢å¼åŒ–ç†è®ºé¡¹ç›®è¿ç§»...")
        
        # 1. åˆ›å»ºå¤‡ä»½
        self._create_backup()
        
        # 2. åˆ†æç°æœ‰å†…å®¹
        print("ğŸ“Š åˆ†æç°æœ‰å†…å®¹...")
        self._analyze_existing_content()
        
        # 3. æ‰§è¡Œå†…å®¹è¿ç§»
        print("ğŸ”„ æ‰§è¡Œå†…å®¹è¿ç§»...")
        self._migrate_content()
        
        # 4. æ›´æ–°äº¤å‰å¼•ç”¨
        print("ğŸ”— æ›´æ–°äº¤å‰å¼•ç”¨...")
        self._update_cross_references()
        
        # 5. ç”Ÿæˆè¿ç§»æŠ¥å‘Š
        print("ğŸ“‹ ç”Ÿæˆè¿ç§»æŠ¥å‘Š...")
        self._generate_migration_report()
        
        print("âœ… è¿ç§»å®Œæˆï¼")
    
    def _create_backup(self):
        """åˆ›å»ºå¤‡ä»½"""
        backup_path = Path(self.config.backup_dir)
        backup_path.mkdir(exist_ok=True)
        
        for source_dir in self.config.source_dirs:
            if os.path.exists(source_dir):
                backup_target = backup_path / Path(source_dir).name
                if backup_target.exists():
                    shutil.rmtree(backup_target)
                shutil.copytree(source_dir, backup_target)
                self.migration_log.append(f"âœ… å¤‡ä»½ {source_dir} åˆ° {backup_target}")
    
    def _analyze_existing_content(self):
        """åˆ†æç°æœ‰å†…å®¹"""
        for source_dir in self.config.source_dirs:
            if not os.path.exists(source_dir):
                continue
                
            for root, dirs, files in os.walk(source_dir):
                for file in files:
                    if file.endswith('.md'):
                        file_path = os.path.join(root, file)
                        analysis = self._analyze_file(file_path)
                        self.analysis_results.append(analysis)
    
    def _analyze_file(self, file_path: str) -> ContentAnalysis:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            line_count = len(content.split('\n'))
            concept_count = len(re.findall(r'##\s+.*', content))
            terminology_issues = self.terminology_checker.check_file(file_path)
            quality_score = self.quality_assessor.assess_file(file_path)
            dependencies = self.dependency_analyzer.analyze_file(file_path)
            
            # æ£€æŸ¥é‡å¤å†…å®¹
            content_hash = self.deduplicator.analyze_content(file_path)
            duplicates = [f for f in self.deduplicator.similar_contents[content_hash] 
                         if f != file_path]
            
            return ContentAnalysis(
                file_path=file_path,
                line_count=line_count,
                concept_count=concept_count,
                terminology_issues=terminology_issues,
                quality_score=quality_score,
                dependencies=dependencies,
                duplicates=duplicates
            )
        except Exception as e:
            return ContentAnalysis(
                file_path=file_path,
                line_count=0,
                concept_count=0,
                terminology_issues=[f"åˆ†æé”™è¯¯: {e}"],
                quality_score=0.0,
                dependencies=[],
                duplicates=[]
            )
    
    def _migrate_content(self):
        """è¿ç§»å†…å®¹åˆ°æ–°ç»“æ„"""
        for analysis in self.analysis_results:
            # ç¡®å®šç›®æ ‡ä½ç½®
            target_path = self._determine_target_path(analysis.file_path)
            if not target_path:
                continue
            
            # åˆ›å»ºç›®æ ‡ç›®å½•
            target_dir = os.path.dirname(target_path)
            os.makedirs(target_dir, exist_ok=True)
            
            # å¤åˆ¶å¹¶ä¼˜åŒ–å†…å®¹
            self._copy_and_optimize_content(analysis.file_path, target_path)
            
            self.migration_log.append(f"ğŸ“ è¿ç§» {analysis.file_path} â†’ {target_path}")
    
    def _determine_target_path(self, source_path: str) -> Optional[str]:
        """ç¡®å®šç›®æ ‡è·¯å¾„"""
        # æ ¹æ®æ–‡ä»¶å†…å®¹å’Œè·¯å¾„ç¡®å®šæ–°ä½ç½®
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ˜ å°„è§„åˆ™å®ç°
        for pattern, target_dir in self.config.target_structure.items():
            if pattern in source_path:
                filename = os.path.basename(source_path)
                return os.path.join(target_dir, filename)
        return None
    
    def _copy_and_optimize_content(self, source_path: str, target_path: str):
        """å¤åˆ¶å¹¶ä¼˜åŒ–å†…å®¹"""
        try:
            with open(source_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åº”ç”¨æœ¯è¯­æ ‡å‡†åŒ–
            optimized_content = self._apply_terminology_fixes(content)
            
            # åº”ç”¨æ ¼å¼æ ‡å‡†åŒ–
            optimized_content = self._apply_format_fixes(optimized_content)
            
            with open(target_path, 'w', encoding='utf-8') as f:
                f.write(optimized_content)
                
        except Exception as e:
            self.migration_log.append(f"âŒ å¤åˆ¶å¤±è´¥ {source_path}: {e}")
    
    def _apply_terminology_fixes(self, content: str) -> str:
        """åº”ç”¨æœ¯è¯­ä¿®æ­£"""
        for standard_term, forbidden_terms in self.config.terminology_dict.items():
            if isinstance(forbidden_terms, list):
                for forbidden_term in forbidden_terms:
                    content = content.replace(forbidden_term, standard_term)
        return content
    
    def _apply_format_fixes(self, content: str) -> str:
        """åº”ç”¨æ ¼å¼ä¿®æ­£"""
        # ä¸­è‹±æ–‡ä¹‹é—´æ·»åŠ ç©ºæ ¼
        content = re.sub(r'([a-zA-Z])([ä¸­æ–‡])', r'\1 \2', content)
        content = re.sub(r'([ä¸­æ–‡])([a-zA-Z])', r'\1 \2', content)
        
        # æ ‡å‡†åŒ–æ ‡é¢˜æ ¼å¼
        content = re.sub(r'^(#+)\s*(.+)', r'\1 \2', content, flags=re.MULTILINE)
        
        return content
    
    def _update_cross_references(self):
        """æ›´æ–°äº¤å‰å¼•ç”¨"""
        # æ„å»ºè·¯å¾„æ˜ å°„
        path_mapping = {}
        for analysis in self.analysis_results:
            old_path = analysis.file_path
            new_path = self._determine_target_path(old_path)
            if new_path:
                path_mapping[old_path] = new_path
        
        # æ›´æ–°æ‰€æœ‰æ–‡ä»¶ä¸­çš„å¼•ç”¨
        for new_path in path_mapping.values():
            if os.path.exists(new_path):
                self._update_file_references(new_path, path_mapping)
    
    def _update_file_references(self, file_path: str, path_mapping: Dict[str, str]):
        """æ›´æ–°æ–‡ä»¶ä¸­çš„å¼•ç”¨"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ›´æ–°markdowné“¾æ¥
            def replace_link(match):
                link_path = match.group(1)
                if link_path in path_mapping:
                    return f"]({path_mapping[link_path]})"
                return match.group(0)
            
            content = re.sub(r'\]\(([^)]+\.md)\)', replace_link, content)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
        except Exception as e:
            self.migration_log.append(f"âŒ æ›´æ–°å¼•ç”¨å¤±è´¥ {file_path}: {e}")
    
    def _generate_migration_report(self):
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        report = {
            "migration_summary": {
                "total_files_analyzed": len(self.analysis_results),
                "total_files_migrated": len([log for log in self.migration_log if "ğŸ“ è¿ç§»" in log]),
                "average_quality_score": sum(a.quality_score for a in self.analysis_results) / len(self.analysis_results) if self.analysis_results else 0,
            },
            "quality_distribution": self._calculate_quality_distribution(),
            "terminology_issues": self._summarize_terminology_issues(),
            "duplicates_found": self.deduplicator.get_duplicates(),
            "circular_dependencies": self.dependency_analyzer.detect_circular_dependencies(),
            "migration_log": self.migration_log
        }
        
        with open("meta-project/migration-report.json", 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆå¯è¯»æ€§æŠ¥å‘Š
        self._generate_readable_report(report)
    
    def _calculate_quality_distribution(self) -> Dict[str, int]:
        """è®¡ç®—è´¨é‡åˆ†å¸ƒ"""
        distribution = {"ä¼˜ç§€": 0, "è‰¯å¥½": 0, "éœ€è¦æ”¹è¿›": 0, "è´¨é‡ä¸è¶³": 0}
        
        for analysis in self.analysis_results:
            if analysis.quality_score >= 8.0:
                distribution["ä¼˜ç§€"] += 1
            elif analysis.quality_score >= 7.0:
                distribution["è‰¯å¥½"] += 1
            elif analysis.quality_score >= 6.0:
                distribution["éœ€è¦æ”¹è¿›"] += 1
            else:
                distribution["è´¨é‡ä¸è¶³"] += 1
        
        return distribution
    
    def _summarize_terminology_issues(self) -> Dict[str, int]:
        """æ±‡æ€»æœ¯è¯­é—®é¢˜"""
        issues = defaultdict(int)
        for analysis in self.analysis_results:
            for issue in analysis.terminology_issues:
                issues[issue] += 1
        return dict(issues)
    
    def _generate_readable_report(self, report: Dict):
        """ç”Ÿæˆå¯è¯»æ€§æŠ¥å‘Š"""
        readable_report = f"""
# ğŸ”„ Rustå½¢å¼åŒ–ç†è®ºé¡¹ç›®è¿ç§»æŠ¥å‘Š

**è¿ç§»æ—¥æœŸ**: {os.path.basename(__file__)}  
**åˆ†ææ–‡ä»¶æ•°**: {report['migration_summary']['total_files_analyzed']}  
**è¿ç§»æ–‡ä»¶æ•°**: {report['migration_summary']['total_files_migrated']}  
**å¹³å‡è´¨é‡è¯„åˆ†**: {report['migration_summary']['average_quality_score']:.2f}/10  

## ğŸ“Š è´¨é‡åˆ†å¸ƒ

```
ä¼˜ç§€ (8.0+åˆ†): {report['quality_distribution']['ä¼˜ç§€']}ä¸ªæ–‡ä»¶
è‰¯å¥½ (7.0-7.9åˆ†): {report['quality_distribution']['è‰¯å¥½']}ä¸ªæ–‡ä»¶  
éœ€è¦æ”¹è¿› (6.0-6.9åˆ†): {report['quality_distribution']['éœ€è¦æ”¹è¿›']}ä¸ªæ–‡ä»¶
è´¨é‡ä¸è¶³ (<6.0åˆ†): {report['quality_distribution']['è´¨é‡ä¸è¶³']}ä¸ªæ–‡ä»¶
```

## ğŸ”§ æœ¯è¯­é—®é¢˜æ±‡æ€»

{chr(10).join([f"- {issue}: {count}æ¬¡" for issue, count in list(report['terminology_issues'].items())[:10]])}

## ğŸ” å‘ç°çš„é‡å¤å†…å®¹

{chr(10).join([f"- é‡å¤ç»„ {i+1}: {', '.join(group)}" for i, group in enumerate(report['duplicates_found'][:5])])}

## âš ï¸ å¾ªç¯ä¾èµ–

{chr(10).join([f"- å¾ªç¯ {i+1}: {' â†’ '.join(cycle)}" for i, cycle in enumerate(report['circular_dependencies'][:3])])}

## ğŸ“ è¿ç§»æ—¥å¿—

{chr(10).join(report['migration_log'][-20:])}  # åªæ˜¾ç¤ºæœ€å20æ¡

---

**æŠ¥å‘ŠçŠ¶æ€**: âœ… **è¿ç§»å®Œæˆ**  
**è´¨é‡æ”¹è¿›**: ğŸ“ˆ **æŒç»­ç›‘æ§**  
**ä¸‹ä¸€æ­¥**: ğŸ”„ **éªŒè¯å’Œä¼˜åŒ–**
"""
        
        with open("meta-project/migration-report.md", 'w', encoding='utf-8') as f:
            f.write(readable_report)

def load_migration_config() -> MigrationConfig:
    """åŠ è½½è¿ç§»é…ç½®"""
    # åŸºæœ¬æœ¯è¯­å­—å…¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
    terminology_dict = {
        "ç‰¹è´¨": ["ç‰¹å¾", "ç‰¹æ€§"],
        "å€Ÿç”¨": ["å¼•ç”¨"],
        "æ‰€æœ‰æƒ": ["æ‹¥æœ‰æƒ"],
        "ç”Ÿå‘½å‘¨æœŸ": ["å­˜æ´»æœŸ", "ç”Ÿå­˜æœŸ"],
        "å¹¶å‘": ["å¹¶è¡Œ"],  # æ³¨æ„ï¼šè¿™ä¸¤ä¸ªæ¦‚å¿µå®é™…ä¸Šä¸åŒï¼Œéœ€è¦åŒºåˆ†
    }
    
    # ç›®æ ‡ç»“æ„æ˜ å°„
    target_structure = {
        "ownership": "theoretical-foundations/mathematical-models/linear-logic",
        "type": "theoretical-foundations/type-theory",
        "memory": "theoretical-foundations/memory-models",
        "concurrency": "theoretical-foundations/concurrency-models",
        "async": "theoretical-foundations/concurrency-models/async-models",
        "compiler": "system-mechanisms/compiler-theory",
        "runtime": "system-mechanisms/runtime-systems",
        "design_pattern": "engineering-practices/design-patterns",
        "performance": "engineering-practices/performance-engineering",
    }
    
    # è´¨é‡æ ‡å‡†
    quality_standards = {
        "min_lines": 100,
        "min_concepts": 5,
        "max_terminology_issues": 3,
        "min_structure_score": 0.7,
    }
    
    return MigrationConfig(
        source_dirs=["formal_rust", "crates", "docs"],
        target_structure=target_structure,
        terminology_dict=terminology_dict,
        quality_standards=quality_standards
    )

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Rustå½¢å¼åŒ–ç†è®ºé¡¹ç›®è‡ªåŠ¨åŒ–è¿ç§»å·¥å…·")
    print("=" * 50)
    
    config = load_migration_config()
    engine = MigrationEngine(config)
    
    try:
        engine.run_migration()
        print("\nğŸ‰ è¿ç§»æˆåŠŸå®Œæˆï¼")
        print("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: meta-project/migration-report.md")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¿ç§»è¿‡ç¨‹")
    except Exception as e:
        print(f"\nâŒ è¿ç§»è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 