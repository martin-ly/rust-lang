# è¿‡ç¨‹å®è¯­ä¹‰æ·±åº¦åˆ†æ

## ğŸ“… æ–‡æ¡£ä¿¡æ¯

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-08-11  
**æœ€åæ›´æ–°**: 2025-08-11  
**çŠ¶æ€**: å·²å®Œæˆ  
**è´¨é‡ç­‰çº§**: é’»çŸ³çº§ â­â­â­â­â­

---



## ç›®å½•

- [è¿‡ç¨‹å®è¯­ä¹‰æ·±åº¦åˆ†æ](#è¿‡ç¨‹å®è¯­ä¹‰æ·±åº¦åˆ†æ)
  - [ç›®å½•](#ç›®å½•)
  - [1. ç†è®ºåŸºç¡€](#1-ç†è®ºåŸºç¡€)
    - [1.1 æ•°å­¦å®šä¹‰](#11-æ•°å­¦å®šä¹‰)
    - [1.2 å½¢å¼åŒ–è¯­ä¹‰](#12-å½¢å¼åŒ–è¯­ä¹‰)
    - [1.3 ç¼–è¯‘æ—¶è®¡ç®—ç†è®º](#13-ç¼–è¯‘æ—¶è®¡ç®—ç†è®º)
    - [1.4 å…ƒç¼–ç¨‹èŒƒç•´è®º](#14-å…ƒç¼–ç¨‹èŒƒç•´è®º)
  - [2. Rustå®ç°åˆ†æ](#2-rustå®ç°åˆ†æ)
    - [2.1 æ ¸å¿ƒæ¶æ„ç‰¹æ€§](#21-æ ¸å¿ƒæ¶æ„ç‰¹æ€§)
    - [2.2 TokenStreamå¤„ç†æœºåˆ¶](#22-tokenstreamå¤„ç†æœºåˆ¶)
    - [2.3 å±æ€§å®å®ç°](#23-å±æ€§å®å®ç°)
    - [2.4 æ´¾ç”Ÿå®æœºåˆ¶](#24-æ´¾ç”Ÿå®æœºåˆ¶)
    - [2.5 å‡½æ•°å¼å®è®¾è®¡](#25-å‡½æ•°å¼å®è®¾è®¡)
  - [3. å®é™…åº”ç”¨](#3-å®é™…åº”ç”¨)
    - [3.1 å·¥ç¨‹æ¡ˆä¾‹åˆ†æ](#31-å·¥ç¨‹æ¡ˆä¾‹åˆ†æ)
    - [3.2 æœ€ä½³å®è·µç­–ç•¥](#32-æœ€ä½³å®è·µç­–ç•¥)
    - [3.3 æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯](#33-æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯)
  - [4. ç†è®ºå‰æ²¿](#4-ç†è®ºå‰æ²¿)
    - [4.1 æœ€æ–°å‘å±•è¶‹åŠ¿](#41-æœ€æ–°å‘å±•è¶‹åŠ¿)
    - [4.2 ç ”ç©¶æ–¹å‘æ¢ç´¢](#42-ç ”ç©¶æ–¹å‘æ¢ç´¢)
    - [4.3 åˆ›æ–°åº”ç”¨åœºæ™¯](#43-åˆ›æ–°åº”ç”¨åœºæ™¯)

## 1. ç†è®ºåŸºç¡€

### 1.1 æ•°å­¦å®šä¹‰

**å®šä¹‰ 5.2.3** (è¿‡ç¨‹å®è¯­ä¹‰åŸŸ)
è¿‡ç¨‹å®çš„è¯­ä¹‰åŸŸå®šä¹‰ä¸ºä¸‰å…ƒç»„ï¼š
$$\mathcal{M}_{proc} = (T, F, E)$$

å…¶ä¸­ï¼š

- $T$ æ˜¯TokenStreamæŠ½è±¡è¯­æ³•æ ‘ç©ºé—´
- $F: T \rightarrow T$ æ˜¯è½¬æ¢å‡½æ•°ç©ºé—´
- $E: Code \times \mathcal{M}_{proc} \rightarrow Code$ æ˜¯ä»£ç ç”Ÿæˆå‡½æ•°

**å®šä¹‰ 5.2.4** (ç¼–è¯‘æ—¶è®¡ç®—è¯­ä¹‰)
ç¼–è¯‘æ—¶è®¡ç®—çš„å½¢å¼åŒ–å®šä¹‰ï¼š
$$\text{CompileTime}: \mathcal{AST} \rightarrow \mathcal{Code}$$

æ»¡è¶³ï¼š

- **ç¡®å®šæ€§**: $\forall ast. \text{CompileTime}(ast)$ äº§ç”Ÿå”¯ä¸€ç»“æœ
- **ç»ˆæ­¢æ€§**: æ‰€æœ‰ç¼–è¯‘æ—¶è®¡ç®—éƒ½åœ¨æœ‰é™æ­¥éª¤å†…ç»ˆæ­¢
- **çº¯å‡½æ•°æ€§**: æ²¡æœ‰å‰¯ä½œç”¨ï¼Œåªä¾èµ–è¾“å…¥å‚æ•°

### 1.2 å½¢å¼åŒ–è¯­ä¹‰

**è§„åˆ™ 5.2.4** (è¿‡ç¨‹å®è°ƒç”¨è§„åˆ™)

```text
Î“ âŠ¢ input: TokenStream
Î“ âŠ¢ proc_macro: TokenStream â†’ TokenStream  
Î“ âŠ¢ proc_macro(input) â‡’ output
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Î“ âŠ¢ proc_macro!(input) â‡’ parse(output)
```

**è§„åˆ™ 5.2.5** (å±æ€§å®è¯­ä¹‰è§„åˆ™)

```text
Î“ âŠ¢ attrs: TokenStream
Î“ âŠ¢ item: TokenStream
Î“ âŠ¢ attr_macro(attrs, item) â‡’ new_item
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Î“ âŠ¢ #[attr_macro(attrs)] item â‡’ new_item
```

### 1.3 ç¼–è¯‘æ—¶è®¡ç®—ç†è®º

è¿‡ç¨‹å®çš„æ ¸å¿ƒæ˜¯ç¼–è¯‘æ—¶è®¡ç®—ç³»ç»Ÿï¼š

```mermaid
graph TD
    A[æºä»£ç ] --> B[è¯æ³•åˆ†æ]
    B --> C[è¯­æ³•åˆ†æ]
    C --> D[è¿‡ç¨‹å®è°ƒç”¨æ£€æµ‹]
    D --> E[TokenStreamæ„å»º]
    E --> F[è¿‡ç¨‹å®æ‰§è¡Œ]
    F --> G[æ–°ä»£ç ç”Ÿæˆ]
    G --> H[é‡æ–°è§£æ]
    H --> I[ç»§ç»­ç¼–è¯‘]
    
    subgraph "ç¼–è¯‘æ—¶ç¯å¢ƒ"
        F
        J[è¯­æ³•æ ‘è®¿é—®]
        K[ç±»å‹ä¿¡æ¯æŸ¥è¯¢]
        L[å±æ€§è§£æ]
    end
    
    F --> J
    F --> K
    F --> L
```

**å®šç† 5.2.3** (ç¼–è¯‘æ—¶è®¡ç®—å®Œå¤‡æ€§)
è¿‡ç¨‹å®ç³»ç»Ÿåœ¨ç¼–è¯‘æ—¶è®¡ç®—æ–¹é¢æ˜¯å›¾çµå®Œå¤‡çš„ï¼š
$$\forall f: \text{ComputableFunction}. \exists m: \text{ProcMacro}. m \text{ implements } f$$

### 1.4 å…ƒç¼–ç¨‹èŒƒç•´è®º

**å®šä¹‰ 5.2.5** (å…ƒç¼–ç¨‹èŒƒç•´)
å…ƒç¼–ç¨‹å½¢æˆèŒƒç•´ $\mathcal{M}$ï¼š

- **å¯¹è±¡**: ç¨‹åºè¡¨ç¤ºï¼ˆAST, TokenStreamï¼‰
- **æ€å°„**: ç¨‹åºå˜æ¢å‡½æ•°
- **åˆæˆ**: å˜æ¢çš„å¤åˆ
- **å•ä½æ€å°„**: æ’ç­‰å˜æ¢

**å‡½å­æ€§è´¨**ï¼š
$$\text{ProcMacro}: \mathcal{C}_{syntax} \rightarrow \mathcal{C}_{semantics}$$

ä¿æŒç»“æ„ï¼š

- $F(\text{id}_A) = \text{id}_{F(A)}$
- $F(g \circ f) = F(g) \circ F(f)$

## 2. Rustå®ç°åˆ†æ

### 2.1 æ ¸å¿ƒæ¶æ„ç‰¹æ€§

**åŸºç¡€è¿‡ç¨‹å®ç»“æ„**ï¼š

```rust
use proc_macro::TokenStream;
use quote::quote;
use syn::{parse_macro_input, DeriveInput, Data, Fields};

// åŸºç¡€è¿‡ç¨‹å®å®šä¹‰
#[proc_macro]
pub fn my_macro(input: TokenStream) -> TokenStream {
    // è§£æè¾“å…¥
    let parsed = parse_macro_input!(input as DeriveInput);
    
    // ç”Ÿæˆä»£ç 
    let expanded = quote! {
        // ç”Ÿæˆçš„ä»£ç 
    };
    
    TokenStream::from(expanded)
}

// å±æ€§å®å®šä¹‰
#[proc_macro_attribute]
pub fn my_attribute(args: TokenStream, input: TokenStream) -> TokenStream {
    let args = parse_macro_input!(args as syn::AttributeArgs);
    let input = parse_macro_input!(input as syn::ItemFn);
    
    let expanded = quote! {
        // è½¬æ¢åçš„ä»£ç 
    };
    
    TokenStream::from(expanded)
}

// æ´¾ç”Ÿå®å®šä¹‰
#[proc_macro_derive(MyTrait, attributes(my_attr))]
pub fn my_derive(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    
    let name = &input.ident;
    let expanded = quote! {
        impl MyTrait for #name {
            // è‡ªåŠ¨å®ç°
        }
    };
    
    TokenStream::from(expanded)
}
```

### 2.2 TokenStreamå¤„ç†æœºåˆ¶

**é«˜çº§TokenStreamæ“ä½œ**ï¼š

```rust
use proc_macro2::{TokenStream, TokenTree, Delimiter, Group, Punct, Spacing};
use quote::{quote, quote_spanned, ToTokens};
use syn::spanned::Spanned;

fn advanced_token_processing(input: TokenStream) -> TokenStream {
    let mut output = TokenStream::new();
    
    for token in input {
        match token {
            TokenTree::Group(group) => {
                // å¤„ç†åˆ†ç»„æ ‡è®°
                let delimiter = group.delimiter();
                let stream = group.stream();
                
                let processed = match delimiter {
                    Delimiter::Parenthesis => process_parens(stream),
                    Delimiter::Brace => process_braces(stream),
                    Delimiter::Bracket => process_brackets(stream),
                    Delimiter::None => process_none(stream),
                };
                
                output.extend(processed);
            }
            TokenTree::Ident(ident) => {
                // æ ‡è¯†ç¬¦è½¬æ¢
                let new_ident = transform_identifier(&ident);
                output.extend(quote! { #new_ident });
            }
            TokenTree::Punct(punct) => {
                // æ ‡ç‚¹ç¬¦å·å¤„ç†
                output.extend(handle_punctuation(punct));
            }
            TokenTree::Literal(lit) => {
                // å­—é¢é‡å¤„ç†
                output.extend(transform_literal(lit));
            }
        }
    }
    
    output
}

// é”™è¯¯å¤„ç†å’Œè¯Šæ–­
fn error_handling_macro(input: TokenStream) -> TokenStream {
    match syn::parse2::<syn::DeriveInput>(input) {
        Ok(parsed) => {
            // æˆåŠŸè§£æï¼Œç”Ÿæˆä»£ç 
            generate_code(parsed)
        }
        Err(error) => {
            // è§£æé”™è¯¯ï¼Œç”Ÿæˆç¼–è¯‘é”™è¯¯
            let error_msg = error.to_string();
            quote! {
                compile_error!(#error_msg);
            }
        }
    }
}
```

### 2.3 å±æ€§å®å®ç°

**å¤æ‚å±æ€§å®æ¡ˆä¾‹**ï¼š

```rust
// æ€§èƒ½ç›‘æ§å±æ€§å®
#[proc_macro_attribute]
pub fn monitor_performance(args: TokenStream, input: TokenStream) -> TokenStream {
    let args = parse_macro_input!(args as syn::AttributeArgs);
    let input_fn = parse_macro_input!(input as syn::ItemFn);
    
    let fn_name = &input_fn.sig.ident;
    let fn_name_str = fn_name.to_string();
    
    // è§£æå‚æ•°
    let enable_logging = args.iter().any(|arg| {
        if let syn::NestedMeta::Meta(syn::Meta::Path(path)) = arg {
            path.is_ident("log")
        } else {
            false
        }
    });
    
    let monitoring_code = if enable_logging {
        quote! {
            println!("Entering function: {}", #fn_name_str);
            let _start = std::time::Instant::now();
        }
    } else {
        quote! {
            let _start = std::time::Instant::now();
        }
    };
    
    let cleanup_code = if enable_logging {
        quote! {
            let _duration = _start.elapsed();
            println!("Function {} took: {:?}", #fn_name_str, _duration);
        }
    } else {
        quote! {}
    };
    
    let original_body = &input_fn.block;
    let visibility = &input_fn.vis;
    let signature = &input_fn.sig;
    
    let expanded = quote! {
        #visibility #signature {
            #monitoring_code
            
            let result = #original_body;
            
            #cleanup_code
            
            result
        }
    };
    
    TokenStream::from(expanded)
}

// ä½¿ç”¨ç¤ºä¾‹
#[monitor_performance(log)]
fn expensive_computation(n: u64) -> u64 {
    (0..n).sum()
}
```

### 2.4 æ´¾ç”Ÿå®æœºåˆ¶

**é«˜çº§æ´¾ç”Ÿå®å®ç°**ï¼š

```rust
// Builderæ¨¡å¼æ´¾ç”Ÿå®
#[proc_macro_derive(Builder, attributes(builder))]
pub fn derive_builder(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    
    let name = &input.ident;
    let builder_name = format_ident!("{}Builder", name);
    
    let fields = match &input.data {
        Data::Struct(data) => {
            match &data.fields {
                Fields::Named(fields) => &fields.named,
                _ => panic!("Builderåªæ”¯æŒå‘½åå­—æ®µ"),
            }
        }
        _ => panic!("Builderåªæ”¯æŒç»“æ„ä½“"),
    };
    
    // ç”ŸæˆBuilderå­—æ®µ
    let builder_fields = fields.iter().map(|field| {
        let name = &field.ident;
        let ty = &field.ty;
        quote! {
            #name: Option<#ty>
        }
    });
    
    // ç”Ÿæˆsetteræ–¹æ³•
    let setters = fields.iter().map(|field| {
        let name = &field.ident;
        let ty = &field.ty;
        quote! {
            pub fn #name(mut self, #name: #ty) -> Self {
                self.#name = Some(#name);
                self
            }
        }
    });
    
    // ç”Ÿæˆbuildæ–¹æ³•
    let build_assignments = fields.iter().map(|field| {
        let name = &field.ident;
        quote! {
            #name: self.#name.ok_or_else(|| 
                format!("Field '{}' is required", stringify!(#name)))?
        }
    });
    
    let expanded = quote! {
        impl #name {
            pub fn builder() -> #builder_name {
                #builder_name::new()
            }
        }
        
        pub struct #builder_name {
            #(#builder_fields),*
        }
        
        impl #builder_name {
            pub fn new() -> Self {
                Self {
                    #(#fields.ident: None),*
                }
            }
            
            #(#setters)*
            
            pub fn build(self) -> Result<#name, String> {
                Ok(#name {
                    #(#build_assignments),*
                })
            }
        }
    };
    
    TokenStream::from(expanded)
}

// ä½¿ç”¨ç¤ºä¾‹
#[derive(Builder)]
struct User {
    name: String,
    age: u32,
    email: String,
}

// ç”Ÿæˆçš„ä»£ç å…è®¸è¿™æ ·ä½¿ç”¨ï¼š
// let user = User::builder()
//     .name("Alice".to_string())
//     .age(30)
//     .email("alice@example.com".to_string())
//     .build()
//     .unwrap();
```

### 2.5 å‡½æ•°å¼å®è®¾è®¡

**å‡½æ•°å¼è¿‡ç¨‹å®æ¨¡å¼**ï¼š

```rust
// DSLæ„å»ºå®
#[proc_macro]
pub fn sql_dsl(input: TokenStream) -> TokenStream {
    let parsed = parse_macro_input!(input as SqlQuery);
    
    let expanded = generate_sql_code(parsed);
    TokenStream::from(expanded)
}

// è‡ªå®šä¹‰è§£æå™¨
struct SqlQuery {
    select: Vec<syn::Ident>,
    from: syn::Ident,
    where_clause: Option<syn::Expr>,
}

impl syn::parse::Parse for SqlQuery {
    fn parse(input: syn::parse::ParseStream) -> syn::Result<Self> {
        // SELECTå…³é”®å­—
        input.parse::<syn::Token![select]>()?;
        
        // å­—æ®µåˆ—è¡¨
        let select = Punctuated::<syn::Ident, syn::Token![,]>::parse_separated_nonempty(input)?
            .into_iter()
            .collect();
        
        // FROMå…³é”®å­—
        input.parse::<syn::Token![from]>()?;
        let from = input.parse()?;
        
        // å¯é€‰WHEREå­å¥
        let where_clause = if input.peek(syn::Token![where]) {
            input.parse::<syn::Token![where]>()?;
            Some(input.parse()?)
        } else {
            None
        };
        
        Ok(SqlQuery {
            select,
            from,
            where_clause,
        })
    }
}

fn generate_sql_code(query: SqlQuery) -> proc_macro2::TokenStream {
    let select_fields = query.select.iter()
        .map(|field| field.to_string())
        .collect::<Vec<_>>()
        .join(", ");
    
    let table_name = query.from.to_string();
    
    let where_part = if let Some(where_expr) = query.where_clause {
        quote! {
            query.push_str(" WHERE ");
            query.push_str(&#where_expr);
        }
    } else {
        quote! {}
    };
    
    quote! {
        {
            let mut query = String::new();
            query.push_str("SELECT ");
            query.push_str(#select_fields);
            query.push_str(" FROM ");
            query.push_str(#table_name);
            #where_part
            query
        }
    }
}
```

## 3. å®é™…åº”ç”¨

### 3.1 å·¥ç¨‹æ¡ˆä¾‹åˆ†æ

**æ¡ˆä¾‹1ï¼šORMä»£ç ç”Ÿæˆ**:

```rust
// æ•°æ®åº“å®ä½“å®
#[proc_macro_derive(Entity, attributes(table, column))]
pub fn derive_entity(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    
    // æå–è¡¨å
    let table_name = extract_table_name(&input.attrs)
        .unwrap_or_else(|| input.ident.to_string().to_lowercase());
    
    let struct_name = &input.ident;
    let fields = extract_fields(&input.data);
    
    // ç”ŸæˆSQLæŸ¥è¯¢æ–¹æ³•
    let find_by_id = generate_find_by_id(&table_name, &fields);
    let insert_method = generate_insert(&table_name, &fields);
    let update_method = generate_update(&table_name, &fields);
    let delete_method = generate_delete(&table_name);
    
    let expanded = quote! {
        impl #struct_name {
            #find_by_id
            #insert_method
            #update_method
            #delete_method
        }
    };
    
    TokenStream::from(expanded)
}

// ä½¿ç”¨ç¤ºä¾‹
#[derive(Entity)]
#[table(name = "users")]
struct User {
    #[column(primary_key)]
    id: i64,
    #[column(unique)]
    email: String,
    name: String,
    created_at: chrono::DateTime<chrono::Utc>,
}
```

**æ¡ˆä¾‹2ï¼šåºåˆ—åŒ–ä¼˜åŒ–å®**:

```rust
// é«˜æ€§èƒ½åºåˆ—åŒ–å®
#[proc_macro_derive(FastSerialize)]
pub fn derive_fast_serialize(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    
    let name = &input.ident;
    let serialization_code = generate_optimized_serialization(&input);
    let deserialization_code = generate_optimized_deserialization(&input);
    
    let expanded = quote! {
        impl FastSerialize for #name {
            fn serialize_fast(&self, writer: &mut impl std::io::Write) -> std::io::Result<()> {
                #serialization_code
            }
            
            fn deserialize_fast(reader: &mut impl std::io::Read) -> std::io::Result<Self> {
                #deserialization_code
            }
        }
    };
    
    TokenStream::from(expanded)
}
```

### 3.2 æœ€ä½³å®è·µç­–ç•¥

**ç­–ç•¥1ï¼šé”™è¯¯å¤„ç†æ¨¡å¼**:

```rust
// é”™è¯¯å¤„ç†å®
#[proc_macro_attribute]
pub fn handle_errors(args: TokenStream, input: TokenStream) -> TokenStream {
    let input_fn = parse_macro_input!(input as syn::ItemFn);
    
    // æ£€æŸ¥è¿”å›ç±»å‹æ˜¯å¦æ˜¯Result
    let return_type = match &input_fn.sig.output {
        syn::ReturnType::Type(_, ty) => ty,
        _ => {
            return syn::Error::new_spanned(
                &input_fn.sig,
                "handle_errorsåªèƒ½ç”¨äºè¿”å›Resultçš„å‡½æ•°"
            ).to_compile_error().into();
        }
    };
    
    // éªŒè¯è¿”å›ç±»å‹
    if !is_result_type(return_type) {
        return syn::Error::new_spanned(
            return_type,
            "å‡½æ•°å¿…é¡»è¿”å›Resultç±»å‹"
        ).to_compile_error().into();
    }
    
    // ç”Ÿæˆé”™è¯¯å¤„ç†ä»£ç ...
    generate_error_handling_wrapper(input_fn)
}
```

**ç­–ç•¥2ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€æœ¯**:

```rust
// ç¼–è¯‘æ—¶è®¡ç®—ç¼“å­˜
static COMPUTATION_CACHE: std::sync::Mutex<std::collections::HashMap<String, String>> = 
    std::sync::Mutex::new(std::collections::HashMap::new());

#[proc_macro]
pub fn cached_computation(input: TokenStream) -> TokenStream {
    let input_str = input.to_string();
    
    // æ£€æŸ¥ç¼“å­˜
    if let Ok(cache) = COMPUTATION_CACHE.lock() {
        if let Some(cached_result) = cache.get(&input_str) {
            return cached_result.parse().unwrap();
        }
    }
    
    // æ‰§è¡Œè®¡ç®—
    let result = expensive_computation(input);
    let result_str = result.to_string();
    
    // æ›´æ–°ç¼“å­˜
    if let Ok(mut cache) = COMPUTATION_CACHE.lock() {
        cache.insert(input_str, result_str);
    }
    
    result
}
```

### 3.3 æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

**æŠ€æœ¯1ï¼šé›¶åˆ†é…ä¼˜åŒ–**:

```rust
// é›¶åˆ†é…å®å®ç°
#[proc_macro]
pub fn zero_alloc_format(input: TokenStream) -> TokenStream {
    let format_args = parse_macro_input!(input as FormatArgs);
    
    // åˆ†ææ ¼å¼å­—ç¬¦ä¸²ï¼Œç”Ÿæˆä¼˜åŒ–çš„ä»£ç 
    let optimized_code = if format_args.is_compile_time_constant() {
        // ç¼–è¯‘æ—¶å·²çŸ¥ï¼Œç›´æ¥åµŒå…¥
        let result = format_args.evaluate_at_compile_time();
        quote! { #result }
    } else {
        // è¿è¡Œæ—¶è®¡ç®—ï¼Œä½†é¿å…åˆ†é…
        generate_stack_based_formatting(format_args)
    };
    
    TokenStream::from(optimized_code)
}
```

**æŠ€æœ¯2ï¼šå†…è”ä¼˜åŒ–**:

```rust
// å¼ºåˆ¶å†…è”å®
#[proc_macro_attribute]
pub fn force_inline(_args: TokenStream, input: TokenStream) -> TokenStream {
    let mut input_fn = parse_macro_input!(input as syn::ItemFn);
    
    // æ·»åŠ å†…è”å±æ€§
    input_fn.attrs.push(syn::parse_quote! {
        #[inline(always)]
    });
    
    // æ·»åŠ æ€§èƒ½æç¤º
    input_fn.attrs.push(syn::parse_quote! {
        #[cold]  // å¦‚æœå‡½æ•°å¾ˆå°‘è¢«è°ƒç”¨
    });
    
    quote! { #input_fn }.into()
}
```

## 4. ç†è®ºå‰æ²¿

### 4.1 æœ€æ–°å‘å±•è¶‹åŠ¿

**1. ç¼–è¯‘æ—¶åå°„ç³»ç»Ÿ**:

```rust
// æœªæ¥å¯èƒ½çš„åå°„å®
#[proc_macro]
pub fn reflect_type(input: TokenStream) -> TokenStream {
    let type_name = parse_macro_input!(input as syn::Type);
    
    // ç¼–è¯‘æ—¶ç±»å‹ä¿¡æ¯è®¿é—®
    let type_info = get_compile_time_type_info(&type_name);
    
    quote! {
        TypeInfo {
            name: #type_info.name,
            size: #type_info.size,
            alignment: #type_info.alignment,
            fields: &[#(#type_info.fields),*],
            methods: &[#(#type_info.methods),*],
        }
    }.into()
}
```

**2. å¢é‡ç¼–è¯‘ä¼˜åŒ–**:

```rust
// å¢é‡ç¼–è¯‘æ„ŸçŸ¥å®
#[proc_macro_derive(IncrementalDerive)]
pub fn incremental_derive(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    
    // è®¡ç®—è¾“å…¥å“ˆå¸Œ
    let input_hash = calculate_hash(&input);
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ
    if is_cached_and_valid(input_hash) {
        return load_cached_result(input_hash);
    }
    
    // ç”Ÿæˆæ–°ä»£ç å¹¶ç¼“å­˜
    let result = generate_code(&input);
    cache_result(input_hash, &result);
    
    result
}
```

### 4.2 ç ”ç©¶æ–¹å‘æ¢ç´¢

**æ–¹å‘1ï¼šç±»å‹çº§ç¼–ç¨‹é›†æˆ**:

```rust
// ç±»å‹çº§è®¡ç®—å®
#[proc_macro]
pub fn type_level_computation(input: TokenStream) -> TokenStream {
    // åœ¨ç±»å‹çº§åˆ«è¿›è¡Œè®¡ç®—
    let computation = parse_type_level_expr(input);
    let result_type = evaluate_type_expr(computation);
    
    quote! {
        type ComputationResult = #result_type;
    }.into()
}
```

**æ–¹å‘2ï¼šå½¢å¼åŒ–éªŒè¯æ”¯æŒ**:

```rust
// éªŒè¯å±æ€§å®
#[proc_macro_attribute]
pub fn verify(args: TokenStream, input: TokenStream) -> TokenStream {
    let verification_spec = parse_macro_input!(args as VerificationSpec);
    let function = parse_macro_input!(input as syn::ItemFn);
    
    // ç”ŸæˆéªŒè¯ä»£ç 
    let verification_code = generate_verification_checks(&verification_spec);
    
    // é›†æˆåˆ°å‡½æ•°ä¸­
    integrate_verification(function, verification_code)
}
```

### 4.3 åˆ›æ–°åº”ç”¨åœºæ™¯

**åº”ç”¨1ï¼šAIä»£ç ç”Ÿæˆé›†æˆ**:

```rust
// AIè¾…åŠ©ä»£ç ç”Ÿæˆå®
#[proc_macro_attribute]
pub fn ai_generate(args: TokenStream, input: TokenStream) -> TokenStream {
    let spec = parse_macro_input!(args as GenerationSpec);
    let template = parse_macro_input!(input as syn::ItemStruct);
    
    // è°ƒç”¨AIæ¨¡å‹ç”Ÿæˆä»£ç 
    let generated_code = ai_model.generate_code(spec, template);
    
    // éªŒè¯ç”Ÿæˆçš„ä»£ç 
    validate_generated_code(&generated_code);
    
    generated_code
}
```

**åº”ç”¨2ï¼šè·¨è¯­è¨€ä»£ç ç”Ÿæˆ**:

```rust
// å¤šè¯­è¨€ç»‘å®šç”Ÿæˆ
#[proc_macro_derive(MultiLangBindings, attributes(export_to))]
pub fn derive_multi_lang_bindings(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    
    let target_languages = extract_target_languages(&input.attrs);
    
    let mut generated = TokenStream::new();
    
    for lang in target_languages {
        let binding_code = match lang.as_str() {
            "c" => generate_c_bindings(&input),
            "python" => generate_python_bindings(&input),
            "javascript" => generate_js_bindings(&input),
            _ => continue,
        };
        
        generated.extend(binding_code);
    }
    
    generated
}
```

---

> **é“¾æ¥ç½‘ç»œ**ï¼š
>
> - [å£°æ˜å¼å®è¯­ä¹‰åˆ†æ](02_declarative_macro_semantics.md)
> - [å®å±•å¼€è¯­ä¹‰æ¨¡å‹](04_macro_expansion_semantics.md)
> - [ç¼–è¯‘æ—¶è½¬æ¢ç†è®º](../../01_compile_time_transformation/)
> - [traitç³»ç»Ÿè¯­ä¹‰](../../03_trait_system_semantics/)

---

> **ç‰ˆæœ¬ä¿¡æ¯**ï¼šæ–‡æ¡£ç‰ˆæœ¬ v1.0.0ï¼Œæœ€åæ›´æ–°äº 2024-12-30
