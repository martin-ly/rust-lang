# Rust 2025: 分布式系统设计模式实现指南

```text
这个问题要求我提供关于分布式设计模式的全面解释和Rust实现。我需要：

1. 梳理分布式设计模式的分类、核心概念、定义、优缺点
2. 为每个模式提供实现代码（使用async异步机制、多线程机制、泛型机制）
3. 分析Rust开源库中这些模式的使用，并推荐优秀实现

先来列出主要的分布式设计模式分类：

1. **通信模式**
   - 请求-响应模式
   - 发布-订阅模式
   - 消息队列模式
   - RPC模式
   - 流处理模式

2. **一致性与复制模式**
   - 主从复制
   - 多主复制
   - 无主复制
   - 分布式共识（Paxos, Raft）
   - 最终一致性

3. **分区模式**
   - 分片
   - 一致性哈希
   - 副本分布

4. **容错模式**
   - 熔断器
   - 舱壁模式
   - 超时与重试
   - 背压

5. **事务模式**
   - 两阶段提交
   - 三阶段提交
   - SAGA模式
   - TCC模式

6. **缓存模式**
   - 本地缓存
   - 分布式缓存
   - 缓存穿透/击穿防御

7. **服务发现与配置**
   - 服务注册与发现
   - 配置中心
   - 边车模式

8. **调度与负载均衡**
   - 负载均衡模式
   - 服务网格
   - 任务调度

现在，我将为每个主要类别选择一些关键模式，
提供其概念、优缺点，以及Rust 2025年版本的实现。
```

## 目录

1. [通信模式](#1-通信模式)  
   1.1 [请求-响应模式](#11-请求-响应模式)  
   1.2 [发布-订阅模式](#12-发布-订阅模式)  
   1.3 [事件溯源模式](#13-事件溯源模式)  

2. [一致性与复制模式](#2-一致性与复制模式)  
   2.1 [分布式共识 (Raft)](#21-分布式共识-raft)  
   2.2 [CRDT (无冲突复制数据类型)](#22-crdt-无冲突复制数据类型)  

3. [容错模式](#3-容错模式)  
   3.1 [熔断器模式](#31-熔断器模式)  
   3.2 [舱壁模式](#32-舱壁模式)  
   3.3 [背压模式](#33-背压模式)  

4. [分区与扩展模式](#4-分区与扩展模式)  
   4.1 [一致性哈希](#41-一致性哈希)  
   4.2 [分片模式](#42-分片模式)  

5. [综合模式与开源库分析](#5-综合模式与开源库分析)

## 1. 通信模式

### 1.1 请求-响应模式

**核心概念**: 客户端发送请求，服务器处理并返回响应。可以是同步的也可以是异步的。

**优点**:

- 简单直观，易于实现和理解
- 适合请求-响应类型的交互
- 可以集成重试、超时等机制

**缺点**:

- 紧耦合：客户端必须等待响应
- 可能导致阻塞
- 难以扩展到复杂的交互模式

#### 使用异步机制实现

```rust
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, oneshot};
use tokio::time::{timeout, Duration};
use anyhow::{Result, anyhow};
use serde::{Serialize, Deserialize};
use uuid::Uuid;
use tracing::{info, error, debug};

// 请求和响应类型
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Request<T> {
    request_id: String,
    timestamp: u64,
    payload: T,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Response<T> {
    request_id: String,
    timestamp: u64,
    payload: T,
}

// 请求-响应服务器
pub struct RequestResponseServer<Req, Resp> 
where
    Req: Send + Sync + 'static + Clone,
    Resp: Send + Sync + 'static + Clone,
{
    request_rx: mpsc::Receiver<(Request<Req>, oneshot::Sender<Response<Resp>>)>,
    handler: Arc<dyn Fn(Request<Req>) -> Result<Response<Resp>> + Send + Sync>,
}

impl<Req, Resp> RequestResponseServer<Req, Resp>
where
    Req: Send + Sync + 'static + Clone,
    Resp: Send + Sync + 'static + Clone,
{
    pub fn new(
        request_rx: mpsc::Receiver<(Request<Req>, oneshot::Sender<Response<Resp>>)>,
        handler: impl Fn(Request<Req>) -> Result<Response<Resp>> + Send + Sync + 'static,
    ) -> Self {
        Self {
            request_rx,
            handler: Arc::new(handler),
        }
    }

    pub async fn run(mut self) {
        while let Some((request, response_tx)) = self.request_rx.recv().await {
            let handler = self.handler.clone();
            
            // 处理请求
            let result = handler(request);
            
            // 发送响应
            match result {
                Ok(response) => {
                    if response_tx.send(response).is_err() {
                        error!("客户端已断开连接，无法发送响应");
                    }
                }
                Err(err) => {
                    error!("处理请求失败: {}", err);
                    // 也可以发送错误响应
                }
            }
        }
    }
}

// 请求-响应客户端
pub struct RequestResponseClient<Req, Resp>
where
    Req: Send + Sync + 'static + Clone,
    Resp: Send + Sync + 'static + Clone,
{
    request_tx: mpsc::Sender<(Request<Req>, oneshot::Sender<Response<Resp>>)>,
    pending_requests: Arc<Mutex<HashMap<String, oneshot::Sender<Response<Resp>>>>>,
    timeout_duration: Duration,
}

impl<Req, Resp> RequestResponseClient<Req, Resp>
where
    Req: Send + Sync + 'static + Clone,
    Resp: Send + Sync + 'static + Clone,
{
    pub fn new(
        request_tx: mpsc::Sender<(Request<Req>, oneshot::Sender<Response<Resp>>)>,
        timeout_duration: Duration,
    ) -> Self {
        Self {
            request_tx,
            pending_requests: Arc::new(Mutex::new(HashMap::new())),
            timeout_duration,
        }
    }

    pub async fn send_request(&self, payload: Req) -> Result<Resp> {
        // 创建请求
        let request_id = Uuid::new_v4().to_string();
        let request = Request {
            request_id: request_id.clone(),
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
            payload,
        };
        
        // 创建用于接收响应的通道
        let (response_tx, response_rx) = oneshot::channel();
        
        // 存储挂起的请求
        {
            let mut pending = self.pending_requests.lock().await;
            pending.insert(request_id.clone(), response_tx.clone());
        }
        
        // 发送请求
        if let Err(e) = self.request_tx.send((request, response_tx)).await {
            error!("发送请求失败: {}", e);
            return Err(anyhow!("发送请求失败: {}", e));
        }
        
        // 等待响应，带超时
        match timeout(self.timeout_duration, response_rx).await {
            Ok(Ok(response)) => {
                // 移除挂起的请求
                let mut pending = self.pending_requests.lock().await;
                pending.remove(&request_id);
                
                Ok(response.payload)
            }
            Ok(Err(_)) => {
                error!("响应通道关闭");
                Err(anyhow!("响应通道关闭"))
            }
            Err(_) => {
                // 超时
                let mut pending = self.pending_requests.lock().await;
                pending.remove(&request_id);
                
                error!("请求超时");
                Err(anyhow!("请求超时"))
            }
        }
    }
}

// 使用例子
#[tokio::main]
async fn main() -> Result<()> {
    // 创建通道
    let (request_tx, request_rx) = mpsc::channel(100);
    
    // 创建服务器
    let server = RequestResponseServer::new(
        request_rx,
        |request: Request<String>| {
            // 简单的回显服务
            let response = Response {
                request_id: request.request_id,
                timestamp: std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_secs(),
                payload: format!("回应: {}", request.payload),
            };
            Ok(response)
        },
    );
    
    // 创建客户端
    let client = RequestResponseClient::new(
        request_tx,
        Duration::from_secs(5),
    );
    
    // 启动服务器
    tokio::spawn(async move {
        server.run().await;
    });
    
    // 发送请求
    let response = client.send_request("你好，服务器".to_string()).await?;
    println!("收到响应: {}", response);
    
    Ok(())
}
```

### 1.2 发布-订阅模式

**核心概念**: 发布者产生消息，订阅者接收其感兴趣的消息，发布者和订阅者之间完全解耦。

**优点**:

- 松耦合：发布者不需要知道有哪些订阅者
- 灵活的消息传递模式
- 易于扩展：可以添加更多订阅者而不影响现有组件

**缺点**:

- 一致性保证较弱
- 可能出现消息丢失
- 难以处理订阅者失败的情况

#### 使用多线程和泛型机制实现

```rust
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use std::thread;
use std::sync::mpsc::{self, Sender, Receiver};
use std::marker::PhantomData;
use uuid::Uuid;
use thiserror::Error;

// 错误定义
#[derive(Debug, Error)]
pub enum PubSubError {
    #[error("发送消息失败")]
    SendError,
    #[error("主题未找到")]
    TopicNotFound,
    #[error("订阅者已存在")]
    SubscriberExists,
}

// 发布-订阅代理
pub struct PubSubBroker<T: Clone + Send + 'static> {
    topics: Arc<RwLock<HashMap<String, Vec<Sender<T>>>>>,
    _marker: PhantomData<T>,
}

// 订阅者
pub struct Subscriber<T: Clone + Send + 'static> {
    id: String,
    receiver: Receiver<T>,
    broker: Arc<PubSubBroker<T>>,
    topics: Vec<String>,
}

impl<T: Clone + Send + 'static> PubSubBroker<T> {
    pub fn new() -> Arc<Self> {
        Arc::new(Self {
            topics: Arc::new(RwLock::new(HashMap::new())),
            _marker: PhantomData,
        })
    }

    // 发布消息到主题
    pub fn publish(&self, topic: &str, message: T) -> Result<(), PubSubError> {
        let topics = self.topics.read().unwrap();
        
        if let Some(subscribers) = topics.get(topic) {
            for subscriber in subscribers {
                if subscriber.send(message.clone()).is_err() {
                    return Err(PubSubError::SendError);
                }
            }
            Ok(())
        } else {
            Err(PubSubError::TopicNotFound)
        }
    }

    // 创建新订阅者
    pub fn subscribe(&self, topics: Vec<String>) -> Result<Subscriber<T>, PubSubError> {
        let (tx, rx) = mpsc::channel();
        let subscriber_id = Uuid::new_v4().to_string();
        
        // 为每个主题添加订阅者
        {
            let mut topics_map = self.topics.write().unwrap();
            
            for topic in &topics {
                let entry = topics_map.entry(topic.clone()).or_insert_with(Vec::new);
                entry.push(tx.clone());
            }
        }
        
        Ok(Subscriber {
            id: subscriber_id,
            receiver: rx,
            broker: Arc::new(self.clone()),
            topics: topics.clone(),
        })
    }

    // 取消订阅
    fn unsubscribe(&self, subscriber_id: &str, topics: &[String]) {
        let mut topics_map = self.topics.write().unwrap();
        
        for topic in topics {
            if let Some(subscribers) = topics_map.get_mut(topic) {
                // 从该主题移除订阅者
                // 在实际系统中，我们会存储订阅者ID和发送者的映射
                // 这里简化了实现
            }
        }
    }
}

impl<T: Clone + Send + 'static> Clone for PubSubBroker<T> {
    fn clone(&self) -> Self {
        Self {
            topics: self.topics.clone(),
            _marker: PhantomData,
        }
    }
}

impl<T: Clone + Send + 'static> Subscriber<T> {
    // 接收消息
    pub fn receive(&self) -> Option<T> {
        self.receiver.recv().ok()
    }

    // 启动订阅者处理线程
    pub fn start_processing<F>(&self, handler: F)
    where
        F: Fn(T) + Send + 'static
    {
        let receiver = self.receiver.clone();
        let id = self.id.clone();
        
        thread::spawn(move || {
            println!("订阅者 {} 开始处理消息", id);
            
            while let Ok(message) = receiver.recv() {
                handler(message);
            }
            
            println!("订阅者 {} 停止处理消息", id);
        });
    }

    // 取消所有订阅
    pub fn unsubscribe(self) {
        self.broker.unsubscribe(&self.id, &self.topics);
    }
}

impl<T: Clone + Send + 'static> Drop for Subscriber<T> {
    fn drop(&mut self) {
        // 确保释放时取消订阅
        self.broker.unsubscribe(&self.id, &self.topics);
    }
}

// 使用示例
fn main() {
    // 创建代理
    let broker = PubSubBroker::<String>::new();
    
    // 创建订阅者
    let subscriber1 = broker.subscribe(vec![
        "news".to_string(), 
        "sports".to_string()
    ]).unwrap();
    
    let subscriber2 = broker.subscribe(vec![
        "news".to_string(), 
        "tech".to_string()
    ]).unwrap();
    
    // 启动订阅者处理线程
    subscriber1.start_processing(|msg| {
        println!("订阅者1收到: {}", msg);
    });
    
    subscriber2.start_processing(|msg| {
        println!("订阅者2收到: {}", msg);
    });
    
    // 发布消息
    broker.publish("news", "重要新闻".to_string()).unwrap();
    broker.publish("sports", "体育更新".to_string()).unwrap();
    broker.publish("tech", "技术资讯".to_string()).unwrap();
    
    // 等待消息处理
    thread::sleep(std::time::Duration::from_secs(1));
}
```

### 1.3 事件溯源模式

**核心概念**: 将应用程序状态的变化捕获为事件序列，而不是仅保存当前状态。通过重放这些事件可以重建系统状态。

**优点**:

- 完整的审计跟踪
- 能够重建任意时间点的系统状态
- 事件可以被用于分析和派生新的视图

**缺点**:

- 增加了系统复杂性
- 可能带来性能开销
- 需要处理事件演化问题

#### 使用异步和泛型实现

```rust
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use std::fmt::Debug;
use std::marker::PhantomData;
use std::sync::Arc;
use tokio::sync::{RwLock, Mutex};
use uuid::Uuid;
use thiserror::Error;
use futures::stream::{self, StreamExt};

// 事件接口
#[async_trait]
pub trait Event: Clone + Send + Sync + 'static {
    type AggregateId: Clone + Send + Sync + Eq + std::hash::Hash + 'static;
    type Aggregate: Aggregate<Id = Self::AggregateId>;
    
    fn event_id(&self) -> Uuid;
    fn aggregate_id(&self) -> Self::AggregateId;
    fn event_type(&self) -> String;
    fn timestamp(&self) -> DateTime<Utc>;
    
    // 应用事件到聚合体
    async fn apply(&self, aggregate: &mut Self::Aggregate) -> Result<(), EventError>;
}

// 聚合接口
pub trait Aggregate: Clone + Send + Sync + 'static {
    type Id: Clone + Send + Sync + Eq + std::hash::Hash + 'static;
    
    fn id(&self) -> Self::Id;
    fn version(&self) -> u64;
    fn set_version(&mut self, version: u64);
    
    // 创建一个新的初始状态聚合
    fn new(id: Self::Id) -> Self;
}

// 事件存储接口
#[async_trait]
pub trait EventStore<E: Event>: Send + Sync {
    // 保存事件
    async fn save_events(&self, events: Vec<E>) -> Result<(), EventError>;
    
    // 获取特定聚合的所有事件
    async fn get_events_for_aggregate(
        &self, 
        aggregate_id: &E::AggregateId
    ) -> Result<Vec<E>, EventError>;
    
    // 获取所有事件流
    async fn get_all_events(&self) -> Result<Vec<E>, EventError>;
}

// 仓库接口
#[async_trait]
pub trait AggregateRepository<A: Aggregate>: Send + Sync {
    // 通过ID获取聚合
    async fn get_by_id(&self, id: &A::Id) -> Result<Option<A>, EventError>;
    
    // 保存聚合（保存产生的事件）
    async fn save(&self, aggregate: &A, events: Vec<Box<dyn Event<AggregateId=A::Id, Aggregate=A>>>) 
        -> Result<(), EventError>;
}

// 事件错误
#[derive(Debug, Error)]
pub enum EventError {
    #[error("事件存储错误: {0}")]
    EventStoreError(String),
    
    #[error("聚合错误: {0}")]
    AggregateError(String),
    
    #[error("版本冲突: 预期 {expected}, 实际 {actual}")]
    VersionConflict {
        expected: u64,
        actual: u64,
    },
    
    #[error("其他错误: {0}")]
    Other(String),
}

// 内存事件存储实现
pub struct InMemoryEventStore<E: Event> {
    events: Arc<RwLock<Vec<E>>>,
}

impl<E: Event> InMemoryEventStore<E> {
    pub fn new() -> Self {
        Self {
            events: Arc::new(RwLock::new(Vec::new())),
        }
    }
}

#[async_trait]
impl<E: Event> EventStore<E> for InMemoryEventStore<E> {
    async fn save_events(&self, events: Vec<E>) -> Result<(), EventError> {
        let mut store = self.events.write().await;
        store.extend(events);
        Ok(())
    }
    
    async fn get_events_for_aggregate(&self, aggregate_id: &E::AggregateId) 
        -> Result<Vec<E>, EventError> {
        let store = self.events.read().await;
        let filtered_events = store
            .iter()
            .filter(|event| &event.aggregate_id() == aggregate_id)
            .cloned()
            .collect();
        Ok(filtered_events)
    }
    
    async fn get_all_events(&self) -> Result<Vec<E>, EventError> {
        let store = self.events.read().await;
        Ok(store.clone())
    }
}

// 事件源仓库实现
pub struct EventSourcedRepository<A, E, S>
where
    A: Aggregate,
    E: Event<Aggregate=A, AggregateId=A::Id>,
    S: EventStore<E>,
{
    event_store: Arc<S>,
    aggregates_cache: Arc<Mutex<HashMap<A::Id, A>>>,
    _marker: PhantomData<E>,
}

impl<A, E, S> EventSourcedRepository<A, E, S>
where
    A: Aggregate,
    E: Event<Aggregate=A, AggregateId=A::Id>,
    S: EventStore<E>,
{
    pub fn new(event_store: Arc<S>) -> Self {
        Self {
            event_store,
            aggregates_cache: Arc::new(Mutex::new(HashMap::new())),
            _marker: PhantomData,
        }
    }
    
    // 重建聚合状态
    async fn rebuild_aggregate(&self, id: &A::Id) -> Result<A, EventError> {
        let events = self.event_store.get_events_for_aggregate(id).await?;
        
        if events.is_empty() {
            return Ok(A::new(id.clone()));
        }
        
        let mut aggregate = A::new(id.clone());
        for event in events {
            event.apply(&mut aggregate).await?;
            aggregate.set_version(aggregate.version() + 1);
        }
        
        Ok(aggregate)
    }
}

#[async_trait]
impl<A, E, S> AggregateRepository<A> for EventSourcedRepository<A, E, S>
where
    A: Aggregate,
    E: Event<Aggregate=A, AggregateId=A::Id>,
    S: EventStore<E>,
{
    async fn get_by_id(&self, id: &A::Id) -> Result<Option<A>, EventError> {
        // 检查缓存
        {
            let cache = self.aggregates_cache.lock().await;
            if let Some(aggregate) = cache.get(id) {
                return Ok(Some(aggregate.clone()));
            }
        }
        
        // 重建聚合
        let aggregate = self.rebuild_aggregate(id).await?;
        
        // 更新缓存
        {
            let mut cache = self.aggregates_cache.lock().await;
            cache.insert(id.clone(), aggregate.clone());
        }
        
        Ok(Some(aggregate))
    }
    
    async fn save(&self, aggregate: &A, events: Vec<Box<dyn Event<AggregateId=A::Id, Aggregate=A>>>) 
        -> Result<(), EventError> {
        if events.is_empty() {
            return Ok(());
        }
        
        // 转换泛型事件（这部分在实际使用中需要优化，这里简化了）
        let events_to_save: Vec<E> = Vec::new(); // 实际中需要转换事件
        
        // 保存事件
        self.event_store.save_events(events_to_save).await?;
        
        // 更新缓存
        {
            let mut rebuilt_aggregate = aggregate.clone();
            for event in &events {
                event.apply(&mut rebuilt_aggregate).await?;
                rebuilt_aggregate.set_version(rebuilt_aggregate.version() + 1);
            }
            
            let mut cache = self.aggregates_cache.lock().await;
            cache.insert(aggregate.id(), rebuilt_aggregate);
        }
        
        Ok(())
    }
}

// 示例：用户聚合和事件
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    id: String,
    name: String,
    email: String,
    version: u64,
}

impl Aggregate for User {
    type Id = String;
    
    fn id(&self) -> Self::Id {
        self.id.clone()
    }
    
    fn version(&self) -> u64 {
        self.version
    }
    
    fn set_version(&mut self, version: u64) {
        self.version = version;
    }
    
    fn new(id: Self::Id) -> Self {
        Self {
            id,
            name: String::new(),
            email: String::new(),
            version: 0,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum UserEvent {
    UserCreated {
        event_id: Uuid,
        user_id: String,
        name: String,
        email: String,
        timestamp: DateTime<Utc>,
    },
    UserNameChanged {
        event_id: Uuid,
        user_id: String,
        new_name: String,
        timestamp: DateTime<Utc>,
    },
    UserEmailChanged {
        event_id: Uuid,
        user_id: String,
        new_email: String,
        timestamp: DateTime<Utc>,
    },
}

#[async_trait]
impl Event for UserEvent {
    type AggregateId = String;
    type Aggregate = User;
    
    fn event_id(&self) -> Uuid {
        match self {
            UserEvent::UserCreated { event_id, .. } => *event_id,
            UserEvent::UserNameChanged { event_id, .. } => *event_id,
            UserEvent::UserEmailChanged { event_id, .. } => *event_id,
        }
    }
    
    fn aggregate_id(&self) -> Self::AggregateId {
        match self {
            UserEvent::UserCreated { user_id, .. } => user_id.clone(),
            UserEvent::UserNameChanged { user_id, .. } => user_id.clone(),
            UserEvent::UserEmailChanged { user_id, .. } => user_id.clone(),
        }
    }
    
    fn event_type(&self) -> String {
        match self {
            UserEvent::UserCreated { .. } => "UserCreated".to_string(),
            UserEvent::UserNameChanged { .. } => "UserNameChanged".to_string(),
            UserEvent::UserEmailChanged { .. } => "UserEmailChanged".to_string(),
        }
    }
    
    fn timestamp(&self) -> DateTime<Utc> {
        match self {
            UserEvent::UserCreated { timestamp, .. } => *timestamp,
            UserEvent::UserNameChanged { timestamp, .. } => *timestamp,
            UserEvent::UserEmailChanged { timestamp, .. } => *timestamp,
        }
    }
    
    async fn apply(&self, aggregate: &mut Self::Aggregate) -> Result<(), EventError> {
        match self {
            UserEvent::UserCreated { name, email, .. } => {
                aggregate.name = name.clone();
                aggregate.email = email.clone();
            },
            UserEvent::UserNameChanged { new_name, .. } => {
                aggregate.name = new_name.clone();
            },
            UserEvent::UserEmailChanged { new_email, .. } => {
                aggregate.email = new_email.clone();
            },
        }
        
        Ok(())
    }
}

// 使用示例
#[tokio::main]
async fn main() -> Result<(), EventError> {
    // 创建事件存储
    let event_store = Arc::new(InMemoryEventStore::<UserEvent>::new());
    
    // 创建仓库
    let repository = EventSourcedRepository::<User, UserEvent, _>::new(event_store.clone());
    
    // 创建用户
    let user_id = "user-123".to_string();
    let created_event = UserEvent::UserCreated {
        event_id: Uuid::new_v4(),
        user_id: user_id.clone(),
        name: "张三".to_string(),
        email: "zhangsan@example.com".to_string(),
        timestamp: Utc::now(),
    };
    
    // 保存创建事件
    event_store.save_events(vec![created_event]).await?;
    
    // 获取用户
    let user = repository.get_by_id(&user_id).await?.unwrap();
    println!("用户: {:?}", user);
    
    // 修改用户名
    let name_changed_event = UserEvent::UserNameChanged {
        event_id: Uuid::new_v4(),
        user_id: user_id.clone(),
        new_name: "张三丰".to_string(),
        timestamp: Utc::now(),
    };
    
    // 保存修改事件
    event_store.save_events(vec![name_changed_event]).await?;
    
    // 再次获取用户
    let updated_user = repository.get_by_id(&user_id).await?.unwrap();
    println!("更新后的用户: {:?}", updated_user);
    
    Ok(())
}
```

## 2. 一致性与复制模式

### 2.1 分布式共识 (Raft)

**核心概念**: Raft是一种分布式共识算法，用于确保分布式系统中的状态机复制，使得多个节点对数据的操作顺序达成一致。

**优点**:

- 比Paxos更易于理解和实现
- 提供强一致性保证
- 容错和自愈能力

**缺点**:

- 性能开销较大
- 需要多数节点可用
- 实现复杂度仍然较高

#### 使用异步和多线程实现简化版Raft

```rust
use std::collections::{HashMap, HashSet};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use rand::Rng;
use tokio::sync::{mpsc, RwLock};
use tokio::time::{sleep, timeout};
use thiserror::Error;
use serde::{Serialize, Deserialize};
use tracing::{info, warn, error, debug};

// RPC消息
#[derive(Debug, Clone, Serialize, Deserialize)]
enum RaftRpc {
    // 候选人请求投票
    RequestVote {
        term: u64,               // 候选人的任期
        candidate_id: String,    // 候选人ID
        last_log_index: u64,     // 候选人的最后日志条目索引
        last_log_term: u64,      // 候选人最后日志条目的任期
    },
    
    // 投票响应
    RequestVoteResponse {
        term: u64,               // 当前任期，候选人可以更新自己
        vote_granted: bool,      // true表示候选人获得了选票
    },
    
    // 领导者的心跳消息和日志复制
    AppendEntries {
        term: u64,               // 领导者的任期
        leader_id: String,       // 领导者ID
        prev_log_index: u64,     // 紧邻新日志条目之前的日志索引
        prev_log_term: u64,      // prev_log_index处的日志条目的任期
        entries: Vec<LogEntry>,  // 需要存储的日志条目（心跳消息为空）
        leader_commit: u64,      // 领导者的提交索引
    },
    
    // 附加日志条目的响应
    AppendEntriesResponse {
        term: u64,               // 当前任期，领导者可以更新自己
        success: bool,           // 如果follower包含匹配prev_log_index和prev_log_term的条目，返回true
        match_index: u64,        // 跟随者匹配的最高日志索引
    },
}

// 日志条目
#[derive(Debug, Clone, Serialize, Deserialize)]
struct LogEntry {
    term: u64,                   // 日志条目写入时的任期
    index: u64,                  // 日志条目索引
    command: Vec<u8>,            // 命令数据
}

// 节点状态
#[derive(Debug, PartialEq, Clone, Copy)]
enum NodeState {
    Follower,
    Candidate,
    Leader,
}

// Raft错误
#[derive(Debug, Error)]
enum RaftError {
    #[error("RPC通信失败: {0}")]
    RpcError(String),
    
    #[error("日志复制失败: {0}")]
    LogReplicationError(String),
    
    #[error("选举超时: {0}")]
    ElectionTimeoutError(String),
    
    #[error("状态机应用失败: {0}")]
    StateMachineError(String),
}

// Raft节点
struct RaftNode {
    // 节点标识
    id: String,
    
    // 状态机状态
    state: NodeState,
    
    // 持久化状态
    current_term: u64,             // 当前任期
    voted_for: Option<String>,     // 本任期内投票给谁
    log: Vec<LogEntry>,            // 日志条目
    
    // 易失状态
    commit_index: u64,             // 已知已提交的最高日志条目索引
    last_applied: u64,             // 已应用到状态机的最高日志条目索引
    
    // 领导者状态（领导者选举后重新初始化）
    next_index: HashMap<String, u64>,  // 对于每台服务器，发送到该服务器的下一个日志条目的索引
    match_index: HashMap<String, u64>, // 对于每台服务器，已知在服务器上复制的最高日志条目索引
    
    // 集群配置
    peers: HashSet<String>,         // 所有节点ID
    
    // 内部计时器
    election_timeout: Duration,
    last_heartbeat: Instant,
    
    // 通信通道
    rpc_tx: HashMap<String, mpsc::Sender<RaftRpc>>,   // 向其他节点发送RPC的通道
    rpc_rx: mpsc::Receiver<RaftRpc>,                  // 接收来自其他节点的RPC
    
    // 客户端命令通道
    cmd_tx: mpsc::Sender<(Vec<u8>, mpsc::Sender<Result<(), RaftError>>)>,
    cmd_rx: mpsc::Receiver<(Vec<u8>, mpsc::Sender<Result<(), RaftError>>)>,
    
    // 状态机应用通道
    apply_tx: mpsc::Sender<LogEntry>,
}

/*
我需要继续编写 RaftNode 实现以及剩余的分布式设计模式。
接下来应该完成 Raft 实现的其余部分，然后继续编写 CRDT、熔断器等其他模式。
让我确保代码质量保持一致，并提供每种模式的详细说明和实现。
*/

impl RaftNode {
    // 创建新的Raft节点
    pub fn new(
        id: String,
        peers: HashSet<String>,
        rpc_rx: mpsc::Receiver<RaftRpc>,
        rpc_tx: HashMap<String, mpsc::Sender<RaftRpc>>,
        apply_tx: mpsc::Sender<LogEntry>,
    ) -> (Self, mpsc::Sender<(Vec<u8>, mpsc::Sender<Result<(), RaftError>>)>) {
        // 创建命令通道
        let (cmd_tx, cmd_rx) = mpsc::channel(100);
        
        // 随机选择选举超时时间(150ms-300ms)
        let mut rng = rand::thread_rng();
        let election_timeout = Duration::from_millis(rng.gen_range(150..300));
        
        let node = Self {
            id,
            state: NodeState::Follower,
            current_term: 0,
            voted_for: None,
            log: Vec::new(),
            commit_index: 0,
            last_applied: 0,
            next_index: HashMap::new(),
            match_index: HashMap::new(),
            peers,
            election_timeout,
            last_heartbeat: Instant::now(),
            rpc_tx,
            rpc_rx,
            cmd_tx: cmd_tx.clone(),
            cmd_rx,
            apply_tx,
        };
        
        (node, cmd_tx)
    }
    
    // 启动Raft节点
    pub async fn run(mut self) {
        info!("节点 {} 启动，状态为 {:?}", self.id, self.state);
        
        // 初始化为follower
        self.become_follower(self.current_term);
        
        // 主事件循环
        loop {
            match self.state {
                NodeState::Follower => self.run_follower().await,
                NodeState::Candidate => self.run_candidate().await,
                NodeState::Leader => self.run_leader().await,
            }
        }
    }
    
    // Follower状态下的行为
    async fn run_follower(&mut self) {
        debug!("节点 {} 以Follower身份运行，任期 {}", self.id, self.current_term);
        
        // 检查是否超时（没有收到领导者的心跳）
        let mut election_timer = tokio::time::interval(Duration::from_millis(10));
        
        loop {
            tokio::select! {
                _ = election_timer.tick() => {
                    // 检查是否超过选举超时
                    if self.last_heartbeat.elapsed() > self.election_timeout {
                        info!("节点 {} 选举超时，成为候选人", self.id);
                        self.become_candidate();
                        return;
                    }
                }
                
                Some(rpc) = self.rpc_rx.recv() => {
                    self.handle_rpc(rpc).await;
                }
                
                Some((cmd, resp_tx)) = self.cmd_rx.recv() => {
                    // Follower将命令转发给Leader
                    warn!("Follower收到客户端命令，但不处理。应转发给Leader");
                    let _ = resp_tx.send(Err(RaftError::RpcError("不是Leader".to_string()))).await;
                }
            }
        }
    }
    
    // Candidate状态下的行为
    async fn run_candidate(&mut self) {
        // 增加当前任期
        self.current_term += 1;
        let term = self.current_term;
        
        // 投票给自己
        self.voted_for = Some(self.id.clone());
        
        // 计票
        let mut votes = 1; // 包括自己的票
        let needed_votes = (self.peers.len() + 1) / 2 + 1; // 多数票
        
        info!("节点 {} 开始选举，任期 {}", self.id, self.current_term);
        
        // 发送RequestVote RPC给所有其他服务器
        let last_log_index = self.log.last().map_or(0, |e| e.index);
        let last_log_term = self.log.last().map_or(0, |e| e.term);
        
        for peer in &self.peers {
            if peer == &self.id {
                continue; // 跳过自己
            }
            
            if let Some(tx) = self.rpc_tx.get(peer) {
                let rpc = RaftRpc::RequestVote {
                    term: self.current_term,
                    candidate_id: self.id.clone(),
                    last_log_index,
                    last_log_term,
                };
                
                if let Err(e) = tx.send(rpc).await {
                    error!("发送RequestVote RPC给节点 {} 失败: {}", peer, e);
                }
            }
        }
        
        // 设置选举超时
        let mut rng = rand::thread_rng();
        let timeout_duration = Duration::from_millis(rng.gen_range(150..300));
        let election_deadline = Instant::now() + timeout_duration;
        
        // 等待投票结果或超时
        while Instant::now() < election_deadline && votes < needed_votes {
            match timeout(
                Duration::from_millis(10),
                self.rpc_rx.recv()
            ).await {
                Ok(Some(rpc)) => {
                    match rpc {
                        RaftRpc::RequestVoteResponse { term, vote_granted } => {
                            // 如果发现更高的任期，成为follower
                            if term > self.current_term {
                                info!("发现更高任期 {}, 成为follower", term);
                                self.become_follower(term);
                                return;
                            }
                            
                            // 如果获得投票且仍在同一任期
                            if vote_granted && term == self.current_term {
                                votes += 1;
                                debug!("获得来自节点的投票, 现在有 {} 票", votes);
                                
                                // 如果获得多数票，成为leader
                                if votes >= needed_votes {
                                    info!("获得多数票 ({}), 成为leader", votes);
                                    self.become_leader();
                                    return;
                                }
                            }
                        }
                        
                        RaftRpc::AppendEntries { term, .. } => {
                            // 如果收到来自新leader的AppendEntries，并且term >= 当前任期
                            if term >= self.current_term {
                                info!("收到来自任期 {} 的leader的AppendEntries, 成为follower", term);
                                self.become_follower(term);
                                self.last_heartbeat = Instant::now(); // 重置选举计时器
                                return;
                            }
                        }
                        
                        // 处理其他类型的RPC
                        _ => self.handle_rpc(rpc).await,
                    }
                }
                
                // 超时或通道关闭
                _ => continue,
            }
        }
        
        // 如果选举超时，重新开始选举
        if votes < needed_votes {
            info!("选举超时，未获得足够选票，重新开始选举");
            self.run_candidate().await;
        }
    }
    
    // Leader状态下的行为
    async fn run_leader(&mut self) {
        info!("节点 {} 成为Leader，任期 {}", self.id, self.current_term);
        
        // 初始化Leader状态
        for peer in &self.peers {
            if peer != &self.id {
                let next_log_index = self.log.last().map_or(1, |e| e.index + 1);
                self.next_index.insert(peer.clone(), next_log_index);
                self.match_index.insert(peer.clone(), 0);
            }
        }
        
        // 创建心跳计时器
        let mut heartbeat_timer = tokio::time::interval(Duration::from_millis(50)); // 50ms心跳
        
        loop {
            tokio::select! {
                // 发送心跳或日志复制
                _ = heartbeat_timer.tick() => {
                    self.send_append_entries().await;
                }
                
                // 处理RPC
                Some(rpc) = self.rpc_rx.recv() => {
                    if !self.handle_rpc(rpc).await {
                        // 如果不再是leader
                        return;
                    }
                }
                
                // 处理客户端命令
                Some((cmd, resp_tx)) = self.cmd_rx.recv() => {
                    self.handle_client_command(cmd, resp_tx).await;
                }
            }
        }
    }
    
    // 处理RPC消息
    async fn handle_rpc(&mut self, rpc: RaftRpc) -> bool {
        match rpc {
            RaftRpc::RequestVote { term, candidate_id, last_log_index, last_log_term } => {
                // 如果term < currentTerm，拒绝投票
                if term < self.current_term {
                    if let Some(tx) = self.rpc_tx.get(&candidate_id) {
                        let response = RaftRpc::RequestVoteResponse {
                            term: self.current_term,
                            vote_granted: false,
                        };
                        let _ = tx.send(response).await;
                    }
                    return true;
                }
                
                // 如果term > currentTerm，更新term并成为follower
                if term > self.current_term {
                    self.become_follower(term);
                }
                
                // 决定是否投票
                let vote_granted = match &self.voted_for {
                    // 尚未投票，或已经投给请求者
                    None | Some(id) if id == &candidate_id => {
                        // 检查候选人日志是否至少与接收者一样新
                        let last_log = self.log.last();
                        let last_self_term = last_log.map_or(0, |e| e.term);
                        let last_self_index = last_log.map_or(0, |e| e.index);
                        
                        last_log_term > last_self_term || 
                            (last_log_term == last_self_term && last_log_index >= last_self_index)
                    },
                    // 已经投票给其他人
                    _ => false,
                };
                
                if vote_granted {
                    // 投票给候选人
                    self.voted_for = Some(candidate_id.clone());
                    self.last_heartbeat = Instant::now(); // 重置选举计时器
                }
                
                // 发送响应
                if let Some(tx) = self.rpc_tx.get(&candidate_id) {
                    let response = RaftRpc::RequestVoteResponse {
                        term: self.current_term,
                        vote_granted,
                    };
                    let _ = tx.send(response).await;
                }
            }
            
            RaftRpc::AppendEntries { term, leader_id, prev_log_index, prev_log_term, entries, leader_commit } => {
                let mut success = false;
                let mut match_index = 0;
                
                // 如果term < currentTerm，拒绝请求
                if term < self.current_term {
                    if let Some(tx) = self.rpc_tx.get(&leader_id) {
                        let response = RaftRpc::AppendEntriesResponse {
                            term: self.current_term,
                            success: false,
                            match_index: 0,
                        };
                        let _ = tx.send(response).await;
                    }
                    return true;
                }
                
                // 如果term >= currentTerm，承认leader
                if term >= self.current_term {
                    self.become_follower(term);
                    self.last_heartbeat = Instant::now(); // 重置选举计时器
                    
                    // 查找prev_log_index和prev_log_term是否匹配
                    let log_ok = if prev_log_index == 0 {
                        // 特殊情况：空日志
                        true
                    } else {
                        self.log.get(prev_log_index as usize - 1)
                            .map_or(false, |e| e.term == prev_log_term)
                    };
                    
                    if log_ok {
                        success = true;
                        
                        // 处理日志条目
                        if !entries.is_empty() {
                            // 删除冲突的条目
                            if prev_log_index < self.log.len() as u64 {
                                self.log.truncate(prev_log_index as usize);
                            }
                            
                            // 追加新条目
                            for entry in &entries {
                                self.log.push(entry.clone());
                                match_index = entry.index;
                            }
                        } else {
                            // 心跳消息
                            match_index = prev_log_index;
                        }
                        
                        // 更新commitIndex
                        if leader_commit > self.commit_index {
                            self.commit_index = leader_commit.min(
                                self.log.last().map_or(0, |e| e.index)
                            );
                            self.apply_committed_entries().await;
                        }
                    }
                    
                    // 发送响应
                    if let Some(tx) = self.rpc_tx.get(&leader_id) {
                        let response = RaftRpc::AppendEntriesResponse {
                            term: self.current_term,
                            success,
                            match_index,
                        };
                        let _ = tx.send(response).await;
                    }
                }
            }
            
            RaftRpc::RequestVoteResponse { term, vote_granted: _ } => {
                // 如果收到更高任期，成为follower
                if term > self.current_term {
                    self.become_follower(term);
                    return false;
                }
            }
            
            RaftRpc::AppendEntriesResponse { term, success, match_index } => {
                // 如果收到更高任期，成为follower
                if term > self.current_term {
                    self.become_follower(term);
                    return false;
                }
                
                // 仅作为leader处理回复
                if self.state == NodeState::Leader && term == self.current_term {
                    // 从发送者地址找出是哪个peer
                    let peer_id = match self.rpc_tx.iter().find_map(|(id, _)| {
                        // 这里简化了实现方式，实际中需要更好地跟踪哪个响应来自哪个peer
                        if id != &self.id { Some(id.clone()) } else { None }
                    }) {
                        Some(id) => id,
                        None => return true,
                    };
                    
                    if success {
                        // 更新match_index和next_index
                        self.match_index.insert(peer_id.clone(), match_index);
                        self.next_index.insert(peer_id, match_index + 1);
                        
                        // 检查是否可以提交新的日志条目
                        self.update_commit_index();
                        self.apply_committed_entries().await;
                    } else {
                        // 如果复制失败，减少next_index并重试
                        if let Some(next_index) = self.next_index.get_mut(&peer_id) {
                            *next_index = (*next_index).saturating_sub(1);
                        }
                    }
                }
            }
        }
        
        true // 继续当前状态
    }
    
    // 处理客户端命令
    async fn handle_client_command(
        &mut self,
        cmd: Vec<u8>,
        resp_tx: mpsc::Sender<Result<(), RaftError>>,
    ) {
        // 只有Leader可以处理客户端命令
        if self.state != NodeState::Leader {
            let _ = resp_tx.send(Err(RaftError::RpcError("不是Leader".to_string()))).await;
            return;
        }
        
        // 创建新日志条目
        let index = self.log.last().map_or(1, |e| e.index + 1);
        let entry = LogEntry {
            term: self.current_term,
            index,
            command: cmd,
        };
        
        // 添加到自己的日志
        self.log.push(entry.clone());
        
        // 向所有peer发送AppendEntries
        self.send_append_entries().await;
        
        // 等待命令被提交
        // 注意：真实实现需要跟踪每个命令的提交状态
        tokio::spawn(async move {
            // 这里简化了实现，实际需要等待直到确认提交
            sleep(Duration::from_millis(100)).await;
            let _ = resp_tx.send(Ok(())).await;
        });
    }
    
    // 发送AppendEntries RPC
    async fn send_append_entries(&mut self) {
        for peer in &self.peers {
            if peer == &self.id {
                continue; // 跳过自己
            }
            
            if let Some(tx) = self.rpc_tx.get(peer) {
                let next_idx = *self.next_index.get(peer).unwrap_or(&1);
                
                // 确定prev_log_index和prev_log_term
                let prev_log_index = next_idx - 1;
                let prev_log_term = if prev_log_index == 0 {
                    0
                } else {
                    self.log.get(prev_log_index as usize - 1)
                        .map_or(0, |e| e.term)
                };
                
                // 获取要发送的日志条目
                let entries = self.log.iter()
                    .filter(|e| e.index >= next_idx)
                    .cloned()
                    .collect::<Vec<_>>();
                
                let rpc = RaftRpc::AppendEntries {
                    term: self.current_term,
                    leader_id: self.id.clone(),
                    prev_log_index,
                    prev_log_term,
                    entries,
                    leader_commit: self.commit_index,
                };
                
                if let Err(e) = tx.send(rpc).await {
                    error!("发送AppendEntries RPC给节点 {} 失败: {}", peer, e);
                }
            }
        }
    }
    
    // 更新commitIndex
    fn update_commit_index(&mut self) {
        // 找出已复制到大多数服务器的最高日志索引
        let mut sorted_indices: Vec<u64> = self.match_index.values()
            .cloned()
            .collect();
        sorted_indices.push(self.log.last().map_or(0, |e| e.index)); // 加入leader自己的索引
        sorted_indices.sort_unstable();
        
        // 多数派索引
        let majority_idx = sorted_indices[sorted_indices.len() / 2];
        
        // 如果多数派索引 > commitIndex，且该索引日志的任期等于当前任期，则更新commitIndex
        if majority_idx > self.commit_index {
            if let Some(entry) = self.log.get(majority_idx as usize - 1) {
                if entry.term == self.current_term {
                    self.commit_index = majority_idx;
                }
            }
        }
    }
    
    // 应用已提交的日志条目到状态机
    async fn apply_committed_entries(&mut self) {
        while self.last_applied < self.commit_index {
            self.last_applied += 1;
            
            if let Some(entry) = self.log.get(self.last_applied as usize - 1) {
                // 应用到状态机
                if let Err(e) = self.apply_tx.send(entry.clone()).await {
                    error!("应用日志条目到状态机失败: {}", e);
                    return;
                }
                
                debug!("应用日志条目 {} 到状态机", self.last_applied);
            }
        }
    }
    
    // 转换为Follower
    fn become_follower(&mut self, term: u64) {
        self.state = NodeState::Follower;
        self.current_term = term;
        self.voted_for = None;
        info!("节点 {} 成为Follower，任期 {}", self.id, self.current_term);
    }
    
    // 转换为Candidate
    fn become_candidate(&mut self) {
        self.state = NodeState::Candidate;
        info!("节点 {} 成为Candidate", self.id);
    }
    
    // 转换为Leader
    fn become_leader(&mut self) {
        self.state = NodeState::Leader;
        info!("节点 {} 成为Leader，任期 {}", self.id, self.current_term);
    }
}

// 使用示例
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 设置日志
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .init();
    
    // 创建一个3节点集群
    let node_ids = vec!["node1".to_string(), "node2".to_string(), "node3".to_string()];
    let mut nodes = HashMap::new();
    let mut rpc_channels = HashMap::new();
    let mut cmd_channels = HashMap::new();
    
    // 创建RPC通道
    for id in &node_ids {
        let (tx, rx) = mpsc::channel(100);
        rpc_channels.insert(id.clone(), (tx, rx));
    }
    
    // 创建节点
    for id in &node_ids {
        let peers: HashSet<String> = node_ids.iter()
            .filter(|&n| n != id)
            .cloned()
            .collect();
        
        let mut rpc_tx = HashMap::new();
        for peer in &peers {
            if let Some((tx, _)) = rpc_channels.get(peer) {
                rpc_tx.insert(peer.clone(), tx.clone());
            }
        }
        
        let (_, rpc_rx) = rpc_channels.get(id).unwrap();
        
        // 状态机应用通道
        let (apply_tx, _apply_rx) = mpsc::channel(100);
        
        // 创建Raft节点
        let (node, cmd_tx) = RaftNode::new(
            id.clone(),
            peers.clone(),
            rpc_rx.clone(),
            rpc_tx,
            apply_tx.clone(),
        );
        
        nodes.insert(id.clone(), node);
        cmd_channels.insert(id.clone(), cmd_tx);
    }
    
    // 启动节点
    for (id, node) in nodes {
        let id_clone = id.clone();
        tokio::spawn(async move {
            info!("启动节点 {}", id_clone);
            node.run().await;
        });
    }
    
    // 简单测试：向节点发送命令
    tokio::time::sleep(Duration::from_secs(2)).await; // 等待选举完成
    
    if let Some(cmd_tx) = cmd_channels.get("node1") {
        let (resp_tx, resp_rx) = mpsc::channel(1);
        cmd_tx.send(("test command".as_bytes().to_vec(), resp_tx)).await?;
        
        match resp_rx.await {
            Ok(Ok(())) => info!("命令成功提交"),
            Ok(Err(e)) => error!("命令提交失败: {:?}", e),
            Err(e) => error!("等待响应失败: {}", e),
        }
    }
    
    // 保持程序运行
    tokio::signal::ctrl_c().await?;
    info!("收到退出信号，关闭程序");
    
    Ok(())
}
```

### 2.2 CRDT (无冲突复制数据类型)

**核心概念**: CRDT是一类特殊的数据结构，允许多个副本在不需要严格协调的情况下并行修改，并能最终达成一致。

**优点**:

- 无需集中式协调
- 适合高延迟网络环境
- 自然支持离线操作
- 保证最终一致性

**缺点**:

- 元数据开销大
- 不是所有数据类型都可以转换为CRDT
- 删除操作复杂

#### 使用泛型和异步实现

```rust
use std::collections::{BTreeMap, HashSet};
use std::hash::Hash;
use std::sync::Arc;
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};
use uuid::Uuid;
use chrono::{DateTime, Utc};

// 逻辑时钟（向量时钟）
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct VectorClock<NodeId> 
where
    NodeId: Clone + Eq + Hash + Ord,
{
    counters: BTreeMap<NodeId, u64>,
}

impl<NodeId> VectorClock<NodeId> 
where
    NodeId: Clone + Eq + Hash + Ord,
{
    pub fn new() -> Self {
        Self {
            counters: BTreeMap::new(),
        }
    }
    
    // 增加某个节点的计数
    pub fn increment(&mut self, node_id: NodeId) {
        let counter = self.counters.entry(node_id).or_insert(0);
        *counter += 1;
    }
    
    // 合并两个向量时钟，取每个节点的最大值
    pub fn merge(&self, other: &Self) -> Self {
        let mut result = self.clone();
        
        for (node_id, &counter) in &other.counters {
            let entry = result.counters.entry(node_id.clone()).or_insert(0);
            *entry = (*entry).max(counter);
        }
        
        result
    }
    
    // 比较两个向量时钟：如果self的所有计数都小于或等于other，且至少有一个严格小于，则self发生在other之前
    pub fn happens_before(&self, other: &Self) -> bool {
        let mut strictly_smaller = false;
        
        for (node_id, &self_counter) in &self.counters {
            if let Some(&other_counter) = other.counters.get(node_id) {
                if self_counter > other_counter {
                    return false;
                }
                if self_counter < other_counter {
                    strictly_smaller = true;
                }
            }
        }
        
        // 检查other中的其他节点
        for (node_id, &other_counter) in &other.counters {
            if !self.counters.contains_key(node_id) && other_counter > 0 {
                strictly_smaller = true;
            }
        }
        
        strictly_smaller
    }
    
    // 检查是否并发（两个时钟都不发生在对方之前）
    pub fn concurrent_with(&self, other: &Self) -> bool {
        !self.happens_before(other) && !other.happens_before(self)
    }
}

// CRDT特性
pub trait CRDT: Clone + Send + Sync + 'static {
    type Value;
    
    // 查询当前值
    fn value(&self) -> Self::Value;
    
    // 合并与另一个CRDT
    fn merge(&mut self, other: &Self);
}

// Grow-Only Set (G-Set)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GSet<T> 
where
    T: Clone + Eq + Hash + Send + Sync + 'static,
{
    elements: HashSet<T>,
}

impl<T> GSet<T> 
where
    T: Clone + Eq + Hash + Send + Sync + 'static,
{
    pub fn new() -> Self {
        Self {
            elements: HashSet::new(),
        }
    }
    
    pub fn add(&mut self, element: T) {
        self.elements.insert(element);
    }
    
    pub fn contains(&self, element: &T) -> bool {
        self.elements.contains(element)
    }
}

impl<T> CRDT for GSet<T> 
where
    T: Clone + Eq + Hash + Send + Sync + 'static,
{
    type Value = HashSet<T>;
    
    fn value(&self) -> Self::Value {
        self.elements.clone()
    }
    
    fn merge(&mut self, other: &Self) {
        // 合并就是取并集
        for element in &other.elements {
            self.elements.insert(element.clone());
        }
    }
}

// 带移除操作的集合 (Two-Phase Set, 2P-Set)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TwoPSet<T> 
where
    T: Clone + Eq + Hash + Send + Sync + 'static,
{
    // 添加集合
    added: GSet<T>,
    // 移除集合
    removed: GSet<T>,
}

impl<T> TwoPSet<T> 
where
    T: Clone + Eq + Hash + Send + Sync + 'static,
{
    pub fn new() -> Self {
        Self {
            added: GSet::new(),
            removed: GSet::new(),
        }
    }
    
    pub fn add(&mut self, element: T) {
        self.added.add(element);
    }
    
    pub fn remove(&mut self, element: T) {
        // 只有在添加集合中的元素才能被移除
        if self.added.contains(&element) {
            self.removed.add(element);
        }
    }
    
    pub fn contains(&self, element: &T) -> bool {
        self.added.contains(element) && !self.removed.contains(element)
    }
}

impl<T> CRDT for TwoPSet<T> 
where
    T: Clone + Eq + Hash + Send + Sync + 'static,
{
    type Value = HashSet<T>;
    
    fn value(&self) -> Self::Value {
        // 当前值是添加集合减去移除集合
        let mut result = HashSet::new();
        
        for element in &self.added.value() {
            if !self.removed.contains(element) {
                result.insert(element.clone());
            }
        }
        
        result
    }
    
    fn merge(&mut self, other: &Self) {
        // 合并添加集合和移除集合
        self.added.merge(&other.added);
        self.removed.merge(&other.removed);
    }
}

// Last-Writer-Wins Register (LWW-Register)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LWWRegister<T> 
where
    T: Clone + Send + Sync + 'static,
{
    value: Option<T>,
    timestamp: DateTime<Utc>,
    node_id: String,
}

impl<T> LWWRegister<T> 
where
    T: Clone + Send + Sync + 'static,
{
    pub fn new() -> Self {
        Self {
            value: None,
            timestamp: Utc::now(),
            node_id: Uuid::new_v4().to_string(),
        }
    }
    
    pub fn set(&mut self, value: T) {
        self.value = Some(value);
        self.timestamp = Utc::now();
    }
    
    pub fn get(&self) -> Option<T> {
        self.value.clone()
    }
}

impl<T> CRDT for LWWRegister<T> 
where
    T: Clone + Send + Sync + 'static,
{
    type Value = Option<T>;
    
    fn value(&self) -> Self::Value {
        self.value.clone()
    }
    
    fn merge(&mut self, other: &Self) {
        // 比较时间戳，保留最新的值
        // 如果时间戳相同，按照节点ID字典序决定
        if other.timestamp > self.timestamp || 
           (other.timestamp == self.timestamp && other.node_id > self.node_id) {
            self.value = other.value.clone();
            self.timestamp = other.timestamp;
            self.node_id = other.node_id.clone();
        }
    }
}

// 分布式CRDT节点
pub struct CRDTNode<C: CRDT> {
    node_id: String,
    data: Arc<RwLock<C>>,
}

impl<C: CRDT> CRDTNode<C> {
    pub fn new(node_id: &str, initial_data: C) -> Self {
        Self {
            node_id: node_id.to_string(),
            data: Arc::new(RwLock::new(initial_data)),
        }
    }
    
    // 获取当前值
    pub async fn get_value(&self) -> C::Value {
        let data = self.data.read().await;
        data.value()
    }
    
    // 更新本地数据
    pub async fn update<F>(&self, update_fn: F) -> C::Value
    where
        F: FnOnce(&mut C),
    {
        let mut data = self.data.write().await;
        update_fn(&mut *data);
        data.value()
    }
    
    // 合并来自其他节点的数据
    pub async fn merge(&self, other_data: C) -> C::Value {
        let mut data = self.data.write().await;
        data.merge(&other_data);
        data.value()
    }
    
    // 获取当前数据副本（用于发送到其他节点）
    pub async fn get_data_for_sync(&self) -> C {
        let data = self.data.read().await;
        data.clone()
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建两个使用GSet的CRDT节点
    let node1 = CRDTNode::<GSet<String>>::new("node1", GSet::new());
    let node2 = CRDTNode::<GSet<String>>::new("node2", GSet::new());
    
    // 节点1添加元素
    node1.update(|gset| {
        gset.add("apple".to_string());
        gset.add("banana".to_string());
    }).await;
    
    // 节点2添加元素
    node2.update(|gset| {
        gset.add("banana".to_string());
        gset.add("cherry".to_string());
    }).await;
    
    // 模拟节点同步
    let node1_data = node1.get_data_for_sync().await;
    let node2_data = node2.get_data_for_sync().await;
    
    // 节点1合并节点2的数据
    node1.merge(node2_data).await;
    
    // 节点2合并节点1的数据
    node2.merge(node1_data).await;
    
    // 检查两个节点是否一致
    let node1_value = node1.get_value().await;
    let node2_value = node2.get_value().await;
    
    println!("节点1集合: {:?}", node1_value);
    println!("节点2集合: {:?}", node2_value);
    assert_eq!(node1_value, node2_value);
    
    // 测试LWW-Register
    let lww_node1 = CRDTNode::<LWWRegister<String>>::new("lww1", LWWRegister::new());
    let lww_node2 = CRDTNode::<LWWRegister<String>>::new("lww2", LWWRegister::new());
    
/*
让我继续完成CRDT示例代码，接着是LWW-Register的示例，然后再补充一些内容。
*/


    // 节点1设置值
    lww_node1.update(|reg| {
        reg.set("节点1的值".to_string(), "lww1".to_string());
    }).await;
    
    // 等待一小段时间，确保时间戳不同
    tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    
    // 节点2设置值
    lww_node2.update(|reg| {
        reg.set("节点2的值".to_string(), "lww2".to_string());
    }).await;
    
    // 模拟节点同步
    let lww_node1_data = lww_node1.get_data_for_sync().await;
    let lww_node2_data = lww_node2.get_data_for_sync().await;
    
    // 两个节点互相合并数据
    lww_node1.merge(lww_node2_data).await;
    lww_node2.merge(lww_node1_data).await;
    
    // 检查两个节点的值
    let lww_node1_value = lww_node1.get_value().await;
    let lww_node2_value = lww_node2.get_value().await;
    
    println!("LWW节点1值: {:?}", lww_node1_value);
    println!("LWW节点2值: {:?}", lww_node2_value);
    assert_eq!(lww_node1_value, lww_node2_value);
    assert_eq!(lww_node1_value, "节点2的值"); // 因为节点2的时间戳更晚
}
```

## 3. 容错模式

### 3.1 熔断器模式

**核心概念**: 熔断器监控特定操作的失败率，当失败率超过阈值时，熔断器"跳闸"，阻止后续请求，防止级联故障。

**优点**:

- 防止级联故障
- 为故障系统提供恢复时间
- 提高系统弹性

**缺点**:

- 需要正确配置，否则可能过早触发
- 可能导致过早拒绝服务
- 需要与重试策略协调使用

#### 使用异步实现

```rust
use std::sync::{atomic::{AtomicU64, AtomicUsize, Ordering}, Arc};
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use tokio::time::sleep;
use thiserror::Error;
use tracing::{debug, error, info, warn};
use futures::Future;

// 熔断器状态
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum CircuitState {
    Closed,    // 正常，允许请求通过
    Open,      // 熔断，拒绝请求
    HalfOpen,  // 半开，允许有限请求通过以探测恢复
}

// 熔断器配置
#[derive(Debug, Clone)]
pub struct CircuitBreakerConfig {
    // 失败阈值百分比
    pub failure_threshold_percentage: u8,
    // 熔断后保持熔断的时间
    pub reset_timeout: Duration,
    // 熔断器半开状态下允许的请求数
    pub half_open_allowed_requests: usize,
    // 熔断器关闭状态下的滑动窗口大小
    pub closed_window_size: usize,
}

impl Default for CircuitBreakerConfig {
    fn default() -> Self {
        Self {
            failure_threshold_percentage: 50,
            reset_timeout: Duration::from_secs(30),
            half_open_allowed_requests: 10,
            closed_window_size: 100,
        }
    }
}

// 熔断器错误
#[derive(Debug, Error)]
pub enum CircuitBreakerError<E> {
    #[error("熔断器开启，拒绝请求")]
    CircuitOpen,
    
    #[error("底层错误: {0}")]
    UnderlyingError(#[from] E),
}

// 熔断器
pub struct CircuitBreaker {
    // 熔断器名称
    name: String,
    // 当前状态
    state: RwLock<CircuitState>,
    // 上次状态更改的时间
    last_state_change: RwLock<Instant>,
    // 配置
    config: CircuitBreakerConfig,
    
    // 计数器
    // 滑动窗口中成功的请求数
    success_count: AtomicUsize,
    // 滑动窗口中失败的请求数
    failure_count: AtomicUsize,
    // 半开状态下已处理的请求数
    half_open_processed: AtomicUsize,
}

impl CircuitBreaker {
    pub fn new(name: &str, config: CircuitBreakerConfig) -> Arc<Self> {
        Arc::new(Self {
            name: name.to_string(),
            state: RwLock::new(CircuitState::Closed),
            last_state_change: RwLock::new(Instant::now()),
            config,
            success_count: AtomicUsize::new(0),
            failure_count: AtomicUsize::new(0),
            half_open_processed: AtomicUsize::new(0),
        })
    }
    
    // 获取当前状态
    pub async fn current_state(&self) -> CircuitState {
        *self.state.read().await
    }
    
    // 执行受熔断器保护的操作
    pub async fn execute<F, T, E>(&self, f: F) -> Result<T, CircuitBreakerError<E>>
...(about 178 lines omitted)...
// 执行并可能触发状态转换
pub async fn execute<F, Fut, T, E>(&self, f: F) -> Result<T, CircuitBreakerError<E>>
where
    F: FnOnce() -> Fut,
    Fut: Future<Output = Result<T, E>>,
    E: std::error::Error,
{
    let state = self.current_state().await;
    
    match state {
        CircuitState::Open => {
            // 检查是否应该尝试切换到半开
            let last_change = *self.last_state_change.read().await;
            if last_change.elapsed() >= self.config.reset_timeout {
                debug!("熔断器 {} 的重置超时已过，尝试切换到半开状态", self.name);
                self.transition_to_half_open().await;
                // 继续执行，因为我们已经转到半开状态
            } else {
                // 仍处于开路状态，立即拒绝请求
                return Err(CircuitBreakerError::CircuitOpen);
            }
        }
        CircuitState::HalfOpen => {
            // 检查是否已超过半开状态下允许的请求数
            let processed = self.half_open_processed.load(Ordering::SeqCst);
            if processed >= self.config.half_open_allowed_requests {
                return Err(CircuitBreakerError::CircuitOpen);
            }
            // 增加半开状态下已处理的请求计数
            self.half_open_processed.fetch_add(1, Ordering::SeqCst);
        }
        CircuitState::Closed => {
            // 正常处理
        }
    }
    
    // 执行函数
    let result = f().await;
    
    // 根据结果更新统计信息
    match &result {
        Ok(_) => {
            self.record_success().await;
        }
        Err(_) => {
            self.record_failure().await;
        }
    }
    
    // 将底层错误转换为CircuitBreakerError
    result.map_err(CircuitBreakerError::UnderlyingError)
}

// 使用示例
#[tokio::main]
async fn main() {
    // 配置熔断器
    let config = CircuitBreakerConfig {
        failure_threshold_percentage: 50,
        reset_timeout: Duration::from_secs(5),
        half_open_allowed_requests: 3,
        closed_window_size: 10,
    };
    
    // 创建熔断器
    let circuit = CircuitBreaker::new("test-service", config);
    
    // 模拟一个不稳定的服务
    let mut failure_mode = false;
    
    // 循环测试
    for i in 0..100 {
        println!("请求 #{}:", i);
        
        // 每20次请求切换一次故障模式
        if i > 0 && i % 20 == 0 {
            failure_mode = !failure_mode;
            println!("服务状态切换为: {}", if failure_mode { "不稳定" } else { "稳定" });
        }
        
        // 尝试执行受保护的操作
        let result = circuit.execute(|| async {
            // 模拟服务调用
            sleep(Duration::from_millis(50)).await;
            
            if failure_mode && rand::random::<f32>() < 0.7 {
                Err("服务错误".to_string())
            } else {
                Ok("成功响应".to_string())
            }
        }).await;
        
        // 打印结果和熔断器状态
        match result {
            Ok(msg) => println!("  成功: {}", msg),
            Err(CircuitBreakerError::CircuitOpen) => println!("  熔断器开启，请求被拒绝"),
            Err(CircuitBreakerError::UnderlyingError(e)) => println!("  错误: {}", e),
        }
        
        println!("  熔断器状态: {:?}", circuit.current_state().await);
        
        // 暂停一下
        sleep(Duration::from_millis(100)).await;
    }
}
```

### 3.2 舱壁模式

**核心概念**: 舱壁模式将系统分割成隔离的部分，使一个部分的故障不会影响其他部分，类似船舶的舱壁设计。

**优点**:

- 限制故障影响范围
- 提高系统弹性
- 提高资源利用率

**缺点**:

- 增加系统配置复杂性
- 可能导致资源浪费
- 需要正确规划和实现

#### 使用多线程实现

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::thread;
use std::sync::mpsc::{self, Sender, Receiver};
use std::time::{Duration, Instant};
use thiserror::Error;
use tracing::{info, error, warn};

// 舱壁错误
#[derive(Debug, Error)]
pub enum BulkheadError {
    #[error("资源已耗尽")]
    NoCapacity,
    
    #[error("执行超时")]
    Timeout,
    
    #[error("执行错误: {0}")]
    ExecutionError(String),
    
    #[error("舱壁已关闭")]
    BulkheadClosed,
}

// 舱壁配置
#[derive(Debug, Clone)]
pub struct BulkheadConfig {
    // 最大并发执行数
    pub max_concurrent_calls: usize,
    // 最大等待队列长度
    pub max_wait_queue_size: usize,
    // 执行超时
    pub execution_timeout: Duration,
}

impl Default for BulkheadConfig {
    fn default() -> Self {
        Self {
            max_concurrent_calls: 10,
            max_wait_queue_size: 100,
            execution_timeout: Duration::from_secs(30),
        }
    }
}

// 执行结果
type ExecutionResult<T> = Result<T, BulkheadError>;

// 舱壁实现
pub struct Bulkhead<T> {
    name: String,
    config: BulkheadConfig,
    
    // 当前活跃的执行数
    active_count: Arc<Mutex<usize>>,
    // 等待队列长度
    wait_queue_size: Arc<Mutex<usize>>,
    
    // 任务提交通道
    task_tx: Option<Sender<Box<dyn FnOnce() -> ExecutionResult<T> + Send>>>,
    
    // 是否已关闭
    is_closed: Arc<Mutex<bool>>,
}

impl<T: Send + 'static> Bulkhead<T> {
    pub fn new(name: &str, config: BulkheadConfig) -> Self {
        let active_count = Arc::new(Mutex::new(0));
        let wait_queue_size = Arc::new(Mutex::new(0));
        let is_closed = Arc::new(Mutex::new(false));
        
        // 创建任务通道
        let (task_tx, task_rx) = mpsc::channel();
        
        // 启动工作线程池
        Self::start_worker_pool(
            name.to_string(),
            config.clone(),
            task_rx,
            active_count.clone(),
            wait_queue_size.clone(),
            is_closed.clone(),
        );
        
        Self {
            name: name.to_string(),
            config,
            active_count,
            wait_queue_size,
            task_tx: Some(task_tx),
            is_closed,
        }
    }
    
    // 启动工作线程池
    fn start_worker_pool(
        name: String,
        config: BulkheadConfig,
        task_rx: Receiver<Box<dyn FnOnce() -> ExecutionResult<T> + Send>>,
...(about 61 lines omitted)...
    let (result_tx, result_rx) = mpsc::channel();
    
    // 创建任务闭包
    let task = Box::new(move || {
        // 捕获执行中的错误
        let start = Instant::now();
        let result = match std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| task_fn())) {
            Ok(result) => result,
            Err(_) => Err(BulkheadError::ExecutionError("任务执行发生恐慌".to_string())),
        };
        
        let elapsed = start.elapsed();
        info!("舱壁 {} 任务执行完成，耗时: {:?}", task_name, elapsed);
        
        // 发送结果
        if result_tx.send(result).is_err() {
            error!("无法发送结果，接收者可能已关闭");
        }
    });
    
    // 检查舱壁是否已关闭
    {
        let closed = *self.is_closed.lock().unwrap();
        if closed {
            return Err(BulkheadError::BulkheadClosed);
        }
    }
    
    // 检查等待队列是否已满
    {
        let mut wait_queue_size = self.wait_queue_size.lock().unwrap();
        if *wait_queue_size >= self.config.max_wait_queue_size {
            return Err(BulkheadError::NoCapacity);
        }
        *wait_queue_size += 1;
    }
    
    // 发送任务
    if let Some(tx) = &self.task_tx {
        if tx.send(task).is_err() {
            // 减少等待队列计数
            let mut wait_queue_size = self.wait_queue_size.lock().unwrap();
            *wait_queue_size -= 1;
            
            return Err(BulkheadError::BulkheadClosed);
        }
    } else {
        // 减少等待队列计数
        let mut wait_queue_size = self.wait_queue_size.lock().unwrap();
        *wait_queue_size -= 1;
        
        return Err(BulkheadError::BulkheadClosed);
    }
    
    // 等待结果，带超时
    let timeout = self.config.execution_timeout;
    match result_rx.recv_timeout(timeout) {
        Ok(result) => {
            // 返回任务执行结果
            result
        }
        Err(_) => {
            // 超时
            Err(BulkheadError::Timeout)
        }
    }
}

// 使用示例
fn main() {
    // 创建舱壁
    let db_bulkhead = Bulkhead::<String>::new("database", BulkheadConfig {
        max_concurrent_calls: 5,
        max_wait_queue_size: 10,
        execution_timeout: Duration::from_secs(2),
    });
    
    let api_bulkhead = Bulkhead::<String>::new("api-client", BulkheadConfig {
        max_concurrent_calls: 20,
        max_wait_queue_size: 50,
        execution_timeout: Duration::from_secs(5),
    });
    
    // 模拟数据库操作
    let db_op = |id: u32| {
        let result = db_bulkhead.execute(move || {
            println!("执行数据库操作 #{}", id);
            thread::sleep(Duration::from_millis(500));
            Ok(format!("DB结果 #{}", id))
        });
        
        match result {
            Ok(data) => println!("DB操作 #{} 成功: {}", id, data),
            Err(e) => println!("DB操作 #{} 失败: {}", id, e),
        }
    };
    
    // 模拟API调用
    let api_call = |id: u32| {
        let result = api_bulkhead.execute(move || {
            println!("执行API调用 #{}", id);
            
            // 模拟一些API调用偶尔超时
            if id % 7 == 0 {
                thread::sleep(Duration::from_secs(10));
            } else {
                thread::sleep(Duration::from_millis(300));
            }
            
            Ok(format!("API结果 #{}", id))
        });
        
        match result {
            Ok(data) => println!("API调用 #{} 成功: {}", id, data),
            Err(e) => println!("API调用 #{} 失败: {}", id, e),
        }
    };
    
    // 创建多个线程进行并发操作
    let mut handles = vec![];
    
    // 创建30个数据库操作
    for i in 0..30 {
        let handle = thread::spawn(move || {
            db_op(i);
        });
        handles.push(handle);
    }
    
    // 创建50个API调用
    for i in 0..50 {
        let handle = thread::spawn(move || {
            api_call(i);
        });
        handles.push(handle);
    }
    
    // 等待所有线程完成
    for handle in handles {
        let _ = handle.join();
    }
    
    println!("所有操作完成");
}
```

### 3.3 背压模式

**核心概念**: 当系统无法处理输入的速度时，向上游系统施加压力以减缓输入速度，防止系统过载。

**优点**:

- 防止系统资源耗尽
- 保持系统稳定性
- 提高处理质量

**缺点**:

- 可能导致上游系统阻塞
- 实现复杂度较高
- 需要跨系统协调

#### 使用异步和多线程实现

```rust
use std::sync::{Arc, atomic::{AtomicUsize, Ordering}};
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, Semaphore};
use tokio::time::{sleep, timeout};
use futures::stream::{self, StreamExt};
use thiserror::Error;
use tracing::{info, warn, error, debug};

// 背压策略
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum BackpressureStrategy {
    // 简单丢弃多余请求
    Drop,
    // 阻塞生产者直到有容量
    Block,
    // 限制接收速率
    Throttle,
}

// 背压错误
#[derive(Debug, Error)]
pub enum BackpressureError {
    #[error("缓冲区已满")]
    BufferFull,
    
    #[error("处理超时")]
    Timeout,
    
    #[error("通道已关闭")]
    ChannelClosed,
    
    #[error("处理错误: {0}")]
    ProcessingError(String),
}

// 背压配置
#[derive(Debug, Clone)]
pub struct BackpressureConfig {
    // 缓冲区大小
    pub buffer_size: usize,
    // 背压策略
    pub strategy: BackpressureStrategy,
    // 限流时每秒最大请求数
    pub throttle_rate: f64,
    // 处理超时
    pub processing_timeout: Duration,
}

impl Default for BackpressureConfig {
    fn default() -> Self {
        Self {
            buffer_size: 100,
            strategy: BackpressureStrategy::Block,
            throttle_rate: 10.0,
            processing_timeout: Duration::from_secs(30),
        }
    }
}

// 背压控制器
pub struct BackpressureController<T> {
    config: BackpressureConfig,
    buffer_count: Arc<AtomicUsize>,
    sender: mpsc::Sender<T>,
    receiver: Arc<tokio::sync::Mutex<mpsc::Receiver<T>>>,
    permits: Arc<Semaphore>,
    throttle_timestamp: Arc<tokio::sync::Mutex<Instant>>,
}

impl<T: Send + 'static> BackpressureController<T> {
    pub fn new(config: BackpressureConfig) -> Self {
        let (sender, receiver) = mpsc::channel(config.buffer_size);
        let buffer_count = Arc::new(AtomicUsize::new(0));
        
        // 创建一个信号量，用于限制并发处理数
        let permits = Arc::new(Semaphore::new(config.buffer_size));
        
        Self {
            config,
            buffer_count,
            sender,
            receiver: Arc::new(tokio::sync::Mutex::new(receiver)),
            permits,
...(about 156 lines omitted)...
                    }
                    
                    // 处理完成后释放信号量
                    drop(permit);
                    
                    // 减少缓冲区计数
                    self.buffer_count.fetch_sub(1, Ordering::SeqCst);
                    
                    // 返回处理结果
                    match result {
                        Ok(r) => Some(Ok(r)),
                        Err(e) => Some(Err(BackpressureError::ProcessingError(format!("{}", e)))),
                    }
                })
                .collect()
                .await;
            
            Ok(results)
        }
    }
}

// 使用示例
#[tokio::main]
async fn main() {
    // 创建背压控制器
    let controller = BackpressureController::new(BackpressureConfig {
        buffer_size: 20,
        strategy: BackpressureStrategy::Throttle,
        throttle_rate: 5.0,
        processing_timeout: Duration::from_secs(2),
    });
    
    // 启动消费者
    let processor = controller.clone();
    tokio::spawn(async move {
        // 处理函数
        let handler = |item: u32| async move {
            println!("处理项目: {}", item);
            
            // 模拟处理时间
            let process_time = if item % 10 == 0 {
                // 偶尔处理较慢
                3000
            } else {
                500
            };
            
            sleep(Duration::from_millis(process_time)).await;
            
            // 返回处理结果
            Ok(format!("处理结果: {}", item))
        };
        
        // 开始处理
        match processor.process_items(handler).await {
            Ok(results) => {
                for (i, result) in results.into_iter().enumerate() {
                    match result {
                        Ok(r) => println!("结果 #{}: {}", i, r),
                        Err(e) => println!("错误 #{}: {}", i, e),
                    }
                }
            }
            Err(e) => {
                println!("处理错误: {}", e);
            }
        }
    });
    
    // 模拟高速生产者
    for i in 0..100 {
        println!("发送项目: {}", i);
        
        // 尝试提交项目
        match controller.submit(i).await {
            Ok(()) => println!("项目 {} 已提交", i),
            Err(e) => println!("项目 {} 提交失败: {}", i, e),
        }
        
        // 模拟生产者间隔
        if i % 5 == 0 {
            sleep(Duration::from_millis(100)).await;
        } else {
            sleep(Duration::from_millis(10)).await;
        }
    }
    
    // 等待一段时间让处理完成
    sleep(Duration::from_secs(10)).await;
}
```

## 4. 分区与扩展模式

### 4.1 一致性哈希

**核心概念**: 一致性哈希是一种特殊的哈希算法，使得当哈希表大小变化时，只有少量键需要重新映射，适合动态集群环境。

**优点**:

- 集群节点变化时最小化数据迁移
- 良好的负载均衡特性
- 支持节点的动态添加和删除

**缺点**:

- 实现略复杂
- 节点权重不均可能导致负载不均
- 需要正确管理虚拟节点

#### 使用泛型和多线程实现

```rust
use std::collections::{BTreeMap, HashMap};
use std::hash::{Hash, Hasher};
use std::sync::{Arc, RwLock};
use std::hash::BuildHasher;
use std::marker::PhantomData;
use rand::Rng;
use std::default::Default;
use std::fmt::Debug;

// 可哈希的节点标识符
pub trait NodeId: Clone + Eq + Hash + Send + Sync + Debug + 'static {}
impl<T: Clone + Eq + Hash + Send + Sync + Debug + 'static> NodeId for T {}

// 可哈希的键
pub trait Key: Hash + Send + Sync + 'static {}
impl<T: Hash + Send + Sync + 'static> Key for T {}

// 一致性哈希环
pub struct ConsistentHash<K: Key, N: NodeId, H: BuildHasher = fnv::FnvBuildHasher> {
    // 哈希环 - 从哈希值映射到节点
    ring: RwLock<BTreeMap<u64, N>>,
    
    // 节点到其虚拟节点哈希值的映射
    node_to_vnode_hashes: RwLock<HashMap<N, Vec<u64>>>,
    
    // 每个节点的虚拟节点数
    virtual_nodes_per_node: usize,
    
    // 哈希构建器
    hasher_builder: H,
    
    // 类型参数标记
    _marker: PhantomData<K>,
}

impl<K: Key, N: NodeId, H: BuildHasher + Clone + Default + Send + Sync + 'static> ConsistentHash<K, N, H> {
    // 创建新的一致性哈希环
    pub fn new(virtual_nodes_per_node: usize) -> Self {
        Self {
            ring: RwLock::new(BTreeMap::new()),
            node_to_vnode_hashes: RwLock::new(HashMap::new()),
            virtual_nodes_per_node,
            hasher_builder: H::default(),
            _marker: PhantomData,
        }
    }
    
    // 计算哈希值
    fn hash<T: Hash>(&self, item: &T) -> u64 {
        let mut hasher = self.hasher_builder.build_hasher();
        item.hash(&mut hasher);
        hasher.finish()
    }
    
    // 添加节点到哈希环
    pub fn add_node(&self, node: N) {
        let mut ring = self.ring.write().unwrap();
        let mut node_hashes = self.node_to_vnode_hashes.write().unwrap();
        
        // 为节点创建虚拟节点
        let hashes = self.create_virtual_node_hashes(&node);
        
        // 添加虚拟节点到哈希环
        for &hash in &hashes {
            ring.insert(hash, node.clone());
        }
        
        // 保存节点的哈希值映射
        node_hashes.insert(node, hashes);
    }
    
    // 从哈希环移除节点
    pub fn remove_node(&self, node: &N) -> bool {
        let mut ring = self.ring.write().unwrap();
        let mut node_hashes = self.node_to_vnode_hashes.write().unwrap();
        
        // 获取节点的虚拟节点哈希值
        if let Some(hashes) = node_hashes.remove(node) {
            // 从哈希环移除所有虚拟节点
            for hash in hashes {
                ring.remove(&hash);
            }
            true
        } else {
            false
        }
    }
    
    // 为节点创建虚拟节点哈希值
    fn create_virtual_node_hashes(&self, node: &N) -> Vec<u64> {
...(about 57 lines omitted)...
// 使用示例
fn main() {
    let mut rng = rand::thread_rng();
    
    // 创建一致性哈希环，每个节点有100个虚拟节点
    let hash_ring = Arc::new(ConsistentHash::<String, String>::new(100));
    
    // 添加服务器节点
    let servers = vec![
        "server-1".to_string(),
        "server-2".to_string(),
        "server-3".to_string(),
        "server-4".to_string(),
    ];
    
    for server in &servers {
        hash_ring.add_node(server.clone());
    }
    
    // 创建一些测试键
    let keys: Vec<String> = (1..1000)
        .map(|i| format!("key-{}", i))
        .collect();
    
    // 测试键映射到哪个服务器
    let mut distribution: HashMap<String, usize> = HashMap::new();
    for key in &keys {
        let server = hash_ring.get_node(key).unwrap();
        *distribution.entry(server).or_insert(0) += 1;
    }
    
    // 打印分布情况
    println!("初始分布:");
    for (server, count) in &distribution {
        println!("  {} -> {} 个键", server, count);
    }
    
    // 模拟移除一个节点
    let remove_server = &servers[rng.gen_range(0..servers.len())];
    println!("\n移除节点 {}...", remove_server);
    hash_ring.remove_node(remove_server);
    
    // 测试键重新分布
    let mut new_distribution: HashMap<String, usize> = HashMap::new();
    let mut moved_keys = 0;
    
    for key in &keys {
        if let Some(server) = hash_ring.get_node(key) {
            *new_distribution.entry(server).or_insert(0) += 1;
            
            // 检查是否迁移
            let original_server = hash_ring.get_node_or_else(key, || remove_server.clone());
            if original_server != server {
                moved_keys += 1;
            }
        }
    }
    
    // 打印新的分布情况
    println!("\n新分布:");
    for (server, count) in &new_distribution {
        println!("  {} -> {} 个键", server, count);
    }
    
    // 打印迁移率
    println!("\n总共迁移了 {} 个键，迁移率: {:.2}%", 
        moved_keys, 
        (moved_keys as f64 / keys.len() as f64) * 100.0);
    
    // 添加新节点
    let new_server = "server-5".to_string();
    println!("\n添加新节点 {}...", new_server);
    hash_ring.add_node(new_server.clone());
    
    // 再次测试
    let mut final_distribution: HashMap<String, usize> = HashMap::new();
    let mut moved_keys = 0;
    
    for key in &keys {
        let server = hash_ring.get_node(key).unwrap();
        *final_distribution.entry(server).or_insert(0) += 1;
        
        // 检查是否迁移
        if let Some(prev_server) = new_distribution.get(&server) {
            if *prev_server != *final_distribution.get(&server).unwrap() {
                moved_keys += 1;
            }
        }
    }
    
    // 打印最终分布
    println!("\n最终分布:");
    for (server, count) in &final_distribution {
        println!("  {} -> {} 个键", server, count);
    }
    
    // 打印迁移率
    println!("\n添加节点后迁移了 {} 个键，迁移率: {:.2}%", 
        moved_keys, 
        (moved_keys as f64 / keys.len() as f64) * 100.0);
}
```

### 4.2 分片模式

**核心概念**: 分片模式将数据分割成较小的片段，分布在多个节点上，以支持水平扩展和并行处理。

**优点**:

- 提高系统容量和吞吐量
- 改善负载分布
- 支持大规模数据集

**缺点**:

- 增加系统复杂性
- 跨分片操作复杂
- 需要考虑分片平衡和重新分片

#### *使用异步和泛型实现*

```rust
use std::collections::{HashMap, HashSet};
use std::hash::{Hash, Hasher};
use std::sync::Arc;
use std::fmt::Debug;
use tokio::sync::RwLock;
use async_trait::async_trait;
use thiserror::Error;
use tracing::{info, warn, error};
use futures::future::join_all;
use serde::{Serialize, Deserialize};
use rand::seq::SliceRandom;

// 分片错误
#[derive(Error, Debug)]
pub enum ShardingError {
    #[error("分片不存在: {0}")]
    ShardNotFound(String),
    
    #[error("节点不存在: {0}")]
    NodeNotFound(String),
    
    #[error("存储错误: {0}")]
    StorageError(String),
    
    #[error("协调错误: {0}")]
    CoordinationError(String),
}

// 分片存储接口
#[async_trait]
pub trait ShardStore<K, V>: Send + Sync + 'static 
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    // 根据键获取值
    async fn get(&self, key: &K) -> Result<Option<V>, ShardingError>;
    
    // 存储键值对
    async fn put(&self, key: K, value: V) -> Result<(), ShardingError>;
    
    // 删除键
    async fn delete(&self, key: &K) -> Result<bool, ShardingError>;
    
    // 获取所有键
    async fn keys(&self

/*
继续完成分片模式实现的代码，包括分片存储接口方法和分片管理器的实现等。
*/

    // 获取所有键
    async fn keys(&self) -> Result<Vec<K>, ShardingError>;
    
    // 获取分片的大小
    async fn size(&self) -> Result<usize, ShardingError>;
}

// 分片节点信息
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub struct ShardNode {
    pub id: String,
    pub endpoint: String,
    pub status: NodeStatus,
}

// 节点状态
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum NodeStatus {
    Active,
    Inactive,
    Maintenance,
}

// 分片信息
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ShardInfo {
    pub id: String,
    pub node_ids: Vec<String>,
    pub key_range: Option<(u64, u64)>,
    pub size: usize,
}

// 分片管理器
pub struct ShardManager<K, V, S>
where
    K: Clone + Eq + Hash + Send + Sync + Debug + 'static,
    V: Clone + Send + Sync + 'static,
    S: ShardStore<K, V> + 'static,
{
    // 分片数量
    shard_count: usize,
    
    // 节点信息
    nodes: Arc<RwLock<HashMap<String, ShardNode>>>,
    
    // 分片到节点的映射
    shard_to_nodes: Arc<RwLock<HashMap<String, Vec<String>>>>,
    
    // 分片存储实例
    stores: Arc<RwLock<HashMap<String, Arc<S>>>>,
    
    // 每个分片的副本数
    replicas: usize,
}

impl<K, V, S> ShardManager<K, V, S>
where
    K: Clone + Eq + Hash + Send + Sync + Debug + 'static,
    V: Clone + Send + Sync + 'static,
    S: ShardStore<K, V> + 'static,
{
    // 创建新的分片管理器
    pub fn new(shard_count: usize, replicas: usize) -> Self {
        Self {
            shard_count,
            nodes: Arc::new(RwLock::new(HashMap::new())),
            shard_to_nodes: Arc::new(RwLock::new(HashMap::new())),
            stores: Arc::new(RwLock::new(HashMap::new())),
            replicas,
        }
    }
    
    // 添加节点
    pub async fn add_node(&self, node: ShardNode, store: Arc<S>) -> Result<(), ShardingError> {
        let mut nodes = self.nodes.write().await;
        let mut stores = self.stores.write().await;
        
        // 添加节点信息
        nodes.insert(node.id.clone(), node.clone());
        
        // 添加存储实例
        stores.insert(node.id.clone(), store);
        
        // 重新平衡分片
        drop(nodes);
        drop(stores);
        self.rebalance_shards().await?;
        
        info!("节点 {} 已添加", node.id);
        Ok(())
    }
    
    // 移除节点
    pub async fn remove_node(&self, node_id: &str) -> Result<(), ShardingError> {
        let mut nodes = self.nodes.write().await;
        
        if nodes.remove(node_id).is_none() {
            return Err(ShardingError::NodeNotFound(node_id.to_string()));
        }
        
        // 重新平衡分片
        drop(nodes);
        self.rebalance_shards().await?;
        
        info!("节点 {} 已移除", node_id);
        Ok(())
    }
    
    // 计算键所属的分片ID
    fn get_shard_id(&self, key: &K) -> String {
        // 计算哈希值
        let mut hasher = fnv::FnvHasher::default();
        key.hash(&mut hasher);
        let hash = hasher.finish();
        
        // 分配到分片
        let shard_index = (hash % self.shard_count as u64) as usize;
        format!("shard-{}", shard_index)
    }
    
    // 重新平衡分片
    async fn rebalance_shards(&self) -> Result<(), ShardingError> {
        let nodes = self.nodes.read().await;
        let mut shard_to_nodes = self.shard_to_nodes.write().await;
        
        // 获取活跃节点
        let active_nodes: Vec<&ShardNode> = nodes.values()
            .filter(|n| n.status == NodeStatus::Active)
            .collect();
            
        if active_nodes.is_empty() {
            return Err(ShardingError::CoordinationError("没有活跃节点".to_string()));
        }
        
        // 为每个分片分配节点
        for shard_idx in 0..self.shard_count {
            let shard_id = format!("shard-{}", shard_idx);
            
            // 选择节点
            let mut shard_nodes = Vec::new();
            
            for _ in 0..self.replicas.min(active_nodes.len()) {
                // 简单的循环分配策略
                // 在实际系统中，应该使用更智能的分配算法
                let chosen_idx = shard_idx % active_nodes.len();
                shard_nodes.push(active_nodes[chosen_idx].id.clone());
            }
            
            // 更新分片到节点的映射
            shard_to_nodes.insert(shard_id, shard_nodes);
        }
        
        info!("分片重新平衡已完成");
        Ok(())
    }
    
    // 根据键获取值
    pub async fn get(&self, key: &K) -> Result<Option<V>, ShardingError> {
        let shard_id = self.get_shard_id(key);
        let nodes = self.get_nodes_for_shard(&shard_id).await?;
        
        if nodes.is_empty() {
            return Err(ShardingError::ShardNotFound(shard_id));
        }
        
        // 首先尝试主副本
        let stores = self.stores.read().await;
        if let Some(store) = stores.get(&nodes[0]) {
            match store.get(key).await {
                Ok(value) => return Ok(value),
                Err(e) => {
                    warn!("从主副本读取失败: {}", e);
                    // 继续尝试其他副本
                }
            }
        }
        
        // 尝试其他副本
        for node_id in &nodes[1..] {
            if let Some(store) = stores.get(node_id) {
                match store.get(key).await {
                    Ok(value) => return Ok(value),
                    Err(e) => {
                        warn!("从副本 {} 读取失败: {}", node_id, e);
                    }
                }
            }
        }
        
        Err(ShardingError::StorageError("所有副本读取失败".to_string()))
    }
    
    // 存储键值对
    pub async fn put(&self, key: K, value: V) -> Result<(), ShardingError> {
        let shard_id = self.get_shard_id(&key);
        let nodes = self.get_nodes_for_shard(&shard_id).await?;
        
        if nodes.is_empty() {
            return Err(ShardingError::ShardNotFound(shard_id));
        }
        
        let stores = self.stores.read().await;
        let mut futures = Vec::new();
        
        // 向所有副本写入
        for node_id in &nodes {
            if let Some(store) = stores.get(node_id) {
                let store_clone = store.clone();
                let key_clone = key.clone();
                let value_clone = value.clone();
                
                futures.push(async move {
                    match store_clone.put(key_clone, value_clone).await {
                        Ok(()) => Ok(node_id.clone()),
                        Err(e) => Err((node_id.clone(), e)),
                    }
                });
            }
        }
        
        // 等待所有写入操作完成
        let results = join_all(futures).await;
        
        // 检查结果
        let mut success_count = 0;
        let mut errors = Vec::new();
        
        for result in results {
            match result {
                Ok(node_id) => {
                    success_count += 1;
                    info!("成功写入节点 {}", node_id);
                }
                Err((node_id, e)) => {
                    errors.push(format!("节点 {}: {}", node_id, e));
                    warn!("写入节点 {} 失败: {}", node_id, e);
                }
            }
        }
        
        // 只要有一个副本写入成功，我们就认为操作成功
        if success_count > 0 {
            if success_count < nodes.len() {
                warn!("部分副本写入失败: {}", errors.join(", "));
            }
            Ok(())
        } else {
            Err(ShardingError::StorageError(format!(
                "所有副本写入失败: {}", 
                errors.join(", ")
            )))
        }
    }
    
    // 删除键
    pub async fn delete(&self, key: &K) -> Result<bool, ShardingError> {
        let shard_id = self.get_shard_id(key);
        let nodes = self.get_nodes_for_shard(&shard_id).await?;
        
        if nodes.is_empty() {
            return Err(ShardingError::ShardNotFound(shard_id));
        }
        
        let stores = self.stores.read().await;
        let mut futures = Vec::new();
        
        // 从所有副本删除
        for node_id in &nodes {
            if let Some(store) = stores.get(node_id) {
                let store_clone = store.clone();
                let key_clone = key.clone();
                
                futures.push(async move {
                    match store_clone.delete(&key_clone).await {
                        Ok(deleted) => Ok((node_id.clone(), deleted)),
                        Err(e) => Err((node_id.clone(), e)),
                    }
                });
            }
        }
        
        // 等待所有删除操作完成
        let results = join_all(futures).await;
        
        // 检查结果
        let mut success_count = 0;
        let mut deleted = false;
        let mut errors = Vec::new();
        
        for result in results {
            match result {
                Ok((node_id, was_deleted)) => {
                    success_count += 1;
                    if was_deleted {
                        deleted = true;
                    }
                    info!("成功从节点 {} 删除: {}", node_id, was_deleted);
                }
                Err((node_id, e)) => {
                    errors.push(format!("节点 {}: {}", node_id, e));
                    warn!("从节点 {} 删除失败: {}", node_id, e);
                }
            }
        }
        
        // 只要有一个副本删除成功，我们就认为操作成功
        if success_count > 0 {
            if success_count < nodes.len() {
                warn!("部分副本删除失败: {}", errors.join(", "));
            }
            Ok(deleted)
        } else {
            Err(ShardingError::StorageError(format!(
                "所有副本删除失败: {}", 
                errors.join(", ")
            )))
        }
    }
    
    // 获取分片的节点列表
    async fn get_nodes_for_shard(&self, shard_id: &str) -> Result<Vec<String>, ShardingError> {
        let shard_to_nodes = self.shard_to_nodes.read().await;
        
        if let Some(node_ids) = shard_to_nodes.get(shard_id) {
            Ok(node_ids.clone())
        } else {
            Err(ShardingError::ShardNotFound(shard_id.to_string()))
        }
    }
    
    // 获取分片信息
    pub async fn get_shard_info(&self) -> Vec<ShardInfo> {
        let shard_to_nodes = self.shard_to_nodes.read().await;
        let mut result = Vec::new();
        
        for (shard_id, node_ids) in shard_to_nodes.iter() {
            result.push(ShardInfo {
                id: shard_id.clone(),
                node_ids: node_ids.clone(),
                key_range: None, // 简化实现
                size: 0, // 简化实现
            });
        }
        
        result
    }
}

// 内存存储实现
pub struct InMemoryShardStore<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    data: Arc<RwLock<HashMap<K, V>>>,
    node_id: String,
}

impl<K, V> InMemoryShardStore<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    pub fn new(node_id: &str) -> Self {
        Self {
            data: Arc::new(RwLock::new(HashMap::new())),
            node_id: node_id.to_string(),
        }
    }
}

#[async_trait]
impl<K, V> ShardStore<K, V> for InMemoryShardStore<K, V>
where
    K: Clone + Eq + Hash + Send + Sync + Debug + 'static,
    V: Clone + Send + Sync + 'static,
{
    async fn get(&self, key: &K) -> Result<Option<V>, ShardingError> {
        let data = self.data.read().await;
        Ok(data.get(key).cloned())
    }
    
    async fn put(&self, key: K, value: V) -> Result<(), ShardingError> {
        let mut data = self.data.write().await;
        data.insert(key, value);
        Ok(())
    }
    
    async fn delete(&self, key: &K) -> Result<bool, ShardingError> {
        let mut data = self.data.write().await;
        Ok(data.remove(key).is_some())
    }
    
    async fn keys(&self) -> Result<Vec<K>, ShardingError> {
        let data = self.data.read().await;
        Ok(data.keys().cloned().collect())
    }
    
    async fn size(&self) -> Result<usize, ShardingError> {
        let data = self.data.read().await;
        Ok(data.len())
    }
}

// 使用示例
#[tokio::main]
async fn main() -> Result<(), ShardingError> {
    // 创建分片管理器
    let shard_manager = Arc::new(ShardManager::<String, String, InMemoryShardStore<String, String>>::new(
        4, // 4个分片
        2, // 每个分片2个副本
    ));
    
    // 创建并添加节点
    for i in 1..=3 {
        let node_id = format!("node-{}", i);
        let node = ShardNode {
            id: node_id.clone(),
            endpoint: format!("http://localhost:{}", 8000 + i),
            status: NodeStatus::Active,
        };
        
        let store = Arc::new(InMemoryShardStore::new(&node_id));
        shard_manager.add_node(node, store).await?;
    }
    
    // 存储一些数据
    let test_data = vec![
        ("user:1001".to_string(), "张三".to_string()),
        ("user:1002".to_string(), "李四".to_string()),
        ("user:1003".to_string(), "王五".to_string()),
        ("user:1004".to_string(), "赵六".to_string()),
        ("user:1005".to_string(), "钱七".to_string()),
    ];
    
    // 写入数据
    for (key, value) in &test_data {
        shard_manager.put(key.clone(), value.clone()).await?;
        println!("写入: {} -> {}", key, value);
    }
    
    // 读取数据
    for (key, _) in &test_data {
        match shard_manager.get(key).await? {
            Some(value) => println!("读取: {} -> {}", key, value),
            None => println!("键 {} 不存在", key),
        }
    }
    
    // 获取分片信息
    let shard_info = shard_manager.get_shard_info().await;
    println!("\n分片分布:");
    for shard in &shard_info {
        println!("  分片 {} 位于节点: {:?}", shard.id, shard.node_ids);
    }
    
    // 模拟节点故障
    println!("\n模拟节点 node-2 故障...");
    shard_manager.remove_node("node-2").await?;
    
    // 再次获取分片信息
    let shard_info_after_failure = shard_manager.get_shard_info().await;
    println!("\n故障后分片分布:");
    for shard in &shard_info_after_failure {
        println!("  分片 {} 位于节点: {:?}", shard.id, shard.node_ids);
    }
    
    // 再次读取数据，验证故障转移是否正常工作
    println!("\n故障后读取数据:");
    for (key, _) in &test_data {
        match shard_manager.get(key).await? {
            Some(value) => println!("读取: {} -> {}", key, value),
            None => println!("键 {} 不存在", key),
        }
    }
    
    // 添加新节点
    println!("\n添加新节点 node-4...");
    let new_node = ShardNode {
        id: "node-4".to_string(),
        endpoint: "http://localhost:8004".to_string(),
        status: NodeStatus::Active,
    };
    
    let new_store = Arc::new(InMemoryShardStore::new("node-4"));
    shard_manager.add_node(new_node, new_store).await?;
    
    // 获取最终分片信息
    let final_shard_info = shard_manager.get_shard_info().await;
    println!("\n最终分片分布:");
    for shard in &final_shard_info {
        println!("  分片 {} 位于节点: {:?}", shard.id, shard.node_ids);
    }
    
    Ok(())
}
```

## 5. 综合模式与开源库分析

### Rust生态中分布式系统设计模式的优秀实现

#### 1. *通信模式*

**Tokio**: Rust的异步运行时，为网络通信提供了基础设施。

```rust
// Tokio中的请求-响应模式示例
use tokio::net::{TcpListener, TcpStream};
use tokio::io::{AsyncReadExt, AsyncWriteExt};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 创建服务器
    let listener = TcpListener::bind("127.0.0.1:8080").await?;
    
    loop {
        // 接受连接
        let (mut socket, _) = listener.accept().await?;
        
        // 为每个连接创建一个任务
        tokio::spawn(async move {
            let mut buf = [0; 1024];
            
            // 读取请求
            let n = socket.read(&mut buf).await.unwrap();
            
            // 处理请求并生成响应
            let response = format!("收到 {} 字节的请求", n);
            
            // 发送响应
            socket.write_all(response.as_bytes()).await.unwrap();
        });
    }
}
```

**Lapin**: Rust的AMQP客户端，提供了与RabbitMQ等消息队列的强大集成。

```rust
// Lapin发布-订阅示例
use lapin::{
    options::*, types::FieldTable, Connection, Result,
    message::Delivery, BasicProperties,
};
use futures_lite::stream::StreamExt;

#[tokio::main]
async fn main() -> Result<()> {
    // 连接RabbitMQ
    let conn = Connection::connect(
        "amqp://guest:guest@localhost:5672/%2f",
        ConnectionProperties::default(),
    ).await?;
    
    let channel = conn.create_channel().await?;
    
    // 声明交换机和队列
    let exchange = "events";
    channel.exchange_declare(
        exchange, ExchangeKind::Topic, 
        ExchangeDeclareOptions::default(), FieldTable::default()
    ).await?;
    
    // 创建队列
    let queue = channel.queue_declare(
        "my_queue", QueueDeclareOptions::default(), FieldTable::default()
    ).await?;
    
    // 绑定队列到交换机
    channel.queue_bind(
        &queue.name, exchange, "user.#", 
        QueueBindOptions::default(), FieldTable::default()
    ).await?;
    
    // 发布消息
    channel.basic_publish(
        exchange, "user.created", 
        BasicPublishOptions::default(),
        b"新用户已创建", BasicProperties::default()
    ).await?;
    
    // 消费消息
    let mut consumer = channel.basic_consume(
        &queue.name, "my_consumer", 
        BasicConsumeOptions::default(), FieldTable::default()
    ).await?;
    
    while let Some(delivery) = consumer.next().await {
        if let Ok(delivery) = delivery {
            println!("收到消息: {:?}", delivery.data);
            delivery.ack(BasicAckOptions::default()).await?;
        }
    }
    
    Ok(())
}
```

#### 2. *一致性与复制模式*

**Raft实现**: Tikv的Raft实现是业界最成熟的Rust Raft库之一。

```rust
// Tikv的Raft使用示例 (简化版)
use raft::{Config, Raft, RawNode, Storage, StorageError};
use raft::eraftpb::Message;
use slog::{Drain, Logger};

// 配置Raft节点
let config = Config {
    id: 1,
    election_tick: 10,
    heartbeat_tick: 3,
    ..Default::default()
};

// 创建日志
let decorator = slog_term::TermDecorator::new().build();
let drain = slog_term::FullFormat::new(decorator).build().fuse();
let drain = slog_async::Async::new(drain).build().fuse();
let logger = Logger::root(drain, o!());

// 初始化存储
let storage = MemStorage::new();
let mut node = RawNode::new(&config, storage, &logger).unwrap();

// 处理Ready状态
loop {
    match node.ready() {
        Some(ready) => {
            // 处理消息
            for msg in ready.messages {
                // 发送消息到其他节点
            }
            
            // 应用已提交的条目到状态机
            if !ready.committed_entries.is_empty() {
                for entry in ready.committed_entries {
                    // 应用条目到状态机
                }
            }
            
            // 持久化Raft状态
            if let Some(hs) = ready.hs {
                // 保存硬状态
            }
            
            if !ready.entries.is_empty() {
                // 保存日志条目
            }
            
            // 通知Raft状态已处理
            node.advance(ready);
        }
        None => {
            // 没有准备好的状态，可以休眠一会
        }
    }
}
```

**automerge-rs**: Rust实现的CRDT库，支持JSON结构的并发编辑。

```rust
// automerge-rs CRDT使用示例
use automerge::{ActorId, Automerge, Value, transaction::Transactable};

// 创建新文档
let mut doc1 = Automerge::new();
let mut doc2 = Automerge::new();

// 在doc1中编辑
doc1.transact::<_, _, ()>(|tx| {
    tx.put_object(automerge::ROOT, "count", 0)?;
    tx.put_object(automerge::ROOT, "name", "文档")?;
    Ok(())
}).unwrap();

// 将doc1的更改同步到doc2
let changes = doc1.get_changes(&[]);
doc2.apply_changes(changes).unwrap();

// 在doc2中编辑
doc2.transact::<_, _, ()>(|tx| {
    tx.put_object(automerge::ROOT, "count", 1)?;
    Ok(())
}).unwrap();

// 在doc1中独立编辑
doc1.transact::<_, _, ()>(|tx| {
    tx.put_object(automerge::ROOT, "description", "测试文档")?;
    Ok(())
}).unwrap();

// 双向同步变更
let changes1to2 = doc1.get_changes(&[]);
let changes2to1 = doc2.get_changes(&[]);

doc1.apply_changes(changes2to1).unwrap();
doc2.apply_changes(changes1to2).unwrap();

// 现在两个文档应该具有相同的状态
assert_eq!(doc1.to_json(), doc2.to_json());
```

#### 3. *容错模式*

**Failsafe**: 专为Rust设计的熔断器库。

```rust
// Failsafe熔断器示例
use failsafe::{Config, CircuitBreaker, Error};
use std::time::Duration;

// 创建熔断器
let circuit_breaker = Config::new()
    .failure_ratio(0.5)    // 50%的失败率
    .window_size(10)       // 考虑最近10次调用
    .wait_interval(Duration::from_secs(10)) // 跳闸后等待10秒
    .build();

// 使用熔断器保护调用
let result = circuit_breaker.call(|| {
    // 模拟可能失败的操作
    if rand::random::<f64>() < 0.3 {
        Err(Error::Transient)  // 临时错误
    } else {
        Ok("操作成功")
    }
});

match result {
    Ok(value) => println!("调用成功: {}", value),
    Err(Error::Rejected) => println!("熔断器开启，拒绝调用"),
    Err(Error::Inner(e)) => println!("调用失败: {:?}", e),
}
```

**Tokio的任务机制**: Tokio提供了任务隔离，实现舱壁模式。

```rust
// Tokio的舱壁模式示例
use tokio::task::JoinSet;
use tokio::time::{sleep, Duration};
use rand::Rng;

#[tokio::main]
async fn main() {
    let mut set = JoinSet::new();
    
    // 创建三个隔离的组件
    let component_names = ["数据库", "缓存", "API"];
    
    for &name in &component_names {
        set.spawn(async move {
            // 组件任务
            loop {
                // 模拟工作
                let work_time = rand::thread_rng().gen_range(50..200);
                sleep(Duration::from_millis(work_time)).await;
                
                // 模拟随机错误
                if rand::thread_rng().gen_bool(0.05) {
                    eprintln!("组件 {} 发生错误!", name);
                    // 组件错误不会影响其他组件
                }
                
                println!("组件 {} 正常工作中", name);
            }
        });
    }
    
    // 等待任何组件完成（在这个例子中，它们是无限循环）
    if let Some(res) = set.join_next().await {
        println!("一个组件已退出: {:?}", res);
    }
}
```

#### 4. *分区与扩展模式*

**consistent-hashing**: Rust的一致性哈希库。

```rust
// 一致性哈希使用示例
use consistent_hash_ring::ConsistentHashRing;

// 创建一致性哈希环
let mut ring = ConsistentHashRing::default();

// 添加节点
ring.add("server1", 100);
ring.add("server2", 100);
ring.add("server3", 100);

// 根据键确定节点
let server = ring.get_one("user123");
println!("键 'user123' 应该路由到节点: {:?}", server);

// 移除一个节点
ring.remove("server2");

// 重新确定节点
let new_server = ring.get_one("user123");
println!("键 'user123' 现在应该路由到节点: {:?}", new_server);
```

### 综合分析与最佳实践

1. **异步优先**:
   - 现代Rust分布式系统大多采用异步编程模型
   - Tokio作为主流运行时，提供了强大的性能保证
   - 异步特别适合I/O密集型的分布式系统

2. **类型安全**:
   - Rust的类型系统被充分利用来提供编译时保证
   - 泛型允许代码重用而不牺牲性能
   - 错误处理通过Result类型提供清晰的异常路径

3. **零成本抽象**:
   - 高级抽象不增加运行时开销
   - 元编程（宏、特征等）生成优化的代码
   - 性能敏感部分可以直接控制内存和并发

4. **组合优于继承**:
   - 使用特征(Trait)组合而不是层次结构
   - 提高代码复用性的同时保持灵活性
   - 允许无侵入式地添加新功能

5. **开源库互操作性**:
   - 与生态系统其他部分的无缝集成
   - 使用标准接口确保互操作性
   - 接口设计允许组件替换

6. **错误处理**:
   - 使用`thiserror`和`anyhow`提供丰富的错误上下文
   - 遵循"向上传播错误、向下处理错误"的原则
   - 提供足够信息以便调试和追踪故障

7. **可观测性集成**:
   - `tracing`和`metrics`集成到核心组件
   - OpenTelemetry支持
   - 结构化日志记录和问题定位支持

现代Rust分布式系统通常结合使用以上设计模式和最佳实践，
创建具有高性能、高可靠性和可维护性的系统。
同时，Rust的安全保证使得分布式系统可以避免许多传统上由内存错误导致的复杂问题，
使开发人员能够专注于分布式系统本身的挑战，而不是底层实现细节。
