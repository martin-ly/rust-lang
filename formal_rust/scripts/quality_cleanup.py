#!/usr/bin/env python3
"""
Rustå½¢å¼åŒ–ç†è®ºé¡¹ç›®è´¨é‡æ¸…ç†è„šæœ¬
è‡ªåŠ¨è¯†åˆ«å’Œæ¸…ç†ä½è´¨é‡æ–‡ä»¶
"""

import os
import re
from pathlib import Path
from typing import List, Tuple

class QualityCleaner:
    def __init__(self, root_dir: str = "."):
        self.root_dir = Path(root_dir)
        self.low_quality_files = []
        self.duplicate_files = []
        
    def scan_files(self) -> Tuple[List[str], List[str]]:
        """æ‰«ææ‰€æœ‰æ–‡ä»¶ï¼Œè¯†åˆ«ä½è´¨é‡æ–‡ä»¶å’Œé‡å¤æ–‡ä»¶"""
        print("ğŸ” å¼€å§‹æ‰«ææ–‡ä»¶...")
        
        all_files = []
        file_contents = {}
        
        for file_path in self.root_dir.rglob("*.md"):
            if file_path.is_file():
                try:
                    content = file_path.read_text(encoding='utf-8')
                    all_files.append(str(file_path))
                    
                    # æ£€æŸ¥æ–‡ä»¶è´¨é‡
                    if self.is_low_quality(content):
                        self.low_quality_files.append(str(file_path))
                    
                    # æ£€æŸ¥é‡å¤å†…å®¹
                    content_hash = self.get_content_hash(content)
                    if content_hash in file_contents:
                        self.duplicate_files.append(str(file_path))
                    else:
                        file_contents[content_hash] = str(file_path)
                        
                except Exception as e:
                    print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path} - {e}")
        
        return self.low_quality_files, self.duplicate_files
    
    def is_low_quality(self, content: str) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸ºä½è´¨é‡"""
        lines = content.strip().split('\n')
        
        # æ£€æŸ¥è¡Œæ•°
        if len(lines) < 50:
            return True
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åªæœ‰æ ‡é¢˜
        non_empty_lines = [line for line in lines if line.strip() and not line.startswith('#')]
        if len(non_empty_lines) < 20:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åªæœ‰ç®€å•åˆ—è¡¨
        if len(re.findall(r'^- ', content)) > len(non_empty_lines) * 0.8:
            return True
        
        return False
    
    def get_content_hash(self, content: str) -> str:
        """è·å–å†…å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹é‡å¤"""
        # ç§»é™¤ç©ºç™½å­—ç¬¦å’Œæ³¨é‡Š
        cleaned = re.sub(r'\s+', ' ', content.strip())
        cleaned = re.sub(r'<!--.*?-->', '', cleaned)
        return cleaned
    
    def cleanup_low_quality_files(self, dry_run: bool = True) -> int:
        """æ¸…ç†ä½è´¨é‡æ–‡ä»¶"""
        print(f"\nğŸ—‘ï¸ å¼€å§‹æ¸…ç†ä½è´¨é‡æ–‡ä»¶ (dry_run={dry_run})...")
        
        cleaned_count = 0
        for file_path in self.low_quality_files:
            try:
                if dry_run:
                    print(f"  å°†åˆ é™¤: {file_path}")
                else:
                    os.remove(file_path)
                    print(f"  å·²åˆ é™¤: {file_path}")
                cleaned_count += 1
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤å¤±è´¥: {file_path} - {e}")
        
        return cleaned_count
    
    def merge_duplicate_files(self, dry_run: bool = True) -> int:
        """åˆå¹¶é‡å¤æ–‡ä»¶"""
        print(f"\nğŸ”— å¼€å§‹åˆå¹¶é‡å¤æ–‡ä»¶ (dry_run={dry_run})...")
        
        merged_count = 0
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„åˆå¹¶é€»è¾‘
        for file_path in self.duplicate_files:
            try:
                if dry_run:
                    print(f"  å°†åˆå¹¶: {file_path}")
                else:
                    # å®ç°åˆå¹¶é€»è¾‘
                    pass
                merged_count += 1
            except Exception as e:
                print(f"âš ï¸ åˆå¹¶å¤±è´¥: {file_path} - {e}")
        
        return merged_count
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        report = f"""
# è´¨é‡æ¸…ç†æŠ¥å‘Š

## æ‰«æç»“æœ
- æ‰«æç›®å½•: {self.root_dir}
- ä½è´¨é‡æ–‡ä»¶: {len(self.low_quality_files)} ä¸ª
- é‡å¤æ–‡ä»¶: {len(self.duplicate_files)} ä¸ª

## ä½è´¨é‡æ–‡ä»¶åˆ—è¡¨
"""
        
        for file_path in self.low_quality_files:
            report += f"- {file_path}\n"
        
        report += "\n## é‡å¤æ–‡ä»¶åˆ—è¡¨\n"
        for file_path in self.duplicate_files:
            report += f"- {file_path}\n"
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    cleaner = QualityCleaner()
    
    # æ‰«ææ–‡ä»¶
    low_quality, duplicates = cleaner.scan_files()
    
    print(f"\nğŸ“Š æ‰«æç»“æœ:")
    print(f"  ä½è´¨é‡æ–‡ä»¶: {len(low_quality)} ä¸ª")
    print(f"  é‡å¤æ–‡ä»¶: {len(duplicates)} ä¸ª")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = cleaner.generate_report()
    with open("quality_cleanup_report.md", "w", encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: quality_cleanup_report.md")
    
    # è¯¢é—®æ˜¯å¦æ‰§è¡Œæ¸…ç†
    if low_quality or duplicates:
        response = input("\næ˜¯å¦æ‰§è¡Œæ¸…ç†? (y/N): ")
        if response.lower() == 'y':
            cleaner.cleanup_low_quality_files(dry_run=False)
            cleaner.merge_duplicate_files(dry_run=False)
            print("\nâœ… æ¸…ç†å®Œæˆ!")
        else:
            print("\nâ¸ï¸ æ¸…ç†å·²å–æ¶ˆ")
    else:
        print("\nâœ… æ²¡æœ‰å‘ç°éœ€è¦æ¸…ç†çš„æ–‡ä»¶")

if __name__ == "__main__":
    main()
